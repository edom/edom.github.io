<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Making intelligence</title>
  <meta name="description" content="Personal website">

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="https://edom.github.io/intelligence.html">
  <link rel="alternate" type="application/rss+xml" title="Erik Dominikus&#39;s wiki" href="/feed.xml">

  

  
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-12628443-6', 'auto');
  ga('send', 'pageview');

</script>
  

  

  
  <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    jax: ["input/TeX","input/MathML","input/AsciiMath",

    "output/CommonHTML"

    ],
    extensions: ["tex2jax.js","mml2jax.js","asciimath2jax.js","MathMenu.js","MathZoom.js","AssistiveMML.js", "a11y/accessibility-menu.js"],
    TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
        , equationNumbers: {
            autoNumber: "AMS"
        }
    },
    "CommonHTML": {
        scale: 100
    },
    "fast-preview": {
        disabled: true,
    }
});
  </script>
  
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js" async></script>
  
</head>


  <body>

    <header class="site-header" role="banner">
  <div class="wrapper">
    <a class="site-title" rel="author" href="/">Erik Dominikus&#39;s wiki</a>
  </div>
</header>


    
  <div style="display:none;">\(
\renewcommand\emptyset{\varnothing}
\newcommand\abs[1]{\left|#1\right|}
\newcommand\dom{\textrm{dom}}
\newcommand\cod{\textrm{cod}}
\newcommand\Bernoulli{\textrm{Bernoulli}}
\newcommand\Binomial{\textrm{Binomial}}
\newcommand\Expect[1]{\mathbb{E}[#1]}
\newcommand\Nat{\mathbb{N}}
\newcommand\Integers{\mathbb{Z}}
\newcommand\Real{\mathbb{R}}
\newcommand\Rational{\mathbb{Q}}
\newcommand\Complex{\mathbb{C}}
\newcommand\Pr{\mathrm{P}}
\newcommand\Time{\text{Time}}
\newcommand\DTime{\text{DTime}}
\newcommand\NTime{\text{NTime}}
\newcommand\TimeP{\text{P}}
\newcommand\TimeNP{\text{NP}}
\newcommand\TimeExp{\text{ExpTime}}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand\bbA{\mathbb{A}}
\newcommand\bbC{\mathbb{C}}
\newcommand\bbD{\mathbb{D}}
\newcommand\bbE{\mathbb{E}}
\newcommand\bbN{\mathbb{N}}
\newcommand\frakI{\mathfrak{I}}
% deprecated; use TimeExp
\newcommand\ExpTime{\text{ExpTime}}
\newcommand\Compute{\text{Compute}}
\newcommand\Search{\text{Search}}
% model theory structure
\newcommand\struc[1]{\mathcal{#1}}
  \)</div>
    

    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Making intelligence</h1>
  </header>

  <div class="post-content">
    <div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org10df86a">1. intelligence.md</a></li>
<li><a href="#orga2222b5">2. intmeta.md</a></li>
<li><a href="#org64202b5">3. intwhat.md</a></li>
<li><a href="#org397b2bc">4. approx.md</a></li>
<li><a href="#org1e77248">5. atrunc.md</a></li>
<li><a href="#org15adf1d">6. Counterfactual reasoning</a></li>
<li><a href="#orga3dc5db">7. Automatic differentiation?</a></li>
<li><a href="#org150c89c">8. About defining consciousness</a></li>
<li><a href="#orga076ea7">9. <span class="timestamp-wrapper"><span class="timestamp">&lt;2018-09-28&gt; </span></span> Book: "interpretable machine learning"</a></li>
<li><a href="#orgb5b3848">10. Approximation theory and machine learning</a></li>
<li><a href="#org920773d">11. Analogizers, recommender systems, matrices</a></li>
</ul>
</div>
</div>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org10df86a">1. intelligence.md</a>
<ul>
<li><a href="#plan">1.1. Plan</a></li>
<li><a href="#questions">1.2. Questions</a></li>
<li><a href="#note-to-self">1.3. Note to self</a></li>
<li><a href="#others">1.4. Others</a></li>
<li><a href="#non-prioritized-questions">1.5. Non-prioritized questions</a></li>
<li><a href="#how-might-we-build-a-seed-ai">1.6. How might we build a seed AI?</a></li>
<li><a href="#guesses">1.7. Guesses</a></li>
<li><a href="#undigested-information">1.8. Undigested information</a></li>
</ul>
</li>
<li><a href="#orga2222b5">2. intmeta.md</a>
<ul>
<li><a href="#questions">2.1. Questions</a></li>
<li><a href="#how-can-i-become-an-ai-researcher">2.2. How can I become an AI researcher?</a></li>
<li><a href="#how-are-others-works-progressing">2.3. How are others' works progressing?</a></li>
</ul>
</li>
<li><a href="#org64202b5">3. intwhat.md</a>
<ul>
<li><a href="#intelligence-is-an-ordering-2018-04-26">3.1. Intelligence is an ordering (2018-04-26)</a>
<ul>
<li><a href="#how-do-we-decide-which-system-is-more-intelligent">3.1.1. How do we decide which system is more intelligent?</a></li>
</ul>
</li>
<li><a href="#intelligence-is-function-optimization-2018-04-27">3.2. Intelligence is function optimization (2018-04-27)</a></li>
<li><a href="#what-is-a-mathematical-theory-of-intelligence">3.3. What is a mathematical theory of intelligence?</a></li>
<li><a href="#what-is-learning">3.4. What is learning?</a></li>
<li><a href="#what-is-ai">3.5. What is AI?</a></li>
</ul>
</li>
<li><a href="#org397b2bc">4. approx.md</a>
<ul>
<li><a href="#other">4.1. Other</a></li>
</ul>
</li>
<li><a href="#org1e77248">5. atrunc.md</a>
<ul>
<li><a href="#power-series-truncation">5.1. Power series truncation</a>
<ul>
<li><a href="#digression-what-is-an-analytic-function">5.1.1. Digression: What is an analytic function?</a></li>
<li><a href="#digression-what-is-the-relationship-between-polynomial-and-power-series">5.1.2. Digression: What is the relationship between polynomial and power series?</a></li>
</ul>
</li>
<li><a href="#iteration-truncation">5.2. Iteration truncation</a></li>
</ul>
</li>
<li><a href="#org15adf1d">6. Counterfactual reasoning</a></li>
<li><a href="#orga3dc5db">7. Automatic differentiation?</a></li>
<li><a href="#org150c89c">8. About defining consciousness</a></li>
<li><a href="#orga076ea7">9. <span class="timestamp-wrapper"><span class="timestamp">&lt;2018-09-28&gt; </span></span> Book: "interpretable machine learning"</a></li>
<li><a href="#orgb5b3848">10. Approximation theory and machine learning</a></li>
<li><a href="#org920773d">11. Analogizers, recommender systems, matrices</a></li>
</ul>
</div>
</div>
<div id="outline-container-org10df86a" class="outline-2">
<h2 id="org10df86a"><span class="section-number-2">1</span> intelligence.md</h2>
<div class="outline-text-2" id="text-1">
<ul class="org-ul">
<li>Abbreviations:

<ul class="org-ul">
<li>AI: Artificial Intelligence</li>
<li>ML: Machine Learning</li>
<li>COLT: Computational Learning Theory</li>
</ul></li>

<li>What are some AI survey papers, review articles, and expository works?

<ul class="org-ul">
<li>Google query: most recent mathematical ai book</li>
<li><a href="http://eliassi.org/COLTSurveyArticle.pdf">http://eliassi.org/COLTSurveyArticle.pdf</a></li>
<li><a href="https://en.wikipedia.org/wiki/Computational_learning_theory#Surveys">WP: COLT surveys</a></li>
<li><a href="http://www.cs.ox.ac.uk/people/varun.kanade/teaching/CLT-HT2018/lectures/">COLT lecture 2018</a></li>
<li>Book: "An Introduction to Computational Learning Theory" by Kearns and Vazirani</li>
<li><a href="https://mitpress.mit.edu/books/introduction-computational-learning-theory">https://mitpress.mit.edu/books/introduction-computational-learning-theory</a></li>
</ul></li>
</ul>
</div>

<div id="outline-container-org0b6cef7" class="outline-3">
<h3 id="plan"><a id="org0b6cef7"></a><span class="section-number-3">1.1</span> Plan</h3>
<div class="outline-text-3" id="text-plan">
<ul class="org-ul">
<li>Read about universal intelligence

<ul class="org-ul">
<li>Pamela McCorduck's "Machines who think" for some history

<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/Timeline_of_artificial_intelligence">WP: Timeline of artificial intelligence</a></li>
<li><a href="https://en.wikipedia.org/wiki/Progress_in_artificial_intelligence">WP: Progress in artificial intelligence</a></li>
</ul></li>

<li>[Hutter2005Book]</li>
<li><a href="http://www.hutter1.net/ai/uaibook.htm">hutter1.net&#x2026;uaibook.htm</a>

<ul class="org-ul">
<li>He formulated the "degree of intelligence" in 2005</li>
<li>(edited) "AIXI [&#x2026;] learns by eliminating Turing machines [&#x2026;] once they become inconsistent with the progressing history."</li>
</ul></li>

<li><a href="http://www.hutter1.net/ai/suaibook.pdf">Presentation, 393 slides</a></li>
<li><a href="http://users.cecs.anu.edu.au/~ssanner/MLSS2010/Hutter1.pdf">Slides</a>, maybe a draft of the above.</li>
<li>Shane Legg's PhD thesis "Machine super intelligence" [Legg2008]</li>
<li><a href="http://www.vetta.org/documents/universal_intelligence_abstract_ai50.pdf">Legg and Hutter: A formal definition of intelligence for artificial systems</a></li>
<li>2005 Negnevitsky AI book \cite{negnevitsky2005artificial}?</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orge51e0d6" class="outline-3">
<h3 id="questions"><a id="orge51e0d6"></a><span class="section-number-3">1.2</span> Questions</h3>
<div class="outline-text-3" id="text-questions">
<ul class="org-ul">
<li>COLT

<ul class="org-ul">
<li>Should we read this?

<ul class="org-ul">
<li><a href="https://arxiv.org/abs/1405.1513">Ibrahim Alabdulmohsin: A Mathematical Theory of Learning</a></li>
<li>1999: <a href="http://www.cis.syr.edu/people/royer/stl2e/">Sanjay Jain et al.: Systems that learn</a></li>
<li><a href="https://www.quora.com/What-are-the-best-math-books-for-machine-learning">https://www.quora.com/What-are-the-best-math-books-for-machine-learning</a></li>
<li><a href="https://machinelearningwithvick.quora.com/Learning-about-machine-learning">https://machinelearningwithvick.quora.com/Learning-about-machine-learning</a></li>
<li><a href="http://web.archive.org/web/20101102210231/http://measuringmeasures.com/blog/2010/1/15/learning-about-statistical-learning.html">http://web.archive.org/web/20101102210231/http://measuringmeasures.com/blog/2010/1/15/learning-about-statistical-learning.html</a></li>
<li><a href="https://www.quora.com/Which-are-the-best-books-to-get-the-Math-background-for-Machine-Learning">https://www.quora.com/Which-are-the-best-books-to-get-the-Math-background-for-Machine-Learning</a></li>
<li><a href="https://www.quora.com/How-do-I-learn-mathematics-for-machine-learning?share=1">https://www.quora.com/How-do-I-learn-mathematics-for-machine-learning?share=1</a></li>
</ul></li>

<li><a href="http://emis.ams.org/journals/TAC/reprints/articles/22/tr22.pdf">http://emis.ams.org/journals/TAC/reprints/articles/22/tr22.pdf</a>

<ul class="org-ul">
<li><a href="https://www.quora.com/What-are-some-survey-papers-on-artificial-intelligence-and-deep-learning">https://www.quora.com/What-are-some-survey-papers-on-artificial-intelligence-and-deep-learning</a></li>
<li><a href="http://people.idsia.ch/~juergen/deep-learning-conspiracy.html">http://people.idsia.ch/~juergen/deep-learning-conspiracy.html</a></li>
<li><a href="https://arxiv.org/abs/1404.7828">Jürgen Schmidhuber: "Deep Learning in Neural Networks: An Overview"</a></li>
<li><a href="http://www.ijircce.com/upload/2017/june/107_A%20Survey.pdf">http://www.ijircce.com/upload/2017/june/107_A Survey.pdf</a></li>
</ul></li>
</ul></li>
</ul>

<p>
Should we read these?
</p>

<p>
2017, <a href="https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/building-machines-that-learn-and-think-for-themselves/E28DBFEC380D4189FB7754B50066A96F">Building machines that learn and think for themselves</a>
</p>
</div>
</div>

<div id="outline-container-org1c05188" class="outline-3">
<h3 id="note-to-self"><a id="org1c05188"></a><span class="section-number-3">1.3</span> Note to self</h3>
<div class="outline-text-3" id="text-note-to-self">
<ul class="org-ul">
<li>Which AI architecture has won lots of AI contests lately?

<ul class="org-ul">
<li>Is it LSTM RNN?</li>
<li>What is LSTM RNN?

<ul class="org-ul">
<li>"long short-term memory recurrent neural network"</li>
<li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></li>
<li>"The expression <i>long short-term</i> refers to the fact that LSTM is a model
for the <i>short-term memory</i> which can last for a <i>long</i> period of time." (<a href="https://en.wikipedia.org/wiki/Long_short-term_memory">Wikipedia</a>)</li>
</ul></li>
</ul></li>

<li>How do we learn amid lies, deception, disinformation, misinformation?

<ul class="org-ul">
<li>Related to adversarial learning? <a href="https://en.wikipedia.org/wiki/Adversarial_machine_learning">https://en.wikipedia.org/wiki/Adversarial_machine_learning</a> ?</li>
</ul></li>

<li>What are some tools that I can use to make my computer learn?

<ul class="org-ul">
<li>Google TensorFlow</li>
<li>Does OpenAI have tools?</li>
</ul></li>

<li>TODO s/adapt/habituate</li>
<li>Let \(f(t,x)\) be the system's response intensity for stimulus intensity \(x\) at time \(t\). We say the system is <i>habituating</i> between the time \(t_1\) and \(t_2\) iff \(f(t_1,x) > f(t_2,x)\) for all stimulus intensity \(x\).</li>
<li>"The habituation process is a form of adaptive behavior (or neuroplasticity) that is classified as non-associative learning." <a href="https://en.wikipedia.org/wiki/Habituation">https://en.wikipedia.org/wiki/Habituation</a></li>
<li>How many AI approaches are there?

<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/Portal:Artificial_intelligence">WP AI Portal</a> lists 4 approaches</li>
<li>Pedro Domingos lists 5 "tribes"</li>
</ul></li>

<li>(merge AI researchers)

<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/Portal:Artificial_intelligence">WP AI Portal</a> lists several leading AI researchers</li>
</ul></li>

<li>2000, György Turán, <a href="https://link.springer.com/article/10.1023/A:1018948021083">Remarks on COLT</a></li>
<li>2016, Krendzelak, Jakab, <a href="https://ieeexplore.ieee.org/document/7802092/">Fundamental principals of Computational Learning Theory</a>

<ul class="org-ul">
<li>Reading queue:

<ul class="org-ul">
<li>D. Angluin, C. Smith, "Inductive inference: theory and methods", A.C.M. Computing Surveys, vol. 15, pp. 237-269, 1983.</li>
<li>M. Anthony, N. Biggs, "Computational Learning Theory" in , Cambridge university press, 1992.</li>
<li>M.J. Kearns, "The computational Complexity of Machine Learning" in , The MIT Press, May 1990.</li>
<li>L.G. Valiant, "A theory of the learnable", Communications of the A.C.M., vol. 27, no. 11, pp. 1134-1142, 1984.</li>
<li>L. Pitt, L.G. Valiant, "Computational limitations on learning from examples", Journal of the A.C.M., vol. 35, no. 4, pp. 965-984, 1988.</li>
</ul></li>
</ul></li>

<li>helpful slides
<a href="https://cs.uwaterloo.ca/~klarson/teaching/W15-486/lectures/22Colt.pdf">https://cs.uwaterloo.ca/~klarson/teaching/W15-486/lectures/22Colt.pdf</a></li>
<li>Bertoni et
al. <a href="http://elearning.unimib.it/pluginfile.php/283303/mod_resource/content/1/Apprendimento_Automatico/Computational_Learning.pdf">http://elearning.unimib.it/pluginfile.php/283303/mod_resource/content/1/Apprendimento_Automatico/Computational_Learning.pdf</a></li>
<li><a href="https://stats.stackexchange.com/questions/142906/what-does-pac-learning-theory-mean">https://stats.stackexchange.com/questions/142906/what-does-pac-learning-theory-mean</a></li>
<li><a href="https://pdfs.semanticscholar.org/presentation/fbbd/65646c8a81094864d4e0b0dfb9c1f22181af.pdf">https://pdfs.semanticscholar.org/presentation/fbbd/65646c8a81094864d4e0b0dfb9c1f22181af.pdf</a></li>
<li><a href="http://web.cs.iastate.edu/~honavar/colt-tutorial.pdf">http://web.cs.iastate.edu/~honavar/colt-tutorial.pdf</a></li>
<li><a href="https://en.wikipedia.org/wiki/Probably_approximately_correct_learning#cite_note-valiant-1">https://en.wikipedia.org/wiki/Probably_approximately_correct_learning#cite_note-valiant-1</a>
A Theory of the Learnable
Leslie G. Valiant
1984
<a href="http://web.mit.edu/6.435/www/Valiant84.pdf">http://web.mit.edu/6.435/www/Valiant84.pdf</a></li>
<li>kearns vazirani introduction
<a href="ftp://ftp.cis.upenn.edu/pub/cse140/public_html/2002/kvpages.pdf">ftp://ftp.cis.upenn.edu/pub/cse140/public_html/2002/kvpages.pdf</a></li>
<li><a href="http://www.cis.upenn.edu/~mkearns/">http://www.cis.upenn.edu/~mkearns/</a>
the computational complexity of machine learning
<a href="http://www.cis.upenn.edu/~mkearns/papers/thesis.pdf">http://www.cis.upenn.edu/~mkearns/papers/thesis.pdf</a>
<a href="https://www.worldscientific.com/worldscibooks/10.1142/10175">https://www.worldscientific.com/worldscibooks/10.1142/10175</a></li>
<li>2015
<a href="http://www.cs.tufts.edu/~roni/Teaching/CLT/">http://www.cs.tufts.edu/~roni/Teaching/CLT/</a></li>
<li>probably link to this
<a href="http://bactra.org/notebooks/learning-theory.html">http://bactra.org/notebooks/learning-theory.html</a></li>
<li>semantics-first
<a href="https://pdfs.semanticscholar.org/83e7/b615c165209af54dd0fe05c850bb08232625.pdf">https://pdfs.semanticscholar.org/83e7/b615c165209af54dd0fe05c850bb08232625.pdf</a></li>
<li>discrete approximation theory
see the references of this paper
<a href="https://www.worldscientific.com/doi/suppl/10.1142/10175/suppl_file/10175_chap01.pdf">https://www.worldscientific.com/doi/suppl/10.1142/10175/suppl_file/10175_chap01.pdf</a></li>
<li><a href="https://profs.info.uaic.ro/~ciortuz/SLIDES/ml7.pdf">https://profs.info.uaic.ro/~ciortuz/SLIDES/ml7.pdf</a></li>
</ul>

<p>
Optimal learning for humans
<a href="https://www.kqed.org/mindshift/37289">https://www.kqed.org/mindshift/37289</a>
</p>

<p>
Curate from this
<a href="https://thesecondprinciple.com/optimal-learning/">https://thesecondprinciple.com/optimal-learning/</a>
</p>

<p>
Boston dynamics dog robots
</p>

<p>
Tesla car autopilots
</p>

<p>
Google and Uber self-driving cars
</p>

<p>
<a href="https://www.quora.com/Will-we-ever-have-a-rigorous-and-robust-definition-for-intelligence">https://www.quora.com/Will-we-ever-have-a-rigorous-and-robust-definition-for-intelligence</a>
</p>

<p>
rigorous definition of intelligence
The new ai is general and rigorous, idsia
Toward a theory of intelligence,RAND
</p>

<p>
A system responds to a stimulus.
Define: a system is <i>adapting</i> to a stimulus if the same stimulus level elicits decreasing response level from the system.
The stimulus level has to be increased to maintain the response level.
</p>

<p>
Is learning = adapting?
Is intelligence = adaptiveness?
</p>
</div>
</div>

<div id="outline-container-org78ce8a7" class="outline-3">
<h3 id="others"><a id="org78ce8a7"></a><span class="section-number-3">1.4</span> Others</h3>
<div class="outline-text-3" id="text-others">
<ul class="org-ul">
<li>What are some expository works in AI?

<ul class="org-ul">
<li><a href="https://www.sciencedirect.com/science/article/pii/S1574013717300606">The evolution of sentiment analysis&#x2014;A review of research topics, venues, and top cited papers</a></li>
</ul></li>

<li>What are the trends in AI?

<ul class="org-ul">
<li><a href="https://twitter.com/michael_nielsen/status/983502409325395969">Michael Nielsen's tweet</a>:
"I meet lots of people who tell me fatalistically (&amp; often despondently) that it's near impossible to do important work on neural nets today, unless you have huge compute and huge data sets."

<ul class="org-ul">
<li><a href="https://arxiv.org/abs/1712.00409">Deep Learning Scaling is Predictable, Empirically</a></li>
</ul></li>
</ul></li>

<li>Should we read this?

<ul class="org-ul">
<li><a href="http://www.cs.cmu.edu/~16831-f12/notes/F11/16831_lecture15_shorvath.pdf">Boosting: Gradient descent in function space</a></li>
<li><a href="http://alessio.guglielmi.name/res/cos/">Alessio Guglielmi's deep inference</a></li>
<li><a href="https://arxiv.org/abs/1412.1044">Problem theory, Ramón Casares</a></li>
</ul></li>

<li>EcoBot is a robot that can feed itself.

<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/EcoBot">Wikipedia: EcoBot</a>:
"a class of energetically autonomous robots that can remain self-sustainable
by collecting their energy from material, mostly waste matter, in the environment"</li>
</ul></li>

<li><a href="https://www.sciencedaily.com/releases/2016/04/160427081533.htm">A single-celled organism capable of learning</a>: protists may learn by habituation</li>
<li>Selected threads from /r/artificial:

<ul class="org-ul">
<li><a href="https://www.reddit.com/r/artificial/comments/8begcv/what_are_some_of_the_best_books_on_artificial/">What are some of the best books on AI/ML?</a></li>
<li><a href="https://www.reddit.com/r/artificial/comments/8bzrmd/math_phd_want_to_learn_more_about_ai_what_to_read/">Math PhD. Want to learn more about AI. What to read?</a></li>
</ul></li>

<li>What is so bad about human extinction?

<ul class="org-ul">
<li>If you are nihilist, then there is nothing inherently bad about human extinction.</li>
</ul></li>

<li>What is the question?</li>
<li>How do we make an AI?</li>
<li>How do we create a seed AI?</li>
<li>History questions:

<ul class="org-ul">
<li>Why was Raymond J. Solomonoff \cite{SolAlpProb2011, GacsVitanyiSolomonoff} interested in predicting sequences of bits?
What was he interested in?
What was he trying to do?</li>
</ul></li>

<li>Mathematical spaces

<ul class="org-ul">
<li>What is a metric?</li>
<li>What is a norm?</li>
<li>What is a measure?</li>
<li><a href="https://en.wikipedia.org/wiki/Space_(mathematics)">https://en.wikipedia.org/wiki/Space_(mathematics)</a>#Three<sub>taxonomic</sub><sub>ranks</sub></li>
<li><a href="https://en.wikipedia.org/wiki/Topological_space#Classification_of_topological_spaces">https://en.wikipedia.org/wiki/Topological_space#Classification_of_topological_spaces</a></li>
<li><a href="https://en.wikipedia.org/wiki/Functional_analysis">https://en.wikipedia.org/wiki/Functional_analysis</a>

<ul class="org-ul">
<li>What is a Hilbert space?</li>
<li>What is a Banach space?</li>
<li>What is a Sobolev space?</li>
<li>What is a measure?

<ul class="org-ul">
<li>What is a Lebesgue measure?

<ul class="org-ul">
<li>What is an Lp space?

<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/Lp_space#Lp_spaces">Wikipedia: Lp space</a></li>
<li>How is it pronounced?

<ul class="org-ul">
<li>"Lebesgue space with $p$-norm"</li>
</ul></li>
</ul></li>

<li>What is a small lp space?</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org8e68eeb" class="outline-3">
<h3 id="non-prioritized-questions"><a id="org8e68eeb"></a><span class="section-number-3">1.5</span> Non-prioritized questions</h3>
<div class="outline-text-3" id="text-non-prioritized-questions">
<ul class="org-ul">
<li>What is AI? Why should I care?

<ul class="org-ul">
<li>AI is the way for us to become gods.</li>
</ul></li>

<li>What is the relationship between AI and ML?

<ul class="org-ul">
<li>ML is a subset of AI.

<ul class="org-ul">
<li>Then what is the rest of AI that is not ML?

<ul class="org-ul">
<li>Ethics? Philosophy? Rule systems?</li>
<li><a href="https://ai.stackexchange.com/questions/35/what-is-the-difference-between-artificial-intelligence-and-machine-learning">AI SE 35: What is the difference between artificial intelligence and machine learning?</a></li>
<li>What is intelligence without learning?
Non-adaptive intelligence? Static intelligence?</li>
</ul></li>
</ul></li>
</ul></li>

<li>What is a cyborg?</li>
<li>If human goal function is survival, then why exists suicide?

<ul class="org-ul">
<li>Evolutionary noise?</li>
</ul></li>
</ul>

<p>
<a href="https://en.wikipedia.org/wiki/Universal_Darwinism">https://en.wikipedia.org/wiki/Universal_Darwinism</a>
</p>
</div>
</div>

<div id="outline-container-org030a1ce" class="outline-3">
<h3 id="how-might-we-build-a-seed-ai"><a id="org030a1ce"></a><span class="section-number-3">1.6</span> How might we build a seed AI?</h3>
<div class="outline-text-3" id="text-how-might-we-build-a-seed-ai">
<ul class="org-ul">
<li>Use off-the-shelf computers.</li>
<li>Use supercomputers.</li>
<li>Use clusters.</li>
<li>Use computers over the Internet.</li>
<li>Raise an AI like raising a child.</li>
<li>Evolve a system. Create an environment with selection pressure. Run it long enough.

<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/Evolutionary_robotics">WP: Evolutionary robotics</a></li>
<li><a href="https://en.wikipedia.org/wiki/Evolutionary_computation">WP: Evolutionary computation</a></li>
</ul></li>

<li>What is TensorFlow? Keras? CNTK? Theano?

<ul class="org-ul">
<li>The building blocks of AI? Standardized AI components?</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orge128bbf" class="outline-3">
<h3 id="guesses"><a id="orge128bbf"></a><span class="section-number-3">1.7</span> Guesses</h3>
<div class="outline-text-3" id="text-guesses">
<p>
In the future, there are only two kinds of jobs:
telling machines to do things,
and being told to do things by machines.
</p>
</div>
</div>

<div id="outline-container-orgf72e919" class="outline-3">
<h3 id="undigested-information"><a id="orgf72e919"></a><span class="section-number-3">1.8</span> Undigested information</h3>
<div class="outline-text-3" id="text-undigested-information">
<ul class="org-ul">
<li><a href="https://kevinbinz.com/2017/08/13/ml-five-tribes/">kevinbinz.com: Five Tribes of Machine Learning</a>,
part of <a href="https://kevinbinz.com/2017/05/09/sequence-machine-learning/">machine learning sequence</a>,
some contents from Pedro Domingos's book "The master algorithm"</li>
<li><a href="http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html">Introducing state of the art text classification with universal language models</a></li>
<li>Summary of Pedro Domingos's book "The master algorithm"

<ul class="org-ul">
<li>Sparse autoencoders (p. 116).</li>
<li>"A nugget of knowledge so incontestable, so fundamental, that we can build all induction on top of it" (p. 64) in Chapter 9.</li>
<li>Induction is the inverse of deduction,
as subtraction is the inverse of addition. (Is this a quote from the book?)</li>
<li>EM (expectation maximization) algorithm (p. 209).</li>
<li>Metalearning (p. 237).</li>
<li>A classifier that classifies by combining the output of subclassifiers.</li>
<li><a href="http://homes.cs.washington.edu/~pedrod/papers/mlj05.pdf">Markov logic network</a> (p. 246) named <a href="Alchemy">http://alchemy.cs.washington.edu/</a> (p. 250)</li>
</ul></li>

<li>Harvard University the graduate school of arts and sciences:
<a href="http://sitn.hms.harvard.edu/flash/2017/history-artificial-intelligence/">Rockwell Anyoha: History of AI</a></li>
<li><a href="http://jacques.pitrat.pagesperso-orange.fr/">Jacques Pitrat</a> and his CAIA,
bootstrapping AI with AI.</li>
<li><a href="http://www.hutter1.net/ai/uaibook.htm">Marcus Hutter book: Universal Artificial Intelligence: Sequential Decisions based on Algorithmic Probability</a>
and the <a href="http://www.hutter1.net/ai/suaibook.pdf">slides</a>.</li>
<li><a href="http://math.bu.edu/people/mkon/V5Fin.pdf">Mark A. Kon, Louise A. Raphael, Daniel A. Williams:
Extending Girosi's approximation estimates for functions in Sobolev spaces via statistical learning theory</a>

<ul class="org-ul">
<li>"Girosi [8] established an interesting connection between statistical learning theory
(SLT) and approximation theory, showing that SLT methods can be used to
prove results of a purely approximation theoretic nature."</li>
</ul></li>

<li>Speech synthesizer using hidden Markov model?
Someone must have done it. Find the paper.</li>
<li>ISIR (International Society for Intelligence Research)
human intelligence research <a href="http://www.isironline.org/resources/teaching-pages/">teaching pages</a>.</li>
<li><a href="https://en.wikipedia.org/wiki/Artificial_life">https://en.wikipedia.org/wiki/Artificial_life</a></li>
<li>What is the simplest life form? (2008)
<a href="https://www.quora.com/What-is-the-simplest-life-form">https://www.quora.com/What-is-the-simplest-life-form</a></li>
<li><a href="https://stats.stackexchange.com/questions/142906/what-does-pac-learning-theory-mean">https://stats.stackexchange.com/questions/142906/what-does-pac-learning-theory-mean</a></li>
<li><a href="https://brenocon.com/blog/2008/12/statistics-vs-machine-learning-fight/">https://brenocon.com/blog/2008/12/statistics-vs-machine-learning-fight/</a>

<ul class="org-ul">
<li>YC thread for that <a href="https://news.ycombinator.com/item?id=4927168">https://news.ycombinator.com/item?id=4927168</a></li>
</ul></li>

<li><a href="https://www.quora.com/What-are-the-most-important-foundational-papers-in-artificial-intelligence-machine-learning">Quora: What are the most important, foundational papers in artificial intelligence/machine learning?</a></li>
<li>JAIR (Journal of Artificial Intelligence Research):
<a href="https://www.jair.org/index.php/jair/navigationMenu/view/IJCAIJAIR">IJCAI-JAIR awards</a></li>
<li>Schmidhuber, <a href="http://people.idsia.ch/~juergen/fastestuniverse.pdf">The Fastest Way of Computing All Universes</a></li>
<li><a href="http://raysolomonoff.com/dartmouth/">Dartmouth AI archives</a>

<ul class="org-ul">
<li><a href="http://raysolomonoff.com/publications/indinf56.pdf">Solomonoff, "An inductive inference machine"</a></li>
</ul></li>

<li>Shane Legg, Joel Veness: algorithmic intelligence quotient

<ul class="org-ul">
<li><a href="https://github.com/mathemajician/AIQ">https://github.com/mathemajician/AIQ</a></li>
<li>An Approximation of the Universal Intelligence Measure
by Shane Legg and Joel Veness, 2011</li>
</ul></li>

<li><a href="https://courses.cs.washington.edu/courses/csep590/06au/projects/history-ai.pdf">History of AI</a>, University of Washington, History of Computing, CSEP 590A</li>
<li><a href="https://en.wikipedia.org/wiki/Timeline_of_artificial_intelligence">WP: Timeline of AI</a></li>
<li><a href="https://www.quantamagazine.org/why-self-taught-artificial-intelligence-has-trouble-with-the-real-world-20180221/">https://www.quantamagazine.org/why-self-taught-artificial-intelligence-has-trouble-with-the-real-world-20180221/</a></li>
<li><a href="http://news.mit.edu/2010/ai-unification">http://news.mit.edu/2010/ai-unification</a></li>
<li><a href="http://airesearch.com/">http://airesearch.com/</a></li>
<li><a href="https://theconversation.com/understanding-the-four-types-of-ai-from-reactive-robots-to-self-aware-beings-67616">https://theconversation.com/understanding-the-four-types-of-ai-from-reactive-robots-to-self-aware-beings-67616</a></li>
<li><a href="https://artificialintelligence.id/">https://artificialintelligence.id/</a></li>
<li><a href="https://www.asianscientist.com/2017/09/academia/indonesia-ai-nvidia-binus-kinetica/">https://www.asianscientist.com/2017/09/academia/indonesia-ai-nvidia-binus-kinetica/</a></li>
<li><a href="https://arxiv.org/abs/1206.5533">Practical recommendations for gradient-based training of deep architectures</a></li>
<li><a href="https://arxiv.org/abs/1604.06737">Entity Embeddings of Categorical Variables</a></li>
<li>Google Colab</li>
<li><a href="https://qz.com/1172431/artificial-intelligence-ai-should-be-raised-like-children-not-computers/">https://qz.com/1172431/artificial-intelligence-ai-should-be-raised-like-children-not-computers/</a></li>
<li>RNN, LSTM, GRU

<ul class="org-ul">
<li>RNN is recurrent neural network.</li>
<li>LSTM is a kind of RNN.</li>
<li>GRU is a kind of RNN.</li>
<li><a href="https://jhui.github.io/2017/03/15/RNN-LSTM-GRU/">https://jhui.github.io/2017/03/15/RNN-LSTM-GRU/</a></li>
</ul></li>

<li><a href="http://web.mit.edu/tslvr/www/lessons_two_years.html">http://web.mit.edu/tslvr/www/lessons_two_years.html</a></li>
<li><a href="https://gallery.mailchimp.com/dc3a7ef4d750c0abfc19202a3/files/93e40657-1adb-4891-94ad-c65dda68061f/Ng_MLY01_02.pdf">https://gallery.mailchimp.com/dc3a7ef4d750c0abfc19202a3/files/93e40657-1adb-4891-94ad-c65dda68061f/Ng_MLY01_02.pdf</a></li>
<li><a href="https://www.reddit.com/r/MachineLearning/comments/73n9pm/d_confession_as_an_ai_researcher_seeking_advice/#bottom-comments">https://www.reddit.com/r/MachineLearning/comments/73n9pm/d_confession_as_an_ai_researcher_seeking_advice/#bottom-comments</a></li>
<li><a href="http://www.inf.ed.ac.uk/teaching/courses/mlpr/2017/notes/w6b_netflix_prize.html">netflix prize, part of MLPR class notes</a></li>
<li>Scott M. Lundberg, Su-In Lee: A Unified Approach to Interpreting Model Predictions

<ul class="org-ul">
<li><a href="http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf">http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf</a></li>
<li><a href="https://github.com/slundberg/shap">https://github.com/slundberg/shap</a></li>
</ul></li>

<li><a href="https://www.datascience.com/blog/introduction-to-bayesian-inference-learn-data-science-tutorials">datascience.com: Introduction to Bayesian Inference</a></li>
<li><a href="http://www.fc.uaem.mx/~bruno/material/brooks_87_representation.pdf">1987, Intelligence without representation, Rodney A. Brooks</a></li>
<li><a href="http://colah.github.io/posts/2015-08-Backprop/">colah.github.io: Backprop</a></li>
<li>google search "ai theory research"</li>
<li><a href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.2.4835">2002, PhotoTOC: Automatic Clustering for Browsing Personal Photographs, by John C. Platt, Mary Czerwinski, Brent A. Field</a></li>
<li>philosophy of learning

<ul class="org-ul">
<li><a href="http://learning.media.mit.edu/content/publications/EA.Piaget%20_%20Papert.pdf">Piaget's constructivism vs Papert's constructionism</a>, Edith Ackermann</li>
</ul></li>

<li><a href="https://arxiv.org/abs/1508.01084">2015, Deep Convolutional Networks are Hierarchical Kernel Machines</a></li>
<li><a href="https://www.youtube.com/watch?v=F5Z52jl4yHQ">Michio Kaku: Who is right about A.I.: Mark Zuckerberg or Elon Musk?</a></li>
<li><a href="https://stats.stackexchange.com/questions/104385/assigning-meaningful-cluster-name-automatically">Stats SE 104385: text processing: assigning meaningful cluster name automatically</a></li>
<li>The mathematics of deep learning (a website)</li>
<li>Can AI be used to upscale old audio/video recordings? Fix deteriorated pictures, films, documents? Color old pictures, photos, films?
"Modernize" past artifacts? Digital restoration of archives?</li>
<li>brain-computer interface

<ul class="org-ul">
<li>pop science

<ul class="org-ul">
<li><a href="https://www.youtube.com/watch?v=P29EXskk9oU">How Brain Waves Can Control Physical Objects</a></li>
</ul></li>
</ul></li>

<li>machine learning

<ul class="org-ul">
<li>confusion matrix</li>
<li>algebra of words

<ul class="org-ul">
<li><a href="https://medium.com/@erushton214/a-simple-spell-checker-built-from-word-vectors-9f28452b6f26">https://medium.com/@erushton214/a-simple-spell-checker-built-from-word-vectors-9f28452b6f26</a></li>
</ul></li>

<li><a href="https://www.datasciencecentral.com/profiles/blogs/crisp-dm-a-standard-methodology-to-ensure-a-good-outcome">https://www.datasciencecentral.com/profiles/blogs/crisp-dm-a-standard-methodology-to-ensure-a-good-outcome</a></li>
<li><a href="http://www.inference.vc/untitled/">ML beyond Curve Fitting: An Intro to Causal Inference and do-Calculus</a></li>
</ul></li>

<li>deepmind wavenet</li>
<li><a href="https://openreview.net/pdf?id=ByldLrqlx">deepcoder: learning to write programs</a></li>
<li>Ramblings, opinions, guesses, hypotheses, conjectures, speculations

<ul class="org-ul">
<li>AI is approximation (or constrained optimization?) in Sobolev spaces (or ( L<sup>p</sup>(\Real) ) spaces?)?</li>
<li>Intelligent agents are only possible if the world they live in is structured.
If the laws of physics randomly change over time,
then intelligent agents are unlikely.</li>
<li>We should merge machine learning, probability, and statistics?

<ul class="org-ul">
<li><a href="http://en.wikipedia.org/wiki/Recursive_self_improvement">WP:Recursive self-improvement</a></li>
</ul></li>

<li>World = agent + environment.
Environment is everything that the agent does not control directly.
The body of an agent is part of the environment, not of the agent.</li>
</ul></li>

<li><a href="http://dl.acm.org/citation.cfm?id=2567715">Dimension independent similarity computation (DISCO)</a></li>
<li><a href="http://www.jair.org/">Journal of artificial intelligence research</a> (open access)</li>
<li><a href="https://arxiv.org/abs/1802.08195">Adversarial Examples that Fool both Human and Computer Vision</a>,
from <a href="https://www.youtube.com/watch?v=AbxPbfODGcs">two minute papers 241</a>.</li>
<li><a href="https://www.semanticscholar.org/paper/Machine-Theory-of-Mind-Rabinowitz-Perbet/4a48d7528bf1f81f48be8a644ffb1bcc08f1b2c5">Machine theory of mind</a></li>
<li>Ilias Diakonikolas, Daniel Kane and Alistair Stewart. Optimal Learning via the Fourier Transform for Sums of Independent Integer Random Variables</li>
<li><a href="https://en.m.wikipedia.org/wiki/List_of_important_publications_in_computer_science#Machine_learning">https://en.m.wikipedia.org/wiki/List_of_important_publications_in_computer_science#Machine_learning</a></li>
<li><a href="https://arxiv.org/abs/1704.07441">Detecting English Writing Styles For Non Native Speakers</a></li>
<li>"Hicklin envisaged that learning resulted from a dynamic equilibrium between information acquisition and loss."
(<a href="https://onlinelibrary.wiley.com/doi/pdf/10.1002/tea.3660210910">Mathematical modeling of learning, Peter F. W. Preece</a>, 1984)</li>
<li>AI research tries to make a system that can optimize a wide variety of goal functions?</li>
<li><a href="https://cs.nyu.edu/~mohri/mlbook/">Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar; book; "Foundations of machine learning"</a></li>
<li><a href="http://bigthink.com/videos/the-top-3-supplements-for-surviving-the-singularity">http://bigthink.com/videos/the-top-3-supplements-for-surviving-the-singularity</a></li>
<li><a href="https://google.github.io/CausalImpact/CausalImpact.html">https://google.github.io/CausalImpact/CausalImpact.html</a></li>
<li>intelligence testing

<ul class="org-ul">
<li><a href="https://www.youtube.com/watch?v=8YWjSQHfV5U">YT:Jordan Peterson - Example IQ questions and what Career/job fits your IQ</a>

<ul class="org-ul">
<li>problem: no job for people with IQ below 87?</li>
<li><a href="https://www.reddit.com/r/JordanPeterson/comments/84qmsj/source_of_83_iq_minimum_for_the_us_military/">R:source for soldier minimum IQ requirement of 85</a></li>
<li><a href="https://en.wikipedia.org/wiki/Fluid_and_crystallized_intelligence">WP:Fluid and crystallized intelligence</a></li>
<li><a href="https://en.wikipedia.org/wiki/Raven's_Progressive_Matrices">WP:Raven's progressive matrices</a>
is a language-neutral visual test for fluid intelligence?</li>
</ul></li>
</ul></li>

<li><a href="https://www.youtube.com/watch?v=GdTBqBnqhaQ">YT:4 Experiments Where the AI Outsmarted Its Creators | Two Minute Papers #242</a></li>
<li><a href="https://arxiv.org/abs/1509.06569">Tensorizing Neural Networks</a></li>
<li><a href="https://arxiv.org/abs/1502.02367">Gated Feedback Recurrent Neural Networks</a></li>
<li>no information <a href="http://syntience.com/">http://syntience.com/</a></li>
<li><a href="https://www.youtube.com/watch?v=b_6-iVz1R0o">The pattern behind self-deception | Michael Shermer</a>:
patternicity, agenticity, pattern over-recognition, false positive, false negative

<ul class="org-ul">
<li>"false positive" is a much better name than "type 1 error"</li>
</ul></li>

<li>expected 2018, draft book, "Model-based machine learning", <a href="http://www.mbmlbook.com/">html</a></li>
<li>vision (making machines see)

<ul class="org-ul">
<li>Jim Bednar, <a href="http://homepages.inf.ed.ac.uk/jbednar/demos.html">Orientation Perception Demos</a></li>
</ul></li>

<li><a href="https://en.wikipedia.org/wiki/Bayesian_approaches_to_brain_function">https://en.wikipedia.org/wiki/Bayesian_approaches_to_brain_function</a></li>
<li><a href="https://www.youtube.com/watch?v=MvFABFWPBrw">DeepMind Has A Superhuman Level Quake 3 AI Team - YouTube</a>

<ul class="org-ul">
<li>Moby Motion's comment: "Really exciting because of the sparse internal rewards and long term planning. A step towards AI agents that are useful in real life."</li>
</ul></li>

<li>2018 AI is like autistic savants.
They perform one task exceptionally well, but they are bad at everything else.

<ul class="org-ul">
<li>2018, <a href="https://www.youtube.com/watch?v=eSaShQbUJTQ">DeepMind's AI Takes An IQ Test - YouTube</a></li>
</ul></li>

<li>AI

<ul class="org-ul">
<li>2007, article, "Self-taught Learning: Transfer Learning from Unlabeled Data", <a href="https://cs.stanford.edu/people/ang/papers/icml07-selftaughtlearning.pdf">pdf</a></li>
<li><a href="https://en.wikipedia.org/wiki/Category:Open-source_artificial_intelligence">https://en.wikipedia.org/wiki/Category:Open-source_artificial_intelligence</a></li>
<li><a href="https://en.wikipedia.org/wiki/Commonsense_knowledge_(artificial_intelligence)">https://en.wikipedia.org/wiki/Commonsense_knowledge_(artificial_intelligence)</a></li>
<li>2010, article, <a href="https://news.mit.edu/2010/ai-unification">A grand unified theory of AI - MIT News</a></li>
<li>2016, article, <a href="https://ai100.stanford.edu/2016-report/section-i-what-artificial-intelligence/ai-research-trends">AI Research Trends - One Hundred Year Study on Artificial Intelligence (AI100)</a></li>
<li>sequence learning?

<ul class="org-ul">
<li><a href="https://devblogs.nvidia.com/deep-learning-nutshell-sequence-learning/">https://devblogs.nvidia.com/deep-learning-nutshell-sequence-learning/</a></li>
<li><a href="https://en.wikipedia.org/wiki/Sequence_learning">https://en.wikipedia.org/wiki/Sequence_learning</a></li>
</ul></li>

<li>AI perception of time?</li>
</ul></li>

<li><a href="https://www.quora.com/Does-the-human-brain-have-an-internal-language">https://www.quora.com/Does-the-human-brain-have-an-internal-language</a>

<ul class="org-ul">
<li>mereological fallacy, confusing the part and the whole</li>
</ul></li>

<li><a href="https://www.quora.com/Is-the-human-brain-analog-or-digital">https://www.quora.com/Is-the-human-brain-analog-or-digital</a>
<a href="https://en.wikipedia.org/wiki/Mereological_essentialism">https://en.wikipedia.org/wiki/Mereological_essentialism</a></li>
<li>machine learning

<ul class="org-ul">
<li><a href="https://github.com/Avik-Jain/100-Days-Of-ML-Code">Avik-Jain/100-Days-Of-ML-Code: 100 Days of ML Coding</a></li>
</ul></li>

<li>Justifying consciousness using evolution?

<ul class="org-ul">
<li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4122207/">The biological function of consciousness</a></li>
<li><a href="https://www.quora.com/How-does-sentience-benefit-survival-and-why-is-it-developed">How does sentience benefit survival and why is it developed? - Quora</a></li>
</ul></li>

<li><a href="https://www.quora.com/How-do-I-publish-artificial-intelligence-research-if-I-am-not-currently-in-academia-or-an-industry-research-setting">https://www.quora.com/How-do-I-publish-artificial-intelligence-research-if-I-am-not-currently-in-academia-or-an-industry-research-setting</a></li>
<li><a href="https://www.quora.com/How-does-life-fight-against-entropy">How does life fight against entropy? - Quora</a></li>
<li>Life and entropy

<ul class="org-ul">
<li><a href="https://www.quora.com/How-does-life-fight-against-entropy">How does life fight against entropy? - Quora</a></li>
<li><a href="https://en.wikipedia.org/wiki/Entropy_and_life">WP:Entropy and life</a></li>
</ul></li>

<li>Making machine understand human languages

<ul class="org-ul">
<li><a href="https://blogs.microsoft.com/ai/microsoft-creates-ai-can-read-document-answer-questions-well-person/">Microsoft creates AI that can read a document and answer questions about it as well as a person - The AI Blog</a></li>
</ul></li>

<li><a href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html">A (Long) Peek into Reinforcement Learning</a></li>
<li>Competitions

<ul class="org-ul">
<li>Kaggle: get paid to solve machine learning problems.</li>
</ul></li>

<li>HLearn: a machine learning library for Haskell \cite{izbicki2013hlearn}</li>
<li><a href="https://dzone.com/articles/deep-dive-into-machine-learning">Deep Dive Into Machine Learning - DZone AI</a></li>
<li><a href="https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf">https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf</a></li>
<li><a href="https://github.com/keras-team/keras">keras-team/keras: Deep Learning for humans</a></li>
<li><a href="http://cs230.stanford.edu/proj-spring-2018.html">CS230: Deep Learning - Projects</a></li>
<li><a href="http://jonbho.net/2014/09/25/defining-intelligence/">http://jonbho.net/2014/09/25/defining-intelligence/</a></li>
<li><a href="https://github.com/HuwCampbell/grenade">HuwCampbell/grenade: Deep Learning in Haskell</a></li>
<li><a href="http://www.randomhacks.net/2007/03/03/smart-classification-with-haskell/">Smart classification using Bayesian monads in Haskell - Random Hacks</a></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orga2222b5" class="outline-2">
<h2 id="orga2222b5"><span class="section-number-2">2</span> intmeta.md</h2>
<div class="outline-text-2" id="text-2">
<p>
This page is about AI research, not about AI.
</p>
</div>

<div id="outline-container-orgf42ba72" class="outline-3">
<h3 id="questions"><a id="orgf42ba72"></a><span class="section-number-3">2.1</span> Questions</h3>
<div class="outline-text-3" id="text-questions">
<ul class="org-ul">
<li>What is the best place to do AI research?</li>
</ul>
</div>
</div>

<div id="outline-container-org3237c81" class="outline-3">
<h3 id="how-can-i-become-an-ai-researcher"><a id="org3237c81"></a><span class="section-number-3">2.2</span> How can I become an AI researcher?</h3>
<div class="outline-text-3" id="text-how-can-i-become-an-ai-researcher">
<ul class="org-ul">
<li>Where are new results announced?

<ul class="org-ul">
<li><a href="https://en.m.wikipedia.org/wiki/Portal:Artificial_intelligence">Wikipedia AI Portal</a></li>
<li>Reddit <a href="https://www.reddit.com/r/artificial/">/r/artificial</a></li>
</ul></li>

<li>Where is more information?

<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/Artificial_intelligence">Wikipedia: Artificial intelligence</a></li>
</ul></li>

<li>Who are the researchers?

<ul class="org-ul">
<li>See also <a href="https://www.quora.com/Who-is-leading-in-AI-research-among-big-players-like-IBM-Google-Facebook-Apple-and-Microsoft">Quora: Who is leading in AI research among big players like
IBM, Google, Facebook, Apple, and Microsoft?</a>

<ul class="org-ul">
<li>Google Brain, OpenAI, FAIR (Facebook AI Research), Microsoft Research, IBM Research</li>
</ul></li>

<li>Geoffrey Hinton,
<a href="http://www.cs.toronto.edu/~hinton/">UToronto page</a>,
<a href="https://www.reddit.com/r/MachineLearning/comments/2lmo0l/ama_geoffrey_hinton/">Reddit AMA</a>,
<a href="https://www.semanticscholar.org/author/Geoffrey-E.-Hinton/1695689">Semantic Scholar influence graph</a>

<ul class="org-ul">
<li>He is trying to find out how the brain works.</li>
<li>The idea: If a learning algorithm works on machines, then it might have something to do with how brains work.</li>
<li>More interested in physical explanation of how the brain works.
Physics first, math second, although his math is OK.</li>
</ul></li>

<li>Yann LeCun</li>
<li>Jürgen Schmidhuber</li>
<li>Pedro Domingos</li>
<li>Demis Hassabis

<ul class="org-ul">
<li>What is his focus?</li>
</ul></li>

<li>Pamela McCorduck, AI historian

<ul class="org-ul">
<li>2004 anniversary edition of her 1979 book <a href="http://www.pamelamc.com/html/machines_who_think.html">"Machines who think"</a></li>
</ul></li>

<li>Who else? There are lots of people.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org535668d" class="outline-3">
<h3 id="how-are-others-works-progressing"><a id="org535668d"></a><span class="section-number-3">2.3</span> How are others' works progressing?</h3>
<div class="outline-text-3" id="text-how-are-others-works-progressing">
<ul class="org-ul">
<li>How is <a href="https://homes.cs.washington.edu/~pedrod/">Pedro Domingos</a>'s progress of finding the master algorithm unifying the five tribes?

<ul class="org-ul">
<li>Markov logic network unifies probabilists and logicians.

<ul class="org-ul">
<li>How about the other three tribes?</li>
</ul></li>

<li>Hume's question: How do we justify generalization? Why does generalization work?

<ul class="org-ul">
<li>Does Wolpert answer that in "no free lunch theorem"?

<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/No_free_lunch_theorem">Wikipedia: No free lunch theorem</a></li>
</ul></li>

<li>I think induction works because our Universe
happens to have a structure that is amenable to induction.

<ul class="org-ul">
<li>If induction doesn't work, and evolution is true,
then we would have gone extinct long ago, wouldn't we?

<ul class="org-ul">
<li>What structure is that?</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org64202b5" class="outline-2">
<h2 id="org64202b5"><span class="section-number-2">3</span> intwhat.md</h2>
<div class="outline-text-2" id="text-3">
</div>
<div id="outline-container-org381a7dc" class="outline-3">
<h3 id="intelligence-is-an-ordering-2018-04-26"><a id="org381a7dc"></a><span class="section-number-3">3.1</span> Intelligence is an ordering (2018-04-26)</h3>
<div class="outline-text-3" id="text-intelligence-is-an-ordering-2018-04-26">
<p>
This idea goes back at least to 2005 in [Hutter2005Book] (p. 2).
</p>

<p>
Intelligence is an <i>ordering</i> of systems.
</p>

<p>
An order is a transitive antisymmetric relation.
</p>

<p>
<a href="https://brocku.ca/MeadProject/sup/Boring_1923.html">Edwin Boring in 1923</a>
proposed that we start out by defining intelligence as what intelligence tests measure
"until further scientific observation allows us to extend the definition".
That definition makes sense mathematically.
</p>

<p>
<i>Intelligence depends on its measurement</i>. Absolute intelligence doesn't exist.
</p>

<ul class="org-ul">
<li>The <i>behavior</i> of a system is whatever it exhibits that can be observed from outside.</li>
</ul>
</div>

<div id="outline-container-org51ea16c" class="outline-4">
<h4 id="how-do-we-decide-which-system-is-more-intelligent"><a id="org51ea16c"></a><span class="section-number-4">3.1.1</span> How do we decide which system is more intelligent?</h4>
<div class="outline-text-4" id="text-how-do-we-decide-which-system-is-more-intelligent">
<p>
Let \(A\) be a system.
</p>

<p>
Let \(B\) be a system.
</p>

<p>
Let \(T\) be a task.
</p>

<p>
Let \(S\) be a set of tasks.
</p>

<p>
Let \(T(A)\) denote how well system \(A\) does task \(T\).
This is a number.
Higher is better.
We can invent any measurement.
Our definition of "intelligence" is only as good as this measurement.
</p>

<p>
We say "\(A\) is <i>$T$-better</i> than \(B\)" iff \(T(A) > T(B)\).
</p>

<p>
We say "\(A\) <i>$S$-dominates</i> \(B\)" iff \(T(A) > T(B)\) for every task \(T \in S\).
</p>

<p>
We define "to be more $S$-intelligent than" to mean "to $S$-dominate".
</p>

<p>
The $S$-domination relation forms a partial order of all systems.
</p>

<p>
That is how.
</p>
</div>

<ol class="org-ol">
<li><a id="example"></a><a id="org53b905c"></a>Example<br />
<div class="outline-text-5" id="text-example">
<p>
Which is more intelligent, a dog or a rock?
</p>

<p>
That depends on the task set \(S\).
</p>

<p>
It's the rock if ( S = { \text{sit still} } ).
</p>

<p>
It's the dog if ( S = { \text{move around} } ).
</p>
</div>
</li>
</ol>
</div>
</div>

<div id="outline-container-orgdaf974a" class="outline-3">
<h3 id="intelligence-is-function-optimization-2018-04-27"><a id="orgdaf974a"></a><span class="section-number-3">3.2</span> Intelligence is function optimization (2018-04-27)</h3>
<div class="outline-text-3" id="text-intelligence-is-function-optimization-2018-04-27">
<p>
Let \(g\) be a goal function.
</p>

<p>
A system's $g$-intelligence is how well it optimizes \(g\).
</p>

<p>
What is "how well"?
</p>

<p>
Optimization (extremization) is either minimization or maximization.
</p>
</div>
</div>

<div id="outline-container-org6660f66" class="outline-3">
<h3 id="what-is-a-mathematical-theory-of-intelligence"><a id="org6660f66"></a><span class="section-number-3">3.3</span> What is a mathematical theory of intelligence?</h3>
<div class="outline-text-3" id="text-what-is-a-mathematical-theory-of-intelligence">
<ul class="org-ul">
<li>In 2007, on page 12, in the paper <a href="https://arxiv.org/pdf/0712.3329.pdf">Universal intelligence: a definition of machine intelligence</a>,
Shane Legg and Marcus Hutter wrote,
"Intelligence measures an agent's ability to achieve goals in a wide range of environments,"
and then they formalized them.
Here I try another formalization.

<ul class="org-ul">
<li>Let \(E\) be a set of <i>environments</i>.</li>
<li>Let \(G : E \to \Real\) be a <i>goal function</i>.
The value of \(G(e)\) measures how well the agent performs in environment \(e\).</li>
<li>The <i>intelligence</i> of the agent <i>with respect to \(G\) across \(E\)</i> is \(\int_E G\).</li>
<li>A <i>performance</i> consists of an agent and an environment.</li>
<li>Assumption: The agent cannot modify \(G\).</li>
<li>Behavior is a function taking an environment and outputing something.</li>
<li>Intelligence is <i>relative</i> to \(G\) and \(E\): <i>goal</i> and <i>environment</i>.</li>
<li>If we see longevity as intelligence test,
then an illiterate farmer who lives to 80
is more intelligent than a scientist who dies at 20,
but a rock that has been there for 100 years would even be more intelligent than the farmer.</li>
<li>If we see money as intelligence test,
then a corrupt politician who steals billions of dollars without getting caught
is more intelligent than a honest farmer who only has tens of thousands of dollars.</li>
</ul></li>

<li>Gaming the system is a sign of intelligence.
It is hard to design a goal function that gives the desired outcome without undesired side effects.</li>
<li>IQ tests are intelligence measures with small environment set.</li>
<li>Lifespan may be an intelligence measure with huge environment set.</li>
<li>A human can optimize <i>several</i> goal functions across the same environment set.
A human may be asked to clean a floor, to write a report, to run a company, to cook food,
and to find the quickest route between home and office,
and optimize them all.</li>
<li>Some goal functions for humans are (but perhaps not limited to):

<ul class="org-ul">
<li>Maximize happiness</li>
<li>Minimize pain</li>
<li>Optimize the level of a chemical in the brain</li>
<li>Optimize the time integral of such chemical</li>
<li>Maximize the chance of survival</li>
</ul></li>

<li>but I don't know the root goal function
that explains all those behaviors.</li>
<li>Where does the word "intelligence" come from? What is its etymology?

<ul class="org-ul">
<li>The word "intelligent" comes from a Latin word that means "to choose between"
(<a href="http://www.dictionary.com/browse/intelligent">Dictionary.com</a>).</li>
</ul></li>

<li>What are some mathematical definitions of intelligence?

<ul class="org-ul">
<li>"Intelligence measures an agent's ability to achieve goals in a wide range of environments."
[Legg2006][Legg2008]</li>
<li><a href="https://www.researchgate.net/publication/323203054_Defining_intelligence">Shour2018</a>:
"Defining intelligence as a rate of problem solving and using the concept
of network entropy enable measurement, comparison and calculation of
collective and individual intelligence and of computational capacity."</li>
<li>Tononi integrated information theory.
<a href="https://en.wikipedia.org/wiki/Integrated_information_theory">Wikipedia</a>.</li>
<li>Schmidhuber, Hutter, and team have used Solomonoff algorithmic probability
and Kolmogorov complexity to define a theoretically optimal predictor they call AIXI.

<ul class="org-ul">
<li>J"urgen Schmidhuber. <a href="http://www.idsia.ch/~juergen/newai/newai.html">Schmidhuber article</a>.</li>
<li><a href="http://www.cs.uic.edu/~piotr/cs594/Prashant-UniversalAI.pdf">Prashant's slides</a>.
These define "universal" and "optimal".</li>
</ul></li>

<li>Marcus Hutter approached intelligence from \emph{algorithmic} complexity theory (Solomonoff induction)
\cite{DefineMachIntel}.</li>
<li>Warren D. Smith approached intelligence from \emph{computational} complexity theory
(NP-completeness)
\cite{WdsIntel, WdsIntelSlide}</li>
</ul></li>

<li>What are other definitions of intelligence?

<ul class="org-ul">
<li>Legg and Hutter has collected definitions of intelligence in [Legg2007Collection].</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgfad23e7" class="outline-3">
<h3 id="what-is-learning"><a id="orgfad23e7"></a><span class="section-number-3">3.4</span> What is learning?</h3>
<div class="outline-text-3" id="text-what-is-learning">
<ul class="org-ul">
<li>There are so many ML algorithms.
What's the common thing?</li>
<li>Should I read these?

<ul class="org-ul">
<li><a href="https://medium.com/machine-learning-world/learning-path-for-machine-learning-engineer-a7d5dc9de4a4">How To Become A Machine Learning Engineer: Learning Path</a></li>
<li><a href="https://dzone.com/guides/artificial-intelligence-machine-learning-and-predi">https://dzone.com/guides/artificial-intelligence-machine-learning-and-predi</a></li>
</ul></li>

<li>What is the relationship between ML and statistical modeling?</li>
<li>How do we categorize ML algorithms?

<ul class="org-ul">
<li>Online vs offline

<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/Online_machine_learning">Wikipedia: Online machine learning</a></li>
</ul></li>

<li>Discrete-time model vs continuous-time model

<ul class="org-ul">
<li>LTI (linear time-invariant) systems</li>
</ul></li>

<li>Assemble answers from these sources:

<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/Machine_learning#Approaches">Wikipedia: Machine learning, approaches</a></li>
<li><a href="https://en.wikipedia.org/wiki/Outline_of_machine_learning#Machine_learning_algorithms">Wikipedia: Outline of machine learning, algorithms</a></li>
<li><a href="https://en.wikipedia.org/wiki/Outline_of_machine_learning#Machine_learning_methods">Wikipedia: Outline of machine learning, methods</a></li>
<li><a href="https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/">A tour of machine learning algorithms</a></li>
<li><a href="https://towardsdatascience.com/types-of-machine-learning-algorithms-you-should-know-953a08248861">Types of machine learning algorithms you should know</a></li>
<li><a href="https://stats.stackexchange.com/questions/214381/what-exactly-is-the-mathematical-definition-of-a-classifier-classification-alg">Stats SE 214381: mathematical definition of classifier</a></li>
<li><a href="https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/">Common machine learning algorithms</a></li>
</ul></li>
</ul></li>

<li>What is a neural network?

<ul class="org-ul">
<li>A <i>neuron</i> is a function in \(\Real^\infty \to \Real\).</li>
<li>A <i>neural network</i> layer is a function in \(\Real^\infty \to \Real^\infty\).</li>
<li>Why do neural networks work?

<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">Wikipedia: Universal approximation theorem</a></li>
</ul></li>
</ul></li>

<li>Statistical learning</li>
<li>What is backpropagation, from functional analysis point of view?</li>
<li>Who are AI/ML researchers and what are their focuses?

<ul class="org-ul">
<li>Does Geoffrey Hinton specialize in image recognition?</li>
</ul></li>

<li>What is the relationship between intelligence and compression?</li>
<li>Consider endofunctions of infinite-dimensional real tuple space.
That is, consider \(f, g : \Real^\infty \to \Real^\infty\).

<ul class="org-ul">
<li>What is the distance between them?</li>
</ul></li>

<li>Reductionistically, a brain can be thought as a function in \(\Real \to \Real^\infty \to \Real^\infty\).

<ul class="org-ul">
<li>The first parameter is time.</li>
<li>The second parameter is the sensor signals.</li>
<li>The output of the function is the actuator signals.</li>
<li>Can we model a brain by such
<a href="https://en.wikipedia.org/wiki/Functional_differential_equation">functional differential equation</a>
involving <a href="https://en.wikipedia.org/wiki/Functional_derivative">functional derivative</a>s?</li>
<li>\(\norm{f(t+h,x) - f(t,x)} = h \cdot g(t,x)\)</li>
<li>\(\norm{f(t+h) - f(t)} = h \cdot g(t)\)</li>
<li>It seems wrong. Abandon this path. See below.</li>
</ul></li>

<li>We model the input as a function \(x : \Real \to \Real^n\).</li>
<li>We model the output as a function \(y : \Real \to \Real^n\).

<ul class="org-ul">
<li>\(\norm{y(t+h) - y(t)} = h \cdot g(t)\)</li>
<li>\(y(t+h) - y(t) = h \cdot (dy)(t)\)</li>
<li>\(\norm{(dy)(t)} = g(t)\)

<ul class="org-ul">
<li>There are infinitely many \(dy\) that satisfies that. Which one should we choose?</li>
</ul></li>

<li>If \(y : \Real \to \Real^n\) then \(dy : \Real \to \Real^n\).</li>
</ul></li>

<li>A classifier is a function in \(\Real^\infty \to \Real\).</li>
<li>A control system snapshot is a function in \(\Real^\infty \to \Real^\infty\).</li>
<li>A control system is a function in \(\Real \to \Real^\infty \to \Real^\infty\).</li>
<li>How does \(F\) have memory if \(F(t) = \int_0^t f(x) ~ dx\)?</li>
</ul>

<p>
Why has AI mastered chess, but not real life?
Because chess search space is much smaller than real-life search space.
</p>
</div>
</div>

<div id="outline-container-org72562fb" class="outline-3">
<h3 id="what-is-ai"><a id="org72562fb"></a><span class="section-number-3">3.5</span> What is AI?</h3>
<div class="outline-text-3" id="text-what-is-ai">
<ul class="org-ul">
<li>In the 1950s, AI was whatever McCarthy et al. were doing.

<ul class="org-ul">
<li>"McCarthy coined the term 'artificial intelligence' in 1955, and organized the famous Dartmouth Conference in Summer 1956.
This conference started AI as a field."
(<a href="https://en.wikipedia.org/wiki/John_McCarthy_(computer_scientist)">WP: John McCarthy (computer scientist)</a>)</li>
<li><a href="https://en.wikipedia.org/wiki/Dartmouth_workshop">WP: Dartmouth workshop</a></li>
<li><a href="http://raysolomonoff.com/dartmouth/">Ray Solomonoff's Dartmouth archives</a></li>
</ul></li>

<li>What are AI approaches? How are we trying to make an AI?

<ul class="org-ul">
<li>Pedro Domingos categorizes AI approaches into five <i>tribes</i>:

<ul class="org-ul">
<li>symbolists (symbolic logic)</li>
<li>connectionists (neural networks)</li>
<li>evolutionaries (genetic algorithms)</li>
<li>bayesians (statistical learning, probabilistic inference)</li>
<li>analogizers (what is this?)</li>
</ul></li>
</ul></li>

<li>How do we measure intelligence? How do we measure the performance of a learning algorithm?

<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/Computational_learning_theory">Wikipedia: Computational learning theory</a>

<ul class="org-ul">
<li>What is the goal of computational learning theory?

<ul class="org-ul">
<li>"Give a rigorous, computationally detailed and plausible account of how learning can be done." [Angluin1992]</li>
</ul></li>

<li>"a subfield of Artificial Intelligence devoted to studying the design and analysis of machine learning algorithms"</li>
<li>What is a mathematical theory of learning?

<ul class="org-ul">
<li>What is learning?

<ul class="org-ul">
<li>2018-04-19: "To learn something" is to get better at it.
Usually learning uses experience.

<ul class="org-ul">
<li>What is the formal definition of "get better"?

<ul class="org-ul">
<li>Let there be a system.
Pick a task.
Pick a time interval.
Test the system several times throughout the time interval.
Let the test results be the sequence \(X = x_1, x_2, \ldots, x_n\).
We say that the system is <i>learning</i> the task in the time interval
iff \(x_1 < x_2 < \ldots < x_n\)
(that is iff \(X\) is a monotonically increasing sequence).</li>
<li>How do we formalize "get better" and "experience"?

<ul class="org-ul">
<li>"Get better" can be modeled by <i>monotonically increasing score</i></li>
<li>"Experience" can be modeled by a sequence</li>
</ul></li>
</ul></li>

<li>Is experience (past data) necessary for learning?
Are mistakes necessary for learning?</li>
</ul></li>

<li>Supervised learning is extrapolating a function from finite samples.
Usually, the function is high-dimensional, and the samples are few.</li>
<li>It is simple to measure learning success in perfect information games such as chess.
Chess also doesn't require any sensors and motors.</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
</div>


<div id="outline-container-org397b2bc" class="outline-2">
<h2 id="org397b2bc"><span class="section-number-2">4</span> approx.md</h2>
<div class="outline-text-2" id="text-4">
<p>
We are interested in approximation theory because we want to justify how neural networks work.
</p>

<ul class="org-ul">
<li>2016, article, "Deep vs. shallow networks: An approximation theory perspective", <a href="https://arxiv.org/abs/1608.03287">pdf available</a></li>
<li><a href="https://en.wikipedia.org/wiki/Explainable_Artificial_Intelligence">WP:Explainable Artificial Intelligence</a></li>
</ul>

<p>
We should begin by skimming the 1998 book "A Short Course on Approximation Theory" by N. L. Carothers (<a href="http://fourier.math.uoc.gr/~mk/approx1011/carothers.pdf">pdf</a>).
Then we should skim the 2017 lecture notes "Lectures on multivariate polynomial approximation" (<a href="http://www.math.unipd.it/~demarchi/MultInterp/LectureNotesMI.pdf">pdf</a>).
</p>

<p>
The phrase "x <i>approximates</i> y" means "x is <i>close</i> to y", which implies distance, which implies metric space.
</p>

<p>
How close is the approximation?
Suppose that the function \(g\) approximates the function \(f\) in interval \(I\).
Then:
</p>

<ul class="org-ul">
<li>The "approximation error at \(x\)" is \(g(x) - f(x)\).</li>
<li>The "maximum absolute error" is \(\max_{x \in I} \abs{g(x) - f(x)}\).</li>
</ul>

<p>
How do we measure the distance between two \(\Real \to \Real\) functions \(f\) and \(g\)?
There are several ways.
Which should we use?
</p>

<ul class="org-ul">
<li>The maximum norm, in interval \(I\) is \(\max_{x \in I} \abs{f(x) - g(x)}\).
This norm is also called uniform norm, supremum norm, Chebyshev norm, infinity norm, norm-infinity, $L<sub>&infin;</sub>$-norm.
Why is it called "uniform"?
<a href="https://en.wikipedia.org/wiki/Uniform_norm">WP:Uniform norm</a>.</li>
<li>What is this norm called? \(\int_{x \in I} [f(x)-g(x)]^2 ~ dx\).</li>
</ul>
</div>

<div id="outline-container-orgc75594f" class="outline-3">
<h3 id="other"><a id="orgc75594f"></a><span class="section-number-3">4.1</span> Other</h3>
<div class="outline-text-3" id="text-other">
<ul class="org-ul">
<li>Courses

<ul class="org-ul">
<li>2017, <a href="https://www.nada.kth.se/~olofr/Approx/">Approximation Theory, 7.5 ECTS</a></li>
<li>2012, syllabus, Drexel University, Math 680-002 (Approximation Theory), <a href="http://www.math.drexel.edu/~foucart/TeachingFiles/S12/Math680Syl.pdf">pdf</a></li>
<li>2002, <a href="http://math.ucdenver.edu/~aknyazev/teaching/02/5667/">MATH 5667-001: Introduction to Approximation Theory, CU-Denver, Fall 02</a>.</li>
</ul></li>

<li>Subfields of approximation theory

<ul class="org-ul">
<li>Classical approximation theory deals with univariate real functions \(\Real \to \Real\).</li>
<li>Multivariate approximation theory deals with multivariate real functions \(\Real^m \to \Real^n\).</li>
</ul></li>

<li>Scenarios

<ul class="org-ul">
<li>Suppose we want to approximate the function \(f\),
but we don't know the equation for \(f\);
we only have a few input-output samples.

<ul class="org-ul">
<li>Can we approximate \(f\)?</li>
<li>How do approximation and curve-fitting relate?</li>
</ul></li>
</ul></li>

<li>Overview

<ul class="org-ul">
<li>What is a multivariate polynomial?</li>
<li>Commonly conflated concepts

<ul class="org-ul">
<li>Approximation is not estimation.

<ul class="org-ul">
<li>Approximation converges.
Estimation doesn't, because the actual value is unknown.</li>
<li>Approximation doesn't guess.
Estimation does.</li>
<li>Approximation has error.
Estimation has uncertainty.</li>
<li>Approximation is part of analysis.
Estimation is part of statistics.</li>
</ul></li>
</ul></li>
</ul></li>

<li>The <i>uniform norm</i> is &#x2026;</li>
<li>Best approximation is &#x2026;</li>
<li>Uniform approximation is best approximation in uniform norm.</li>
<li><a href="https://en.wikipedia.org/wiki/Approximation_theory#Remez's_algorithm">https://en.wikipedia.org/wiki/Approximation_theory#Remez's_algorithm</a>

<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/Remez_algorithm">https://en.wikipedia.org/wiki/Remez_algorithm</a>

<ul class="org-ul">
<li>Inputs: a function, and an interval.</li>
<li>Output: an optimal polynomial approximating the input function in the input interval.</li>
</ul></li>
</ul></li>

<li>What are Bernstein polynomials?
What question does the Weierstrass approximation theorem answer?

<ul class="org-ul">
<li><a href="http://www4.ncsu.edu/~mtchu/Teaching/Lectures/MA530/chapter7.pdf">http://www4.ncsu.edu/~mtchu/Teaching/Lectures/MA530/chapter7.pdf</a></li>
</ul></li>

<li><a href="https://en.wikipedia.org/wiki/Chebyshev_polynomials">WP:Chebyshev polynomials</a>

<ul class="org-ul">
<li>Why is it important?
How does it relate to best approximation?

<ul class="org-ul">
<li>"Chebyshev polynomials are important in approximation theory because the roots of the Chebyshev polynomials of the first kind, which are also called Chebyshev nodes, are used as nodes in polynomial interpolation.
The resulting interpolation polynomial minimizes the problem of Runge's phenomenon and provides an approximation that is close to the polynomial of best approximation to a continuous function under the maximum norm."</li>
</ul></li>
</ul></li>

<li>Machine learning as relation approximation

<ul class="org-ul">
<li>Machine learning, statistical modelling, function approximation, and curve fitting are related.</li>
<li>Generalize function approximation to relation approximation.</li>
<li>A function can be stated as a relation.</li>
<li>A relation can be stated as a function.</li>
</ul></li>

<li>Consider the least-square solution to an overdetermined system of linear equations.
Is such solution a kind of approximation?

<ul class="org-ul">
<li>There is no exact solution to begin with?</li>
<li>Why is it called "least-squares <i>approximation</i>"?</li>
<li>How can you approximate something that does not exist?

<ul class="org-ul">
<li>1.2 approximates 1.23. Both 1.2 and 1.23 exist.</li>
<li>Contrarily, there is no X such that AX = B.</li>
</ul></li>
</ul></li>

<li>What are approximation schemes?

<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/Polynomial-time_approximation_scheme">https://en.wikipedia.org/wiki/Polynomial-time_approximation_scheme</a></li>
</ul></li>

<li>How do we approximate a function?
Is it even possible to approximate arbitrary functions?

<ul class="org-ul">
<li>If the function is analytic, we can truncate its Taylor series.

<ul class="org-ul">
<li>Commonly-used differentiable functions are analytic.</li>
</ul></li>

<li>Chebyshev polynomials?</li>
<li>If we have an approximation scheme, we may be able to improve it.

<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/Series_acceleration">https://en.wikipedia.org/wiki/Series_acceleration</a>

<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/Aitken's_delta-squared_process">https://en.wikipedia.org/wiki/Aitken's_delta-squared_process</a></li>
</ul></li>
</ul></li>

<li>google search: machine learning approximation theory

<ul class="org-ul">
<li><a href="https://math.stackexchange.com/questions/2680158/approximation-theory-for-deep-learning-models-where-to-start">Approximation Theory for Deep Learning Models: Where to Start? - Mathematics Stack Exchange</a></li>
<li><a href="http://www.vision.jhu.edu/tutorials/ICCV15-Tutorial-Math-Deep-Learning-Intro-Rene-Joan.pdf">http://www.vision.jhu.edu/tutorials/ICCV15-Tutorial-Math-Deep-Learning-Intro-Rene-Joan.pdf</a></li>
<li>2017, slides, "From approximation theory to machine learning: New perspectives in the theory of function spaces and their applications", <a href="http://npfsa2017.uni-jena.de/l_notes/vybiral.pdf">pdf</a></li>
<li>2018, article, "Approximation theory, Numerical Analysis and Deep Learning", <a href="http://at.yorku.ca/c/b/p/g/30.htm">abstract</a>

<ul class="org-ul">
<li>"the problem of numerically solving a large class of (high-dimensional) PDEs (such as linear Black-Scholes or diffusion equations) can be cast into a classical supervised learning problem which can then be solved by deep learning methods"</li>
</ul></li>
</ul></li>
</ul></li>

<li>Determine whether we need to read these

<ul class="org-ul">
<li>Very likely

<ul class="org-ul">
<li>2015, slides, "Best polynomial approximation: multidimensional case", <a href="https://carma.newcastle.edu.au/meetings/spcom/talks/Sukhorukova-SPCOM_2015.pdf">pdf</a></li>
<li><a href="https://en.wikipedia.org/wiki/Bernstein_polynomial#Approximating_continuous_functions">https://en.wikipedia.org/wiki/Bernstein_polynomial#Approximating_continuous_functions</a>

<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/Pointwise_convergence">https://en.wikipedia.org/wiki/Pointwise_convergence</a></li>
<li><a href="https://en.wikipedia.org/wiki/Uniform_convergence">https://en.wikipedia.org/wiki/Uniform_convergence</a></li>
</ul></li>

<li><a href="https://en.wikipedia.org/wiki/Approximation">https://en.wikipedia.org/wiki/Approximation</a>

<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/Approximation_theory">https://en.wikipedia.org/wiki/Approximation_theory</a>

<ul class="org-ul">
<li>is a branch of <a href="https://en.wikipedia.org/wiki/Functional_analysis">https://en.wikipedia.org/wiki/Functional_analysis</a></li>
<li><a href="https://en.wikipedia.org/wiki/Approximation_theory#Chebyshev_approximation">https://en.wikipedia.org/wiki/Approximation_theory#Chebyshev_approximation</a></li>
</ul></li>

<li><a href="https://en.wikipedia.org/wiki/Approximate_computing">https://en.wikipedia.org/wiki/Approximate_computing</a>

<ul class="org-ul">
<li>example: <a href="https://en.wikipedia.org/wiki/Artificial_neural_network">https://en.wikipedia.org/wiki/Artificial_neural_network</a></li>
</ul></li>
</ul></li>

<li><a href="https://en.wikipedia.org/wiki/Telescoping_series">https://en.wikipedia.org/wiki/Telescoping_series</a></li>
</ul></li>

<li>Likely

<ul class="org-ul">
<li>2018, slides, "Deep Learning: Approximation of Functions by Composition", <a href="http://helper.ipam.ucla.edu/publications/dlt2018/dlt2018_14936.pdf">pdf</a>

<ul class="org-ul">
<li>classical approximation vs deep learning</li>
</ul></li>

<li>2013, short survey article draft, "Multivariate approximation", <a href="http://num.math.uni-goettingen.de/schaback/research/papers/MultApp_01.pdf">pdf</a></li>
<li>1995, short introduction, "Multivariate Interpolation and Approximation by Translates of a Basis Function", <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.45.2194&amp;rep=rep1&amp;type=pdf">pdf</a></li>
<li>1989, article, "A Theory of Networks for Approximation and Learning", <a href="http://www.dtic.mil/docs/citations/ADA212359">pdf available</a>

<ul class="org-ul">
<li>What is the summary, especially about learning and approximation theory?</li>
</ul></li>
</ul></li>

<li>Unlikely

<ul class="org-ul">
<li>Survey-like

<ul class="org-ul">
<li>2006, chapter, "Topics in multivariate approximation theory", <a href="https://www.researchgate.net/publication/226303661_Topics_in_multivariate_approximation_theory">pdf available</a></li>
<li>1982, article, "Topics in multivariate approximation theory", <a href="http://www.dtic.mil/dtic/tr/fulltext/u2/a116248.pdf">pdf</a></li>
<li>1986, "Multivariate Approximation Theory: Selected Topics", <a href="https://epubs.siam.org/doi/book/10.1137/1.9781611970197">paywall</a></li>
</ul></li>

<li>Theorem

<ul class="org-ul">
<li>2017, article, "Multivariate polynomial approximation in the hypercube", <a href="https://people.maths.ox.ac.uk/trefethen/hypercube_published.pdf">pdf</a></li>
</ul></li>

<li>2017, article, "Selected open problems in polynomial approximation and potential theory", <a href="http://drna.padovauniversitypress.it/system/files/papers/BaranCiezEgginkKowalskaNagyPierzcha%C5%82a_DRNA2017.pdf">pdf</a></li>
<li>2017, article, "High order approximation theory for Banach space valued functions", <a href="https://ictp.acad.ro/jnaat/journal/article/view/1112">pdf available</a></li>
<li>Articles summarizing people's works

<ul class="org-ul">
<li>2017, article, "Michael J.D. Powell's work in approximation theory and optimisation", <a href="https://www.sciencedirect.com/science/article/abs/pii/S0021904517301053">paywall</a></li>
<li>2000, article, "Weierstrass and Approximation Theory", <a href="https://www.sciencedirect.com/science/article/pii/S0021904500935081">paywall</a></li>
</ul></li>

<li>2013, article, "[1312.5540] Emerging problems in approximation theory for the numerical solution of nonlinear PDEs of integrable type", <a href="https://arxiv.org/abs/1312.5540">pdf available</a></li>
<li>1985, article, "Some problems in approximation theory and numerical analysis - IOPscience", <a href="http://iopscience.iop.org/article/10.1070/RM1985v040n01ABEH003526">pdf available</a></li>
<li>2011, article, "Experiments on Probabilistic Approximations", <a href="https://people.eecs.ku.edu/~jerzygb/c154-clark.pdf">pdf</a></li>
</ul></li>
</ul></li>

<li>Less relevant overview

<ul class="org-ul">
<li>Why do we approximate?

<ul class="org-ul">
<li>Because it is practically inevitable.

<ul class="org-ul">
<li>Fundamental reason: Because computers are finite.</li>
<li>Practical reason: Trade-off between computation time and precision.

<ul class="org-ul">
<li>The more error we can afford, the faster we can run.

<ul class="org-ul">
<li>May be related: 2013 monograph "Faster Algorithms via Approximation Theory" <a href="http://theory.epfl.ch/vishnoi/Publications_files/approx-survey.pdf">pdf</a></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>

<li>2018 book "Recent Advances in Constructive Approximation Theory" <a href="https://www.springer.com/us/book/9783319921648">paywall</a></li>
</ul></li>
</ul>
</div>
</div>
</div>


<div id="outline-container-org1e77248" class="outline-2">
<h2 id="org1e77248"><span class="section-number-2">5</span> atrunc.md</h2>
<div class="outline-text-2" id="text-5">
<p>
We can approximate a series by <i>truncating</i> it.
</p>

<p>
Suppose that the series \(y = x_0 + x_1 + \ldots\) converges.
</p>

<p>
Suppose that the sequence \(\langle x_0, x_1, \ldots \rangle\) converges to zero.
</p>

<p>
Pick where to cut.
Pick a natural number \(n\).
</p>

<p>
Then the series \(x_0 + \ldots + x_n\) approximates the series \(y\).
We cut its tail.
We take finitely many summands from the beginning.
</p>

<p>
Here come examples: Truncate all the series!
</p>
</div>

<div id="outline-container-org6e31d79" class="outline-3">
<h3 id="power-series-truncation"><a id="org6e31d79"></a><span class="section-number-3">5.1</span> Power series truncation</h3>
<div class="outline-text-3" id="text-power-series-truncation">
<p>
Below we truncate a power series.
</p>

<p>
Decimal truncation: \(1.2\) approximates \(1.23\).
Remember that a decimal number is a series.
For example, the number \(1.23\) is the power series
\[ \ldots 01.230 \ldots = \ldots + 0 \cdot 10^1 + 1 \cdot 10^0 + 2 \cdot 10^{-1} + 3 \cdot 10^{-2} + 0 \cdot 10^{-3} + \ldots. \]
</p>

<p>
Polynomial truncation: \(1 + x\) approximates \(1 + x + x^2\) for \(x\) near zero.
</p>

<p>
Taylor series truncation: \(1 + x + \frac{x^2}{2}\) approximates \(e^x\) for \(x\) near zero.
Remember the Taylor series expansion \(e^x = \sum_{n \in \Nat} \frac{x^n}{n!}\).
</p>

<p>
Below we truncate the ratio of two power series.
</p>

<p>
Rational truncation: \(12/23\) approximates \(123/234\).
</p>

<p>
<a href="https://en.wikipedia.org/wiki/Pad%C3%A9_approximant">WP:Padé approximation</a> is a truncation of a ratio of series.
</p>

<p>
Fourier series truncation: The <a href="https://en.wikipedia.org/wiki/Fourier_series#Example_1:_a_simple_Fourier_series">Wikipedia example</a> animates how a Fourier series converges to the sawtooth function as more terms are added.
</p>

<p>
Digression: Is a (complex) Fourier series a power series?
Reminder: A Fourier series looks like \(\sum_{k=0}^{\infty} c_k e^{ikt}\).
</p>

<p>
<a href="https://en.wikipedia.org/wiki/Laurent_series">WP:Laurent series</a> truncation?
</p>
</div>

<div id="outline-container-org4da53a3" class="outline-4">
<h4 id="digression-what-is-an-analytic-function"><a id="org4da53a3"></a><span class="section-number-4">5.1.1</span> Digression: What is an analytic function?</h4>
<div class="outline-text-4" id="text-digression-what-is-an-analytic-function">
<p>
A function is <i>analytic</i> iff it can be represented by power series.
</p>

<p>
Formally, a function \(f\) is <i>analytic</i> iff for every \(x \in \dom(f)\), we can write \(f(x)\) as a power series.
</p>

<p>
See also <a href="https://en.wikipedia.org/wiki/Power_series#Analytic_functions">WP:Definition of "analytic function"</a>.
</p>

<p>
Taylor series expansion is illustrated in the 2015 slides "Taylor Series: Expansions, Approximations and Error" (<a href="https://relate.cs.illinois.edu/course/cs357-f15/file-version/2978ddd5db9824a374db221c47a33f437f2df1da/media/cs357-slides6.pdf">pdf</a>)
</p>
</div>
</div>

<div id="outline-container-org9e6752b" class="outline-4">
<h4 id="digression-what-is-the-relationship-between-polynomial-and-power-series"><a id="org9e6752b"></a><span class="section-number-4">5.1.2</span> Digression: What is the relationship between polynomial and power series?</h4>
<div class="outline-text-4" id="text-digression-what-is-the-relationship-between-polynomial-and-power-series">
<p>
A polynomial is an algebraic expression. It is not a function.
</p>

<p>
Power series is a kind of infinite polynomial.
</p>

<p>
<a href="https://en.wikipedia.org/wiki/Formal_power_series">WP:Formal power series</a>: "A formal power series is a generalization of a polynomial, where the number of terms is allowed to be infinite."
</p>
</div>
</div>
</div>

<div id="outline-container-org27088d4" class="outline-3">
<h3 id="iteration-truncation"><a id="org27088d4"></a><span class="section-number-3">5.2</span> Iteration truncation</h3>
<div class="outline-text-3" id="text-iteration-truncation">
<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/Iterated_function">WP:Iterated function</a></li>
<li><a href="https://en.wikipedia.org/wiki/Iterative_method">WP:Iterative method</a></li>
<li><a href="http://mathworld.wolfram.com/NewtonsIteration.html">Newton's Iteration</a></li>
<li><a href="https://en.wikipedia.org/wiki/Methods_of_computing_square_roots#Babylonian_method">WP:Methods of computing square roots, the Babylonian method</a></li>
<li>An iteration converges to an attractive fixed point.</li>
</ul>

<p>
Example:
Let \(f(x) = x + \frac{1}{x}\).
</p>

<p>
Continued fraction truncation:
We know that \[ 1 + \frac{1}{1 + \frac{1}{1 + \ldots}} = \frac{1 + \sqrt{5}}{2} = \Phi. \]
We can truncate that continued fraction to approximate \(\Phi\).
</p>

<p>
Seeing those examples makes me wonder whether all approximations are truncation.
</p>
</div>
</div>
</div>
<div id="outline-container-org15adf1d" class="outline-2">
<h2 id="org15adf1d"><span class="section-number-2">6</span> Counterfactual reasoning</h2>
<div class="outline-text-2" id="text-6">
<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/Counterfactual_conditional">WP:Counterfactual conditional</a></li>
<li><a href="https://en.wikipedia.org/wiki/Counterfactual_thinking">WP:Counterfactual thinking</a></li>
<li><a href="https://en.wikipedia.org/wiki/Wishful_thinking">WP:Wishful thinking</a></li>
<li>Study <a href="http://bayes.cs.ucla.edu/jp_home.html">Judea Pearl's works</a>.</li>
<li>Is counterfactual reasoning valid?

<ul class="org-ul">
<li>How does it differ from wishful thinking?</li>
</ul></li>

<li>How do we justify counterfactual reasoning?

<ul class="org-ul">
<li>How do we justify statements like "If Hitler had never been made a Chancellor, then World War 2 would have never happened."?

<ul class="org-ul">
<li>If Hitler hadn't done it, wouldn't someone else have?</li>
<li>If Hitler hadn't done it, wouldn't there be someone else more evil?</li>
</ul></li>
</ul></li>

<li>Tacit assumptions

<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/Tacit_assumption">WP:Tacit assumption</a></li>
<li>When reasoning counterfactually, we tacitly assume:

<ul class="org-ul">
<li>The law of nature doesn't change.

<ul class="org-ul">
<li>The law of nature is the same 1,000 years ago.

<ul class="org-ul">
<li>It seems that any attempt at justifying this would crash into Hume's induction problem.

<ul class="org-ul">
<li>The law of nature is the same yesterday.</li>
<li>The law of nature is the same two days ago.</li>
<li>The law of nature has always been the same?

<ul class="org-ul">
<li>We don't know the law of nature before the Big Bang.</li>
</ul></li>

<li>However, for most practical purposes, the law of nature has always been the same.</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orga3dc5db" class="outline-2">
<h2 id="orga3dc5db"><span class="section-number-2">7</span> Automatic differentiation?</h2>
<div class="outline-text-2" id="text-7">
<p>
Justin Le, <a href="https://blog.jle.im/entry/purely-functional-typed-models-1.html">A Purely Functional Typed Approach to Trainable Models</a>
</p>
</div>
</div>
<div id="outline-container-org150c89c" class="outline-2">
<h2 id="org150c89c"><span class="section-number-2">8</span> About defining consciousness</h2>
<div class="outline-text-2" id="text-8">
<p>
2009, "How to define consciousness—and how not to define consciousness", <a href="http://cogprints.org/6453/1/How_to_define_consciousness.pdf">pdf</a>
</p>
</div>
</div>
<div id="outline-container-orga076ea7" class="outline-2">
<h2 id="orga076ea7"><span class="section-number-2">9</span> <span class="timestamp-wrapper"><span class="timestamp">&lt;2018-09-28&gt; </span></span> Book: "interpretable machine learning"</h2>
<div class="outline-text-2" id="text-9">
<p>
<a href="https://christophm.github.io/interpretable-ml-book/">https://christophm.github.io/interpretable-ml-book/</a>
</p>
</div>
</div>
<div id="outline-container-orgb5b3848" class="outline-2">
<h2 id="orgb5b3848"><span class="section-number-2">10</span> Approximation theory and machine learning</h2>
<div class="outline-text-2" id="text-10">
<p>
Conference: "Approximation Theory and Machine Learning", at Purdue University, September 29 - 30, 2018
</p>
<ul class="org-ul">
<li><a href="http://www.math.purdue.edu/calendar/conferences/machinelearning/">http://www.math.purdue.edu/calendar/conferences/machinelearning/</a></li>
<li><a href="http://www.math.purdue.edu/calendar/conferences/machinelearning/abstracts.php">http://www.math.purdue.edu/calendar/conferences/machinelearning/abstracts.php</a></li>
</ul>
</div>
</div>
<div id="outline-container-org920773d" class="outline-2">
<h2 id="org920773d"><span class="section-number-2">11</span> Analogizers, recommender systems, matrices</h2>
<div class="outline-text-2" id="text-11">
<ul class="org-ul">
<li><a href="https://medium.com/s/story/spotifys-discover-weekly-how-machine-learning-finds-your-new-music-19a41ab76efe">https://medium.com/s/story/spotifys-discover-weekly-how-machine-learning-finds-your-new-music-19a41ab76efe</a></li>
</ul>
</div>
</div>

  </div>

</article>

      </div>
    </main>

    
    
<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
var disqus_config = function () {
    this.page.url = "https://edom.github.io/intelligence.html";  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = "https://edom.github.io/intelligence.html"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://edom-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    

    <footer class="site-footer h-card">
    <data class="u-url" href="/"></data>

    <div class="wrapper">
        <p>This page was created on 2017-06-22 03:57:00 +0700.</p>
        <p class="rss-subscribe">There is an <a href="/feed.xml">RSS feed</a>,
        but it's unused because this site is a wiki, not a blog.</p>
        <p>Stop writing books, papers, and blogs! Write a personal wiki instead! Or, even better, contribute to a community wiki.</p>
    </div>

</footer>


  </body>

</html>
