<!DOCTYPE html>
<html lang="">
    <head>
        <meta charset="UTF-8"/>
        <title>Making intelligence</title>
        <link rel="stylesheet" href="/assets/main.css"/>
        <script>
// Help the reader estimate how much time the reading is going to take.
// Show word count and reading time estimation in TOC entry.
//
// TOC = table of contents
//
// Known issue: This janks: this DOM manipulation is done after the page is rendered.
// If we don't want jank, we have to manipulate the HTML source before it reaches the browser.
// We assume that the user doesn't refresh the page while reading.
// The benefit of fixing that jank is not enough for me to justify trying to fix it.
document.addEventListener("DOMContentLoaded", function () {
    function count_word (string) {
        return string.trim().split(/\s+/).length;
    }
    function show_quantity (count, singular) {
        let plural = singular + "s"; // For this script only.
        return count + " " + ((count == 1) ? singular : plural);
    }
    function create_length_indicator (word, minute) {
        let e = document.createElement("span");
        e.className = "toc_entry__length_indicator";
        e.textContent = " (" + show_quantity(word, "word") + " ~ " + show_quantity(minute, "minute") + ")";
        return e;
    }
    // We assume that readers read this many words per minute with 100% comprehension.
    // This assumption may not hold for dense texts such as philosophy and mathematics.
    const wpm_assumption = 200;
    // We assume a certain Jekyll template.
    let page = document.querySelector("main.page-content");
    if (page === null) {
        console.log("toc_generate_estimate: Impossible: CSS selector \"main.page-content\" does not match anything");
        return;
    }
    let page_title = document.querySelector("header.post-header h1.post-title");
    if (page_title === null) {
        console.log("toc_generate_estimate: Impossible: CSS selector \"header.post-header h1.post-title\" does not match anything");
        return;
    }
    let page_word = count_word(page.textContent);
    let page_minute = Math.ceil(page_word / wpm_assumption);
    page_title.insertAdjacentElement("afterend", create_length_indicator(page_word, page_minute));
    // We violate the HTML specification.
    // The page may have several elements with the same ID.
    // We assume that Org HTML Export generates a DIV element with ID "table-of-contents".
    // We assume that Jekyll Markdown-to-HTML generates a UL element with ID "markdown-toc".
    // This only works for Org HTML Export's TOC.
    let toc_entries = document.querySelectorAll("#table-of-contents a, #text-table-of-contents a");
    toc_entries.forEach((toc_entry_a) => {
        let href = toc_entry_a.getAttribute("href"); // We assume that this is a string like "#org0123456".
        if (href.charAt(0) !== '#') {
            console.log("toc_generate_estimate: Impossible: " + href + " does not begin with hash sign");
            return;
        }
        // We can't just document.querySelector(href) because target_id may contain invalid ID characters such as periods.
        let target_id = href.substring(1);
        let id_escaped = target_id.replace("\"", "\\\"");
        let h_elem = document.querySelector("[id=\"" + id_escaped + "\"]"); // We assume that this is the h1/h2/h3 element referred by the TOC entry.
        if (h_elem === null) { // We assume that this is impossible.
            console.log("toc_generate_estimate: Impossible: " + href + " does not refer to anything");
            return;
        }
        let section = h_elem.parentNode;
        let section_word = count_word(section.textContent);
        let section_minute = Math.ceil(section_word / wpm_assumption);
        toc_entry_a.insertAdjacentElement("afterend", create_length_indicator(section_word, section_minute));
    });
});
        </script>

        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-12628443-6"></script>
<script>
  window['ga-disable-UA-12628443-6'] = window.doNotTrack === "1" || navigator.doNotTrack === "1" || navigator.doNotTrack === "yes" || navigator.msDoNotTrack === "1";
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-12628443-6');
</script>
        
        <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            jax: ["input/TeX","input/MathML","input/AsciiMath",
            "output/CommonHTML"
            ],
            extensions: ["tex2jax.js","mml2jax.js","asciimath2jax.js","MathMenu.js","MathZoom.js","AssistiveMML.js", "a11y/accessibility-menu.js"],
            TeX: {
                extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
                , equationNumbers: {
                    autoNumber: "AMS"
                }
            },
            "CommonHTML": {
                scale: 100
            },
            "fast-preview": {
                disabled: true,
            }
        });
        </script>
        <style>
            /*
            PreviewHTML produces small Times New Roman text.
            PreviewHTML scale doesn't work.
            */
            .MathJax_PHTML { font-size: 110%; }
        </style>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js" async defer></script>
    </head>
    <body>
        <header class="site-header" role="banner">
            <div class="wrapper">
                <a class="site-title" rel="author" href="/">Erik Dominikus's wiki</a>
            </div>
        </header>
    <div style="display:none;">\(
    \renewcommand\emptyset{\varnothing}
    \newcommand\abs[1]{\left|#1\right|}
    \newcommand\dom{\textrm{dom}}
    \newcommand\cod{\textrm{cod}}
    \newcommand\Bernoulli{\textrm{Bernoulli}}
    \newcommand\Binomial{\textrm{Binomial}}
    \newcommand\Expect[1]{\mathbb{E}[#1]}
    \newcommand\Nat{\mathbb{N}}
    \newcommand\Integers{\mathbb{Z}}
    \newcommand\Real{\mathbb{R}}
    \newcommand\Rational{\mathbb{Q}}
    \newcommand\Complex{\mathbb{C}}
    \newcommand\Pr{\mathrm{P}}
    \newcommand\Time{\text{Time}}
    \newcommand\DTime{\text{DTime}}
    \newcommand\NTime{\text{NTime}}
    \newcommand\TimeP{\text{P}}
    \newcommand\TimeNP{\text{NP}}
    \newcommand\TimeExp{\text{ExpTime}}
    \newcommand\norm[1]{\left\lVert#1\right\rVert}
    \newcommand\bbA{\mathbb{A}}
    \newcommand\bbC{\mathbb{C}}
    \newcommand\bbD{\mathbb{D}}
    \newcommand\bbE{\mathbb{E}}
    \newcommand\bbN{\mathbb{N}}
    \newcommand\frakI{\mathfrak{I}}
    % deprecated; use TimeExp
    \newcommand\ExpTime{\text{ExpTime}}
    \newcommand\Compute{\text{Compute}}
    \newcommand\Search{\text{Search}}
    % model theory structure
    \newcommand\struc[1]{\mathcal{#1}}
    \)</div>
        <main class="page-content" aria-label="Content">
            <div class="wrapper">
                <article class="post">
                    <header class="post-header">
                        <h1 class="post-title">Making intelligence</h1>
                    </header>
                </article>
                <div class="post-content">
                    <div class="local_table_of_contents">
                    <ul>
                    <li><span class="section_number">1</span><span class="section_title"><a href="#doing-the-last-thing-we-will-ever-need-to-do">Doing the last thing we will ever need to do</a></span><span class="word_count">(148w~1m)</span></li>
                    <li><span class="section_number">2</span><span class="section_title"><a href="#conjectures-about-language-and-logic">Conjectures about language and logic</a></span><span class="word_count">(85w~1m)</span></li>
                    <li><span class="section_number">3</span><span class="section_title"><a href="#unifying-supervised-and-unsupervised-learning">Unifying supervised and unsupervised learning?</a></span><span class="word_count">(5w~1m)</span></li>
                    <li><span class="section_number">4</span><span class="section_title"><a href="#concept-spaces-word-vectors-concept-vectors-bags-of-words">Concept spaces, word vectors, concept vectors, bags of words</a></span><span class="word_count">(44w~1m)</span></li>
                    <li><span class="section_number">5</span><span class="section_title"><a href="#ai-approaches">AI approaches</a></span><span class="word_count">(47w~1m)</span></li>
                    <li><span class="section_number">6</span><span class="section_title"><a href="#what-is-intelligence">What is intelligence?</a></span><span class="word_count">(1339w~7m)</span></li>
                    <li><span class="section_number">7</span><span class="section_title"><a href="#abbreviations">Abbreviations</a></span><span class="word_count">(8w~1m)</span></li>
                    <li><span class="section_number">8</span><span class="section_title"><a href="#surveys-reviews-positions-and-expositions">Surveys, reviews, positions, and expositions</a></span><span class="word_count">(1816w~10m)</span></li>
                    <li><span class="section_number">9</span><span class="section_title"><a href="#artificial-intelligence-research">Artificial intelligence research</a></span><span class="word_count">(226w~2m)</span></li>
                    <li><span class="section_number">10</span><span class="section_title"><a href="#approximation-theory">Approximation theory</a></span><span class="word_count">(841w~5m)</span></li>
                    <li><span class="section_number">11</span><span class="section_title"><a href="#approximation-by-truncation">Approximation by truncation</a></span><span class="word_count">(387w~2m)</span></li>
                    <li><span class="section_number">12</span><span class="section_title"><a href="#automatic-differentiation">Automatic differentiation?</a></span><span class="word_count">(11w~1m)</span></li>
                    <li><span class="section_number">13</span><span class="section_title"><a href="#about-defining-consciousness">About defining consciousness</a></span><span class="word_count">(13w~1m)</span></li>
                    <li><span class="section_number">14</span><span class="section_title"><a href="#book-interpretable-machine-learning">&lt;2018-09-28&gt; Book: &quot;interpretable machine learning&quot;</a></span><span class="word_count">(5w~1m)</span></li>
                    <li><span class="section_number">15</span><span class="section_title"><a href="#approximation-theory-and-machine-learning">Approximation theory and machine learning</a></span><span class="word_count">(18w~1m)</span></li>
                    <li><span class="section_number">16</span><span class="section_title"><a href="#analogizers-recommender-systems-matrices">Analogizers, recommender systems, matrices</a></span><span class="word_count">(4w~1m)</span></li>
                    <li><span class="section_number">17</span><span class="section_title"><a href="#bibliography">Bibliography</a></span><span class="word_count">(165w~1m)</span></li>
                    </ul>
                    </div>
                    <h2 id="doing-the-last-thing-we-will-ever-need-to-do"><span class="section_number">1</span><span class="section_title">Doing the last thing we will ever need to do</span></h2>
                    <p><a href="http://people.idsia.ch/~juergen/">Jürgen Schmidhuber wants to build something smarter than him and then retire.</a></p>
                    <p>I want the same thing.</p>
                    <p><a href="http://people.idsia.ch/~juergen/">Schmidhuber's website</a> contains a lot of content, if not too much. It is hard for an outsider to tell whether he is genius or crazy. But he has won lots of competitions.</p>
                    <p>Isn't Jacques Pitrat's CAIA similar in spirit to what Jürgen Schmidhuber wants?</p>
                    <p>Is Kyndi closest to what we want? &quot;Kyndi serves as a tireless digital assistant, identifying the documents and passages that require human judgment.&quot; <a href="https://www.nytimes.com/2018/06/20/technology/deep-learning-artificial-intelligence.html">https://www.nytimes.com/2018/06/20/technology/deep-learning-artificial-intelligence.html</a></p>
                    <p>Kyndi uses Prolog.</p>
                    <p>I need something similar to Kyndi but able to generate interesting questions for itself to answer. I want it to read journal articles and conference proceedings, understand them, and summarize them for me.</p>
                    <p>Concepts:</p>
                    <ul>
                    <li>artificial general intelligence</li>
                    <li>seed AI</li>
                    </ul>
                    <div class="local_table_of_contents">
                    <ul>
                    <li><span class="section_number">1.1</span><span class="section_title"><a href="#why-there-is-no-human-level-agi-in-2018">Why there is no human-level AGI in 2018?</a></span><span class="word_count">(22w~1m)</span></li>
                    </ul>
                    </div>
                    <h3 id="why-there-is-no-human-level-agi-in-2018"><span class="section_number">1.1</span><span class="section_title">Why there is no human-level AGI in 2018?</span></h3>
                    <p>Which of these is the bottleneck?</p>
                    <ul>
                    <li>hardware</li>
                    <li>software</li>
                    <li>our knowledge</li>
                    </ul>
                    <p>Is it an engineering problem or a philosophical problem?</p>
                    <h2 id="conjectures-about-language-and-logic"><span class="section_number">2</span><span class="section_title">Conjectures about language and logic</span></h2>
                    <p>Conjectures:</p>
                    <ul>
                    <li>Natural languages are just <em>surface syntaxes</em> for first-order logic.</li>
                    </ul>
                    <p>It is straightforward to write a Prolog program that parses some limited English. It is still practical to write a Prolog program that parses some richer English with named entity recognition. Prolog definite-clause grammars make parsing easy</p>
                    <p>Another problems:</p>
                    <ul>
                    <li>Which information source should the computer trust?</li>
                    <li>How should the computer reconcile conflicting information?</li>
                    </ul>
                    <p>2011 &quot;Natural Language Processing With Prolog in the IBM Watson System&quot; <a href="https://www.cs.nmsu.edu/ALP/2011/03/natural-language-processing-with-prolog-in-the-ibm-watson-system/">https://www.cs.nmsu.edu/ALP/2011/03/natural-language-processing-with-prolog-in-the-ibm-watson-system/</a></p>
                    <p>If IBM Watson is possible, then a personal search assistant should be possible.</p>
                    <h2 id="unifying-supervised-and-unsupervised-learning"><span class="section_number">3</span><span class="section_title">Unifying supervised and unsupervised learning?</span></h2>
                    <h2 id="concept-spaces-word-vectors-concept-vectors-bags-of-words"><span class="section_number">4</span><span class="section_title">Concept spaces, word vectors, concept vectors, bags of words</span></h2>
                    <p>Let Car represent the concept of car. Let Red represent the concept of red. Let Modify(Car,Red) represent the concept of red car. Then Modify(X,Red) - Modify(Y,Red) = X - Y.</p>
                    <p>Modify(X,M) - Modify(Y,M) = X - Y.</p>
                    <h2 id="ai-approaches"><span class="section_number">5</span><span class="section_title">AI approaches</span></h2>
                    <ul>
                    <li>logic, symbolism</li>
                    <li>biology, connectionism</li>
                    <li>probabilistic logic programming</li>
                    </ul>
                    <p>What's trending in 2018:</p>
                    <ul>
                    <li>deep learning (DL)</li>
                    <li>generative adversarial network (GAN)</li>
                    <li>long short-term memory (LSTM)</li>
                    </ul>
                    <p>There are two ways to make an &quot;infinite-layer&quot; neural network:</p>
                    <ul>
                    <li>recurrent neural network (RNN), similar to IIR (infinite-impulse-response) filter in control theory</li>
                    <li>neural ordinary differential equations (NODE), similar to Riemann summation in calculus</li>
                    </ul>
                    <h2 id="what-is-intelligence"><span class="section_number">6</span><span class="section_title">What is intelligence?</span></h2>
                    <div class="local_table_of_contents">
                    <ul>
                    <li><span class="section_number">6.1</span><span class="section_title"><a href="#the-most-general-definition-from-2007">The most general definition from 2007</a></span><span class="word_count">(43w~1m)</span></li>
                    <li><span class="section_number">6.2</span><span class="section_title"><a href="#intelligence-is-an-ordering-2018-04-26">Intelligence is an ordering (2018-04-26)</a></span><span class="word_count">(186w~1m)</span></li>
                    <li><span class="section_number">6.3</span><span class="section_title"><a href="#intelligence-is-function-optimization-2018-04-27">Intelligence is function optimization (2018-04-27)</a></span><span class="word_count">(27w~1m)</span></li>
                    <li><span class="section_number">6.4</span><span class="section_title"><a href="#what-is-a-mathematical-theory-of-intelligence">What is a mathematical theory of intelligence?</a></span><span class="word_count">(422w~3m)</span></li>
                    <li><span class="section_number">6.5</span><span class="section_title"><a href="#historical-definitions">Historical definitions</a></span><span class="word_count">(28w~1m)</span></li>
                    <li><span class="section_number">6.6</span><span class="section_title"><a href="#what-is-learning">What is learning?</a></span><span class="word_count">(346w~2m)</span></li>
                    <li><span class="section_number">6.7</span><span class="section_title"><a href="#what-is-ai">What is AI?</a></span><span class="word_count">(291w~2m)</span></li>
                    </ul>
                    </div>
                    <h3 id="the-most-general-definition-from-2007"><span class="section_number">6.1</span><span class="section_title">The most general definition from 2007</span></h3>
                    <p>I think the most general definition is &quot;Intelligence measures an agent's ability to achieve goals in a wide range of environments&quot; <span class="citation" data-cites="DefineMachIntel">[<a href="#ref-DefineMachIntel">5</a>]</span><span class="citation" data-cites="Legg2007Collection">[<a href="#ref-Legg2007Collection">4</a>]</span>. I think it subsumes all other definitions of intelligence in all other fields such as psychology.</p>
                    <h3 id="intelligence-is-an-ordering-2018-04-26"><span class="section_number">6.2</span><span class="section_title">Intelligence is an ordering (2018-04-26)</span></h3>
                    <p>This idea goes back at least to 2004 in <span class="citation" data-cites="hutter2004universal">[<a href="#ref-hutter2004universal">2</a>]</span>.</p>
                    <p>Intelligence is an <em>ordering</em> of systems.</p>
                    <p>An order is a transitive antisymmetric relation.</p>
                    <p><em>Intelligence depends on its measurement</em>. Absolute intelligence doesn't exist.</p>
                    <p>The <em>behavior</em> of a system is whatever it exhibits that can be observed from outside.</p>
                    <p>How do we decide which system is more intelligent?</p>
                    <p>Let <span class="math inline">\(A\)</span> be a system.</p>
                    <p>Let <span class="math inline">\(B\)</span> be a system.</p>
                    <p>Let <span class="math inline">\(T\)</span> be a task.</p>
                    <p>Let <span class="math inline">\(S\)</span> be a set of tasks.</p>
                    <p>Let <span class="math inline">\(T(A)\)</span> denote how well system <span class="math inline">\(A\)</span> does task <span class="math inline">\(T\)</span>. This is a number. Higher is better. We can invent any measurement. Our definition of &quot;intelligence&quot; is only as good as this measurement.</p>
                    <p>We say &quot;<span class="math inline">\(A\)</span> is <em><span class="math inline">\(T\)</span>-better</em> than <span class="math inline">\(B\)</span>&quot; iff <span class="math inline">\(T(A) &gt; T(B)\)</span>.</p>
                    <p>We say &quot;<span class="math inline">\(A\)</span> <em><span class="math inline">\(S\)</span>-dominates</em> <span class="math inline">\(B\)</span>&quot; iff <span class="math inline">\(T(A) &gt; T(B)\)</span> for every task <span class="math inline">\(T \in S\)</span>.</p>
                    <p>We define &quot;to be more <span class="math inline">\(S\)</span>-intelligent than&quot; to mean &quot;to <span class="math inline">\(S\)</span>-dominate&quot;.</p>
                    <p>The <span class="math inline">\(S\)</span>-domination relation forms a partial order of all systems.</p>
                    <p>That is how.</p>
                    <p>Example</p>
                    <p>Which is more intelligent, a dog or a rock?</p>
                    <p>That depends on the task set <span class="math inline">\(S\)</span>.</p>
                    <p>It's the rock if ( S = { sit still } ).</p>
                    <p>It's the dog if ( S = { move around } ).</p>
                    <h3 id="intelligence-is-function-optimization-2018-04-27"><span class="section_number">6.3</span><span class="section_title">Intelligence is function optimization (2018-04-27)</span></h3>
                    <p>Let <span class="math inline">\(g\)</span> be a goal function.</p>
                    <p>A system's <span class="math inline">\(g\)</span>-intelligence is how well it optimizes <span class="math inline">\(g\)</span>.</p>
                    <p>What is &quot;how well&quot;?</p>
                    <p>Optimization (extremization) is either minimization or maximization.</p>
                    <h3 id="what-is-a-mathematical-theory-of-intelligence"><span class="section_number">6.4</span><span class="section_title">What is a mathematical theory of intelligence?</span></h3>
                    <p>Here I try an alternative formalization to <span class="citation" data-cites="DefineMachIntel">[<a href="#ref-DefineMachIntel">5</a>]</span>.</p>
                    <p>Let <span class="math inline">\(E\)</span> be a set of <em>environments</em>.</p>
                    <p>Let <span class="math inline">\(G : E \to \Real\)</span> be a <em>goal function</em>. The value of <span class="math inline">\(G(e)\)</span> measures how well the agent performs in environment <span class="math inline">\(e\)</span>.</p>
                    <p>The <em>intelligence</em> of the agent <em>with respect to <span class="math inline">\(G\)</span> across $E$</em> is <span class="math inline">\(\int_E G\)</span>.</p>
                    <p>A <em>performance</em> consists of an agent and an environment.</p>
                    <p>Assumption: The agent cannot modify <span class="math inline">\(G\)</span>.</p>
                    <p>Behavior is a function taking an environment and outputing something.</p>
                    <p>Intelligence is <em>relative</em> to <span class="math inline">\(G\)</span> and <span class="math inline">\(E\)</span>: <em>goal</em> and <em>environment</em>.</p>
                    <p>If we see longevity as intelligence test, then an illiterate farmer who lives to 80 is more intelligent than a scientist who dies at 20, but a rock that has been there for 100 years would even be more intelligent than the farmer.</p>
                    <p>If we see money as intelligence test, then a corrupt politician who steals billions of dollars without getting caught is more intelligent than a honest farmer who only has tens of thousands of dollars.</p>
                    <p>Gaming the system is a sign of intelligence. It is hard to design a goal function that gives the desired outcome without undesired side effects.</p>
                    <p>IQ tests are intelligence measures with small environment set.</p>
                    <p>Lifespan may be an intelligence measure with huge environment set.</p>
                    <p>A human can optimize <em>several</em> goal functions across the same environment set. A human may be asked to clean a floor, to write a report, to run a company, to cook food, and to find the quickest route between home and office, and optimize them all.</p>
                    <p>Some goal functions for humans are (but perhaps not limited to):</p>
                    <ul>
                    <li>Maximize happiness</li>
                    <li>Minimize pain</li>
                    <li>Optimize the level of a chemical in the brain</li>
                    <li>Optimize the time integral of such chemical</li>
                    <li>Maximize the chance of survival</li>
                    </ul>
                    <p>But I don't know the root goal function that explains all those behaviors.</p>
                    <p>Where does the word &quot;intelligence&quot; come from? What is its etymology?</p>
                    <ul>
                    <li>The word &quot;intelligent&quot; comes from a Latin word that means &quot;to choose between&quot; (<a href="http://www.dictionary.com/browse/intelligent">Dictionary.com</a>).</li>
                    </ul>
                    <p>What are some mathematical definitions of intelligence?</p>
                    <ul>
                    <li>&quot;Intelligence measures an agent's ability to achieve goals in a wide range of environments.&quot; [Legg2006][Legg2008]</li>
                    <li><a href="https://www.researchgate.net/publication/323203054_Defining_intelligence">Shour2018</a>: &quot;Defining intelligence as a rate of problem solving and using the concept of network entropy enable measurement, comparison and calculation of collective and individual intelligence and of computational capacity.&quot;</li>
                    <li>Tononi integrated information theory. <a href="https://en.wikipedia.org/wiki/Integrated_information_theory">Wikipedia</a>.</li>
                    <li>Schmidhuber, Hutter, and team have used Solomonoff algorithmic probability and Kolmogorov complexity to define a theoretically optimal predictor they call AIXI.
                    <ul>
                    <li>J&quot;urgen Schmidhuber. <a href="http://www.idsia.ch/~juergen/newai/newai.html">Schmidhuber article</a>.</li>
                    <li><a href="http://www.cs.uic.edu/~piotr/cs594/Prashant-UniversalAI.pdf">Prashant's slides</a>. These define &quot;universal&quot; and &quot;optimal&quot;.</li>
                    </ul></li>
                    <li>Marcus Hutter approached intelligence from <em>algorithmic</em> complexity theory (Solomonoff induction) <span class="citation" data-cites="DefineMachIntel">[<a href="#ref-DefineMachIntel">5</a>]</span>.</li>
                    <li>Warren D. Smith approached intelligence from <em>computational</em> complexity theory (NP-completeness) <span class="citation" data-cites="WdsIntel WdsIntelSlide">[<a href="#ref-WdsIntelSlide">7</a>, <a href="#ref-WdsIntel">8</a>]</span></li>
                    </ul>
                    <p><span class="citation" data-cites="Legg2007Collection">[<a href="#ref-Legg2007Collection">4</a>]</span> is a collection of definitions of intelligence.</p>
                    <h3 id="historical-definitions"><span class="section_number">6.5</span><span class="section_title">Historical definitions</span></h3>
                    <p><a href="https://brocku.ca/MeadProject/sup/Boring_1923.html">Edwin Boring in 1923</a> proposed that we start out by defining intelligence as what intelligence tests measure &quot;until further scientific observation allows us to extend the definition&quot;.</p>
                    <h3 id="what-is-learning"><span class="section_number">6.6</span><span class="section_title">What is learning?</span></h3>
                    <p>There are so many ML algorithms. What's the common thing?</p>
                    <ul>
                    <li>Should I read these?
                    <ul>
                    <li><a href="https://medium.com/machine-learning-world/learning-path-for-machine-learning-engineer-a7d5dc9de4a4">How To Become A Machine Learning Engineer: Learning Path</a></li>
                    <li><a href="https://dzone.com/guides/artificial-intelligence-machine-learning-and-predi">https://dzone.com/guides/artificial-intelligence-machine-learning-and-predi</a></li>
                    </ul></li>
                    <li>What is the relationship between ML and statistical modeling?</li>
                    <li>How do we categorize ML algorithms?
                    <ul>
                    <li>Online vs offline
                    <ul>
                    <li><a href="https://en.wikipedia.org/wiki/Online_machine_learning">Wikipedia: Online machine learning</a></li>
                    </ul></li>
                    <li>Discrete-time model vs continuous-time model
                    <ul>
                    <li>LTI (linear time-invariant) systems</li>
                    </ul></li>
                    <li>Assemble answers from these sources:
                    <ul>
                    <li><a href="https://en.wikipedia.org/wiki/Machine_learning#Approaches">Wikipedia: Machine learning, approaches</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Outline_of_machine_learning#Machine_learning_algorithms">Wikipedia: Outline of machine learning, algorithms</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Outline_of_machine_learning#Machine_learning_methods">Wikipedia: Outline of machine learning, methods</a></li>
                    <li><a href="https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/">A tour of machine learning algorithms</a></li>
                    <li><a href="https://towardsdatascience.com/types-of-machine-learning-algorithms-you-should-know-953a08248861">Types of machine learning algorithms you should know</a></li>
                    <li><a href="https://stats.stackexchange.com/questions/214381/what-exactly-is-the-mathematical-definition-of-a-classifier-classification-alg">Stats SE 214381: mathematical definition of classifier</a></li>
                    <li><a href="https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/">Common machine learning algorithms</a></li>
                    </ul></li>
                    </ul></li>
                    <li>What is a neural network?
                    <ul>
                    <li>A <em>neuron</em> is a function in <span class="math inline">\(\Real^\infty \to \Real\)</span>.</li>
                    <li>A <em>neural network</em> layer is a function in <span class="math inline">\(\Real^\infty \to \Real^\infty\)</span>.</li>
                    <li>Why do neural networks work?
                    <ul>
                    <li><a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">Wikipedia: Universal approximation theorem</a></li>
                    </ul></li>
                    </ul></li>
                    <li>Statistical learning</li>
                    <li>What is backpropagation, from functional analysis point of view?</li>
                    <li>Who are AI/ML researchers and what are their focuses?
                    <ul>
                    <li>Does Geoffrey Hinton specialize in image recognition?</li>
                    </ul></li>
                    <li>What is the relationship between intelligence and compression?</li>
                    <li>Consider endofunctions of infinite-dimensional real tuple space. That is, consider <span class="math inline">\(f, g : \Real^\infty \to \Real^\infty\)</span>.
                    <ul>
                    <li>What is the distance between them?</li>
                    </ul></li>
                    <li>Reductionistically, a brain can be thought as a function in <span class="math inline">\(\Real \to \Real^\infty \to \Real^\infty\)</span>.
                    <ul>
                    <li>The first parameter is time.</li>
                    <li>The second parameter is the sensor signals.</li>
                    <li>The output of the function is the actuator signals.</li>
                    <li>Can we model a brain by such <a href="https://en.wikipedia.org/wiki/Functional_differential_equation">functional differential equation</a> involving <a href="https://en.wikipedia.org/wiki/Functional_derivative">functional derivative</a>s?</li>
                    <li><span class="math inline">\(\norm{f(t+h,x) - f(t,x)} = h \cdot g(t,x)\)</span></li>
                    <li><span class="math inline">\(\norm{f(t+h) - f(t)} = h \cdot g(t)\)</span></li>
                    <li>It seems wrong. Abandon this path. See below.</li>
                    </ul></li>
                    <li>We model the input as a function <span class="math inline">\(x : \Real \to \Real^n\)</span>.</li>
                    <li>We model the output as a function <span class="math inline">\(y : \Real \to \Real^n\)</span>.
                    <ul>
                    <li><span class="math inline">\(\norm{y(t+h) - y(t)} = h \cdot g(t)\)</span></li>
                    <li><span class="math inline">\(y(t+h) - y(t) = h \cdot (dy)(t)\)</span></li>
                    <li><span class="math inline">\(\norm{(dy)(t)} = g(t)\)</span>
                    <ul>
                    <li>There are infinitely many <span class="math inline">\(dy\)</span> that satisfies that. Which one should we choose?</li>
                    </ul></li>
                    <li>If <span class="math inline">\(y : \Real \to \Real^n\)</span> then <span class="math inline">\(dy : \Real \to \Real^n\)</span>.</li>
                    </ul></li>
                    <li>A classifier is a function in <span class="math inline">\(\Real^\infty \to \Real\)</span>.</li>
                    <li>A control system snapshot is a function in <span class="math inline">\(\Real^\infty \to \Real^\infty\)</span>.</li>
                    <li>A control system is a function in <span class="math inline">\(\Real \to \Real^\infty \to \Real^\infty\)</span>.</li>
                    <li>How does <span class="math inline">\(F\)</span> have memory if <span class="math inline">\(F(t) = \int_0^t f(x) ~ dx\)</span>?</li>
                    </ul>
                    <p>Why has AI mastered chess, but not real life? Because chess search space is much smaller than real-life search space.</p>
                    <h3 id="what-is-ai"><span class="section_number">6.7</span><span class="section_title">What is AI?</span></h3>
                    <ul>
                    <li>In the 1950s, AI was whatever McCarthy et al. were doing.
                    <ul>
                    <li>&quot;McCarthy coined the term 'artificial intelligence' in 1955, and organized the famous Dartmouth Conference in Summer 1956. This conference started AI as a field.&quot; (<a href="https://en.wikipedia.org/wiki/John_McCarthy_(computer_scientist)">WP: John McCarthy (computer scientist)</a>)</li>
                    <li><a href="https://en.wikipedia.org/wiki/Dartmouth_workshop">WP: Dartmouth workshop</a></li>
                    <li><a href="http://raysolomonoff.com/dartmouth/">Ray Solomonoff's Dartmouth archives</a></li>
                    </ul></li>
                    <li>What are AI approaches? How are we trying to make an AI?
                    <ul>
                    <li>Pedro Domingos categorizes AI approaches into five <em>tribes</em>:
                    <ul>
                    <li>symbolists (symbolic logic)</li>
                    <li>connectionists (neural networks)</li>
                    <li>evolutionaries (genetic algorithms)</li>
                    <li>bayesians (statistical learning, probabilistic inference)</li>
                    <li>analogizers (what is this?)</li>
                    </ul></li>
                    </ul></li>
                    <li>How do we measure intelligence? How do we measure the performance of a learning algorithm?
                    <ul>
                    <li><a href="https://en.wikipedia.org/wiki/Computational_learning_theory">Wikipedia: Computational learning theory</a>
                    <ul>
                    <li>What is the goal of computational learning theory?
                    <ul>
                    <li>&quot;Give a rigorous, computationally detailed and plausible account of how learning can be done.&quot; [Angluin1992]</li>
                    </ul></li>
                    <li>&quot;a subfield of Artificial Intelligence devoted to studying the design and analysis of machine learning algorithms&quot;</li>
                    <li>What is a mathematical theory of learning?
                    <ul>
                    <li>What is learning?
                    <ul>
                    <li>2018-04-19: &quot;To learn something&quot; is to get better at it. Usually learning uses experience.
                    <ul>
                    <li>What is the formal definition of &quot;get better&quot;?
                    <ul>
                    <li>Let there be a system. Pick a task. Pick a time interval. Test the system several times throughout the time interval. Let the test results be the sequence <span class="math inline">\(X = x_1, x_2, \ldots, x_n\)</span>. We say that the system is <em>learning</em> the task in the time interval iff <span class="math inline">\(x_1 &lt; x_2 &lt; \ldots &lt; x_n\)</span> (that is iff <span class="math inline">\(X\)</span> is a monotonically increasing sequence).</li>
                    <li>How do we formalize &quot;get better&quot; and &quot;experience&quot;?
                    <ul>
                    <li>&quot;Get better&quot; can be modeled by <em>monotonically increasing score</em></li>
                    <li>&quot;Experience&quot; can be modeled by a sequence</li>
                    </ul></li>
                    </ul></li>
                    <li>Is experience (past data) necessary for learning? Are mistakes necessary for learning?</li>
                    </ul></li>
                    <li>Supervised learning is extrapolating a function from finite samples. Usually, the function is high-dimensional, and the samples are few.</li>
                    <li>It is simple to measure learning success in perfect information games such as chess. Chess also doesn't require any sensors and motors.</li>
                    </ul></li>
                    </ul></li>
                    </ul></li>
                    </ul></li>
                    </ul>
                    <h2 id="abbreviations"><span class="section_number">7</span><span class="section_title">Abbreviations</span></h2>
                    <ul>
                    <li>AI: Artificial Intelligence</li>
                    <li>ML: Machine Learning</li>
                    <li>COLT: Computational Learning Theory</li>
                    </ul>
                    <h2 id="surveys-reviews-positions-and-expositions"><span class="section_number">8</span><span class="section_title">Surveys, reviews, positions, and expositions</span></h2>
                    <ul>
                    <li>Google query: most recent mathematical ai book</li>
                    <li><a href="http://eliassi.org/COLTSurveyArticle.pdf">http://eliassi.org/COLTSurveyArticle.pdf</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Computational_learning_theory#Surveys">WP: COLT surveys</a></li>
                    <li><a href="http://www.cs.ox.ac.uk/people/varun.kanade/teaching/CLT-HT2018/lectures/">COLT lecture 2018</a></li>
                    <li>Book: &quot;An Introduction to Computational Learning Theory&quot; by Kearns and Vazirani</li>
                    <li><a href="https://mitpress.mit.edu/books/introduction-computational-learning-theory">https://mitpress.mit.edu/books/introduction-computational-learning-theory</a></li>
                    </ul>
                    <div class="local_table_of_contents">
                    <ul>
                    <li><span class="section_number">8.1</span><span class="section_title"><a href="#plan">Plan</a></span><span class="word_count">(71w~1m)</span></li>
                    <li><span class="section_number">8.2</span><span class="section_title"><a href="#questions">Questions</a></span><span class="word_count">(36w~1m)</span></li>
                    <li><span class="section_number">8.3</span><span class="section_title"><a href="#note-to-self">Note to self</a></span><span class="word_count">(356w~2m)</span></li>
                    <li><span class="section_number">8.4</span><span class="section_title"><a href="#others">Others</a></span><span class="word_count">(242w~2m)</span></li>
                    <li><span class="section_number">8.5</span><span class="section_title"><a href="#non-prioritized-questions">Non-prioritized questions</a></span><span class="word_count">(74w~1m)</span></li>
                    <li><span class="section_number">8.6</span><span class="section_title"><a href="#how-might-we-build-a-seed-ai">How might we build a seed AI?</a></span><span class="word_count">(49w~1m)</span></li>
                    <li><span class="section_number">8.7</span><span class="section_title"><a href="#guesses">Guesses</a></span><span class="word_count">(23w~1m)</span></li>
                    <li><span class="section_number">8.8</span><span class="section_title"><a href="#undigested-information">Undigested information</a></span><span class="word_count">(948w~5m)</span></li>
                    </ul>
                    </div>
                    <h3 id="plan"><span class="section_number">8.1</span><span class="section_title">Plan</span></h3>
                    <ul>
                    <li><p>Read about universal intelligence</p>
                    <ul>
                    <li><p>Pamela McCorduck's &quot;Machines who think&quot; for some history</p>
                    <ul>
                    <li><a href="https://en.wikipedia.org/wiki/Timeline_of_artificial_intelligence">WP: Timeline of artificial intelligence</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Progress_in_artificial_intelligence">WP: Progress in artificial intelligence</a></li>
                    </ul></li>
                    <li>[Hutter2005Book]</li>
                    <li><p><a href="http://www.hutter1.net/ai/uaibook.htm">hutter1.net…uaibook.htm</a></p>
                    <ul>
                    <li>He formulated the &quot;degree of intelligence&quot; in 2005</li>
                    <li>(edited) &quot;AIXI […] learns by eliminating Turing machines […] once they become inconsistent with the progressing history.&quot;</li>
                    </ul></li>
                    <li><a href="http://www.hutter1.net/ai/suaibook.pdf">Presentation, 393 slides</a></li>
                    <li><a href="http://users.cecs.anu.edu.au/~ssanner/MLSS2010/Hutter1.pdf">Slides</a>, maybe a draft of the above.</li>
                    <li>Shane Legg's PhD thesis &quot;Machine super intelligence&quot; [Legg2008]</li>
                    <li><a href="http://www.vetta.org/documents/universal_intelligence_abstract_ai50.pdf">Legg and Hutter: A formal definition of intelligence for artificial systems</a></li>
                    <li><p>2005 Negnevitsky AI book <span class="citation" data-cites="negnevitsky2005artificial">[<a href="#ref-negnevitsky2005artificial">6</a>]</span>?</p></li>
                    </ul></li>
                    </ul>
                    <h3 id="questions"><span class="section_number">8.2</span><span class="section_title">Questions</span></h3>
                    <ul>
                    <li><p>COLT</p>
                    <ul>
                    <li><p>Should we read this?</p>
                    <ul>
                    <li><a href="https://arxiv.org/abs/1405.1513">Ibrahim Alabdulmohsin: A Mathematical Theory of Learning</a></li>
                    <li>1999: <a href="http://www.cis.syr.edu/people/royer/stl2e/">Sanjay Jain et al.: Systems that learn</a></li>
                    <li><a href="https://www.quora.com/What-are-the-best-math-books-for-machine-learning">https://www.quora.com/What-are-the-best-math-books-for-machine-learning</a></li>
                    <li><a href="https://machinelearningwithvick.quora.com/Learning-about-machine-learning">https://machinelearningwithvick.quora.com/Learning-about-machine-learning</a></li>
                    <li><a href="http://web.archive.org/web/20101102210231/http://measuringmeasures.com/blog/2010/1/15/learning-about-statistical-learning.html">http://web.archive.org/web/20101102210231/http://measuringmeasures.com/blog/2010/1/15/learning-about-statistical-learning.html</a></li>
                    <li><a href="https://www.quora.com/Which-are-the-best-books-to-get-the-Math-background-for-Machine-Learning">https://www.quora.com/Which-are-the-best-books-to-get-the-Math-background-for-Machine-Learning</a></li>
                    <li><a href="https://www.quora.com/How-do-I-learn-mathematics-for-machine-learning?share=1">https://www.quora.com/How-do-I-learn-mathematics-for-machine-learning?share=1</a></li>
                    </ul></li>
                    <li><p><a href="http://emis.ams.org/journals/TAC/reprints/articles/22/tr22.pdf">http://emis.ams.org/journals/TAC/reprints/articles/22/tr22.pdf</a></p>
                    <ul>
                    <li><a href="https://www.quora.com/What-are-some-survey-papers-on-artificial-intelligence-and-deep-learning">https://www.quora.com/What-are-some-survey-papers-on-artificial-intelligence-and-deep-learning</a></li>
                    <li><a href="http://people.idsia.ch/~juergen/deep-learning-conspiracy.html">http://people.idsia.ch/~juergen/deep-learning-conspiracy.html</a></li>
                    <li><a href="https://arxiv.org/abs/1404.7828">Jürgen Schmidhuber: &quot;Deep Learning in Neural Networks: An Overview&quot;</a></li>
                    <li><a href="http://www.ijircce.com/upload/2017/june/107_A%20Survey.pdf">http://www.ijircce.com/upload/2017/june/107_A%20Survey.pdf</a></li>
                    </ul></li>
                    </ul></li>
                    </ul>
                    <p>Should we read these?</p>
                    <p>2017, <a href="https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/building-machines-that-learn-and-think-for-themselves/E28DBFEC380D4189FB7754B50066A96F">Building machines that learn and think for themselves</a></p>
                    <h3 id="note-to-self"><span class="section_number">8.3</span><span class="section_title">Note to self</span></h3>
                    <ul>
                    <li><p>Which AI architecture has won lots of AI contests lately?</p>
                    <ul>
                    <li>Is it LSTM RNN?</li>
                    <li><p>What is LSTM RNN?</p>
                    <ul>
                    <li>&quot;long short-term memory recurrent neural network&quot;</li>
                    <li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></li>
                    <li>&quot;The expression <em>long short-term</em> refers to the fact that LSTM is a model for the <em>short-term memory</em> which can last for a <em>long</em> period of time.&quot; (<a href="https://en.wikipedia.org/wiki/Long_short-term_memory">Wikipedia</a>)</li>
                    </ul></li>
                    </ul></li>
                    <li><p>How do we learn amid lies, deception, disinformation, misinformation?</p>
                    <ul>
                    <li>Related to adversarial learning? <a href="https://en.wikipedia.org/wiki/Adversarial_machine_learning">https://en.wikipedia.org/wiki/Adversarial_machine_learning</a> ?</li>
                    </ul></li>
                    <li><p>What are some tools that I can use to make my computer learn?</p>
                    <ul>
                    <li>Google TensorFlow</li>
                    <li>Does OpenAI have tools?</li>
                    </ul></li>
                    <li>TODO s/adapt/habituate</li>
                    <li>Let <span class="math inline">\(f(t,x)\)</span> be the system's response intensity for stimulus intensity <span class="math inline">\(x\)</span> at time <span class="math inline">\(t\)</span>. We say the system is <em>habituating</em> between the time <span class="math inline">\(t_1\)</span> and <span class="math inline">\(t_2\)</span> iff <span class="math inline">\(f(t_1,x) &gt; f(t_2,x)\)</span> for all stimulus intensity <span class="math inline">\(x\)</span>.</li>
                    <li>&quot;The habituation process is a form of adaptive behavior (or neuroplasticity) that is classified as non-associative learning.&quot; <a href="https://en.wikipedia.org/wiki/Habituation">https://en.wikipedia.org/wiki/Habituation</a></li>
                    <li><p>How many AI approaches are there?</p>
                    <ul>
                    <li><a href="https://en.wikipedia.org/wiki/Portal:Artificial_intelligence">WP AI Portal</a> lists 4 approaches</li>
                    <li>Pedro Domingos lists 5 &quot;tribes&quot;</li>
                    </ul></li>
                    <li><p>(merge AI researchers)</p>
                    <ul>
                    <li><a href="https://en.wikipedia.org/wiki/Portal:Artificial_intelligence">WP AI Portal</a> lists several leading AI researchers</li>
                    </ul></li>
                    <li>2000, György Turán, <a href="https://link.springer.com/article/10.1023%2FA%3A1018948021083">Remarks on COLT</a></li>
                    <li><p>2016, Krendzelak, Jakab, <a href="https://ieeexplore.ieee.org/document/7802092/">Fundamental principals of Computational Learning Theory</a></p>
                    <ul>
                    <li><p>Reading queue:</p>
                    <ul>
                    <li>D. Angluin, C. Smith, &quot;Inductive inference: theory and methods&quot;, A.C.M. Computing Surveys, vol. 15, pp. 237-269, 1983.</li>
                    <li>M. Anthony, N. Biggs, &quot;Computational Learning Theory&quot; in , Cambridge university press, 1992.</li>
                    <li>M.J. Kearns, &quot;The computational Complexity of Machine Learning&quot; in , The MIT Press, May 1990.</li>
                    <li>L.G. Valiant, &quot;A theory of the learnable&quot;, Communications of the A.C.M., vol. 27, no. 11, pp. 1134-1142, 1984.</li>
                    <li>L. Pitt, L.G. Valiant, &quot;Computational limitations on learning from examples&quot;, Journal of the A.C.M., vol. 35, no. 4, pp. 965-984, 1988.</li>
                    </ul></li>
                    </ul></li>
                    <li>helpful slides <a href="https://cs.uwaterloo.ca/~klarson/teaching/W15-486/lectures/22Colt.pdf">https://cs.uwaterloo.ca/~klarson/teaching/W15-486/lectures/22Colt.pdf</a></li>
                    <li>Bertoni et al. http://elearning.unimib.it/pluginfile.php/283303/mod<sub>resource</sub>/content/1/Apprendimento<sub>Automatico</sub>/Computational<sub>Learning</sub>.pdf</li>
                    <li><a href="https://stats.stackexchange.com/questions/142906/what-does-pac-learning-theory-mean">https://stats.stackexchange.com/questions/142906/what-does-pac-learning-theory-mean</a></li>
                    <li><a href="https://pdfs.semanticscholar.org/presentation/fbbd/65646c8a81094864d4e0b0dfb9c1f22181af.pdf">https://pdfs.semanticscholar.org/presentation/fbbd/65646c8a81094864d4e0b0dfb9c1f22181af.pdf</a></li>
                    <li><a href="http://web.cs.iastate.edu/~honavar/colt-tutorial.pdf">http://web.cs.iastate.edu/~honavar/colt-tutorial.pdf</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Probably_approximately_correct_learning#cite_note-valiant-1">https://en.wikipedia.org/wiki/Probably_approximately_correct_learning#cite_note-valiant-1</a> A Theory of the Learnable Leslie G. Valiant 1984 <a href="http://web.mit.edu/6.435/www/Valiant84.pdf">http://web.mit.edu/6.435/www/Valiant84.pdf</a></li>
                    <li>kearns vazirani introduction <a href="ftp://ftp.cis.upenn.edu/pub/cse140/public_html/2002/kvpages.pdf">ftp://ftp.cis.upenn.edu/pub/cse140/public_html/2002/kvpages.pdf</a></li>
                    <li><a href="http://www.cis.upenn.edu/~mkearns/">http://www.cis.upenn.edu/~mkearns/</a> the computational complexity of machine learning <a href="http://www.cis.upenn.edu/~mkearns/papers/thesis.pdf">http://www.cis.upenn.edu/~mkearns/papers/thesis.pdf</a> <a href="https://www.worldscientific.com/worldscibooks/10.1142/10175">https://www.worldscientific.com/worldscibooks/10.1142/10175</a></li>
                    <li>2015 <a href="http://www.cs.tufts.edu/~roni/Teaching/CLT/">http://www.cs.tufts.edu/~roni/Teaching/CLT/</a></li>
                    <li>probably link to this <a href="http://bactra.org/notebooks/learning-theory.html">http://bactra.org/notebooks/learning-theory.html</a></li>
                    <li>semantics-first <a href="https://pdfs.semanticscholar.org/83e7/b615c165209af54dd0fe05c850bb08232625.pdf">https://pdfs.semanticscholar.org/83e7/b615c165209af54dd0fe05c850bb08232625.pdf</a></li>
                    <li>discrete approximation theory see the references of this paper <a href="https://www.worldscientific.com/doi/suppl/10.1142/10175/suppl_file/10175_chap01.pdf">https://www.worldscientific.com/doi/suppl/10.1142/10175/suppl_file/10175_chap01.pdf</a></li>
                    <li><p><a href="https://profs.info.uaic.ro/~ciortuz/SLIDES/ml7.pdf">https://profs.info.uaic.ro/~ciortuz/SLIDES/ml7.pdf</a></p></li>
                    </ul>
                    <p>Optimal learning for humans <a href="https://www.kqed.org/mindshift/37289">https://www.kqed.org/mindshift/37289</a></p>
                    <p>Curate from this <a href="https://thesecondprinciple.com/optimal-learning/">https://thesecondprinciple.com/optimal-learning/</a></p>
                    <p>Boston dynamics dog robots</p>
                    <p>Tesla car autopilots</p>
                    <p>Google and Uber self-driving cars</p>
                    <p><a href="https://www.quora.com/Will-we-ever-have-a-rigorous-and-robust-definition-for-intelligence">https://www.quora.com/Will-we-ever-have-a-rigorous-and-robust-definition-for-intelligence</a></p>
                    <p>rigorous definition of intelligence The new ai is general and rigorous, idsia Toward a theory of intelligence,RAND</p>
                    <p>A system responds to a stimulus. Define: a system is <em>adapting</em> to a stimulus if the same stimulus level elicits decreasing response level from the system. The stimulus level has to be increased to maintain the response level.</p>
                    <p>Is learning = adapting? Is intelligence = adaptiveness?</p>
                    <h3 id="others"><span class="section_number">8.4</span><span class="section_title">Others</span></h3>
                    <ul>
                    <li><p>What are some expository works in AI?</p>
                    <ul>
                    <li><a href="https://www.sciencedirect.com/science/article/pii/S1574013717300606">The evolution of sentiment analysis—A review of research topics, venues, and top cited papers</a></li>
                    </ul></li>
                    <li><p>What are the trends in AI?</p>
                    <ul>
                    <li><p><a href="https://twitter.com/michael_nielsen/status/983502409325395969">Michael Nielsen's tweet</a>: &quot;I meet lots of people who tell me fatalistically (&amp; often despondently) that it's near impossible to do important work on neural nets today, unless you have huge compute and huge data sets.&quot;</p>
                    <ul>
                    <li><a href="https://arxiv.org/abs/1712.00409">Deep Learning Scaling is Predictable, Empirically</a></li>
                    </ul></li>
                    </ul></li>
                    <li><p>Should we read this?</p>
                    <ul>
                    <li><a href="http://www.cs.cmu.edu/~16831-f12/notes/F11/16831_lecture15_shorvath.pdf">Boosting: Gradient descent in function space</a></li>
                    <li><a href="http://alessio.guglielmi.name/res/cos/">Alessio Guglielmi's deep inference</a></li>
                    <li><a href="https://arxiv.org/abs/1412.1044">Problem theory, Ramón Casares</a></li>
                    </ul></li>
                    <li><p>EcoBot is a robot that can feed itself.</p>
                    <ul>
                    <li><a href="https://en.wikipedia.org/wiki/EcoBot">Wikipedia: EcoBot</a>: &quot;a class of energetically autonomous robots that can remain self-sustainable by collecting their energy from material, mostly waste matter, in the environment&quot;</li>
                    </ul></li>
                    <li><a href="https://www.sciencedaily.com/releases/2016/04/160427081533.htm">A single-celled organism capable of learning</a>: protists may learn by habituation</li>
                    <li><p>Selected threads from /r/artificial:</p>
                    <ul>
                    <li><a href="https://www.reddit.com/r/artificial/comments/8begcv/what_are_some_of_the_best_books_on_artificial/">What are some of the best books on AI/ML?</a></li>
                    <li><a href="https://www.reddit.com/r/artificial/comments/8bzrmd/math_phd_want_to_learn_more_about_ai_what_to_read/">Math PhD. Want to learn more about AI. What to read?</a></li>
                    </ul></li>
                    <li><p>What is so bad about human extinction?</p>
                    <ul>
                    <li>If you are nihilist, then there is nothing inherently bad about human extinction.</li>
                    </ul></li>
                    <li>What is the question?</li>
                    <li>How do we make an AI?</li>
                    <li>How do we create a seed AI?</li>
                    <li><p>History questions:</p>
                    <ul>
                    <li>Why was Raymond J. Solomonoff <span class="citation" data-cites="SolAlpProb2011 GacsVitanyiSolomonoff">[<a href="#ref-GacsVitanyiSolomonoff">1</a>, <a href="#ref-SolAlpProb2011">9</a>]</span> interested in predicting sequences of bits? What was he interested in? What was he trying to do?</li>
                    </ul></li>
                    <li><p>Mathematical spaces</p>
                    <ul>
                    <li>What is a metric?</li>
                    <li>What is a norm?</li>
                    <li>What is a measure?</li>
                    <li><a href="https://en.wikipedia.org/wiki/Space_(mathematics)#Three_taxonomic_ranks">https://en.wikipedia.org/wiki/Space_(mathematics)#Three_taxonomic_ranks</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Topological_space#Classification_of_topological_spaces">https://en.wikipedia.org/wiki/Topological_space#Classification_of_topological_spaces</a></li>
                    <li><p><a href="https://en.wikipedia.org/wiki/Functional_analysis">https://en.wikipedia.org/wiki/Functional_analysis</a></p>
                    <ul>
                    <li>What is a Hilbert space?</li>
                    <li>What is a Banach space?</li>
                    <li>What is a Sobolev space?</li>
                    <li><p>What is a measure?</p>
                    <ul>
                    <li><p>What is a Lebesgue measure?</p>
                    <ul>
                    <li><p>What is an Lp space?</p>
                    <ul>
                    <li><a href="https://en.wikipedia.org/wiki/Lp_space#Lp_spaces">Wikipedia: Lp space</a></li>
                    <li><p>How is it pronounced?</p>
                    <ul>
                    <li>&quot;Lebesgue space with <span class="math inline">\(p\)</span>-norm&quot;</li>
                    </ul></li>
                    </ul></li>
                    <li><p>What is a small lp space?</p></li>
                    </ul></li>
                    </ul></li>
                    </ul></li>
                    </ul></li>
                    </ul>
                    <h3 id="non-prioritized-questions"><span class="section_number">8.5</span><span class="section_title">Non-prioritized questions</span></h3>
                    <ul>
                    <li><p>What is AI? Why should I care?</p>
                    <ul>
                    <li>AI is the way for us to become gods.</li>
                    </ul></li>
                    <li><p>What is the relationship between AI and ML?</p>
                    <ul>
                    <li><p>ML is a subset of AI.</p>
                    <ul>
                    <li><p>Then what is the rest of AI that is not ML?</p>
                    <ul>
                    <li>Ethics? Philosophy? Rule systems?</li>
                    <li><a href="https://ai.stackexchange.com/questions/35/what-is-the-difference-between-artificial-intelligence-and-machine-learning">AI SE 35: What is the difference between artificial intelligence and machine learning?</a></li>
                    <li>What is intelligence without learning? Non-adaptive intelligence? Static intelligence?</li>
                    </ul></li>
                    </ul></li>
                    </ul></li>
                    <li>What is a cyborg?</li>
                    <li><p>If human goal function is survival, then why exists suicide?</p>
                    <ul>
                    <li>Evolutionary noise?</li>
                    </ul></li>
                    </ul>
                    <p><a href="https://en.wikipedia.org/wiki/Universal_Darwinism">https://en.wikipedia.org/wiki/Universal_Darwinism</a></p>
                    <h3 id="how-might-we-build-a-seed-ai"><span class="section_number">8.6</span><span class="section_title">How might we build a seed AI?</span></h3>
                    <ul>
                    <li>Use off-the-shelf computers.</li>
                    <li>Use supercomputers.</li>
                    <li>Use clusters.</li>
                    <li>Use computers over the Internet.</li>
                    <li>Raise an AI like raising a child.</li>
                    <li><p>Evolve a system. Create an environment with selection pressure. Run it long enough.</p>
                    <ul>
                    <li><a href="https://en.wikipedia.org/wiki/Evolutionary_robotics">WP: Evolutionary robotics</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Evolutionary_computation">WP: Evolutionary computation</a></li>
                    </ul></li>
                    <li><p>What is TensorFlow? Keras? CNTK? Theano?</p>
                    <ul>
                    <li>The building blocks of AI? Standardized AI components?</li>
                    </ul></li>
                    </ul>
                    <h3 id="guesses"><span class="section_number">8.7</span><span class="section_title">Guesses</span></h3>
                    <p>In the future, there are only two kinds of jobs: telling machines to do things, and being told to do things by machines.</p>
                    <h3 id="undigested-information"><span class="section_number">8.8</span><span class="section_title">Undigested information</span></h3>
                    <ul>
                    <li><a href="https://kevinbinz.com/2017/08/13/ml-five-tribes/">kevinbinz.com: Five Tribes of Machine Learning</a>, part of <a href="https://kevinbinz.com/2017/05/09/sequence-machine-learning/">machine learning sequence</a>, some contents from Pedro Domingos's book &quot;The master algorithm&quot;</li>
                    <li><a href="http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html">Introducing state of the art text classification with universal language models</a></li>
                    <li><p>Summary of Pedro Domingos's book &quot;The master algorithm&quot;</p>
                    <ul>
                    <li>Sparse autoencoders (p. 116).</li>
                    <li>&quot;A nugget of knowledge so incontestable, so fundamental, that we can build all induction on top of it&quot; (p. 64) in Chapter 9.</li>
                    <li>Induction is the inverse of deduction, as subtraction is the inverse of addition. (Is this a quote from the book?)</li>
                    <li>EM (expectation maximization) algorithm (p. 209).</li>
                    <li>Metalearning (p. 237).</li>
                    <li>A classifier that classifies by combining the output of subclassifiers.</li>
                    <li><a href="http://homes.cs.washington.edu/~pedrod/papers/mlj05.pdf">Markov logic network</a> (p. 246) named <a href="Alchemy"><a href="http://alchemy.cs.washington.edu/">http://alchemy.cs.washington.edu/</a></a> (p. 250)</li>
                    </ul></li>
                    <li>Harvard University the graduate school of arts and sciences: <a href="http://sitn.hms.harvard.edu/flash/2017/history-artificial-intelligence/">Rockwell Anyoha: History of AI</a></li>
                    <li><a href="http://jacques.pitrat.pagesperso-orange.fr/">Jacques Pitrat</a> and his CAIA, bootstrapping AI with AI.</li>
                    <li><a href="http://www.hutter1.net/ai/uaibook.htm">Marcus Hutter book: Universal Artificial Intelligence: Sequential Decisions based on Algorithmic Probability</a> and the <a href="http://www.hutter1.net/ai/suaibook.pdf">slides</a>.</li>
                    <li><p><a href="http://math.bu.edu/people/mkon/V5Fin.pdf">Mark A. Kon, Louise A. Raphael, Daniel A. Williams: Extending Girosi's approximation estimates for functions in Sobolev spaces via statistical learning theory</a></p>
                    <ul>
                    <li>&quot;Girosi [8] established an interesting connection between statistical learning theory (SLT) and approximation theory, showing that SLT methods can be used to prove results of a purely approximation theoretic nature.&quot;</li>
                    </ul></li>
                    <li>Speech synthesizer using hidden Markov model? Someone must have done it. Find the paper.</li>
                    <li>ISIR (International Society for Intelligence Research) human intelligence research <a href="http://www.isironline.org/resources/teaching-pages/">teaching pages</a>.</li>
                    <li><a href="https://en.wikipedia.org/wiki/Artificial_life">https://en.wikipedia.org/wiki/Artificial_life</a></li>
                    <li>What is the simplest life form? (2008) <a href="https://www.quora.com/What-is-the-simplest-life-form">https://www.quora.com/What-is-the-simplest-life-form</a></li>
                    <li><a href="https://stats.stackexchange.com/questions/142906/what-does-pac-learning-theory-mean">https://stats.stackexchange.com/questions/142906/what-does-pac-learning-theory-mean</a></li>
                    <li><p><a href="https://brenocon.com/blog/2008/12/statistics-vs-machine-learning-fight/">https://brenocon.com/blog/2008/12/statistics-vs-machine-learning-fight/</a></p>
                    <ul>
                    <li>YC thread for that <a href="https://news.ycombinator.com/item?id=4927168">https://news.ycombinator.com/item?id=4927168</a></li>
                    </ul></li>
                    <li><a href="https://www.quora.com/What-are-the-most-important-foundational-papers-in-artificial-intelligence-machine-learning">Quora: What are the most important, foundational papers in artificial intelligence/machine learning?</a></li>
                    <li>JAIR (Journal of Artificial Intelligence Research): <a href="https://www.jair.org/index.php/jair/navigationMenu/view/IJCAIJAIR">IJCAI-JAIR awards</a></li>
                    <li>Schmidhuber, <a href="http://people.idsia.ch/~juergen/fastestuniverse.pdf">The Fastest Way of Computing All Universes</a></li>
                    <li><p><a href="http://raysolomonoff.com/dartmouth/">Dartmouth AI archives</a></p>
                    <ul>
                    <li><a href="http://raysolomonoff.com/publications/indinf56.pdf">Solomonoff, &quot;An inductive inference machine&quot;</a></li>
                    </ul></li>
                    <li><p>Shane Legg, Joel Veness: algorithmic intelligence quotient</p>
                    <ul>
                    <li><a href="https://github.com/mathemajician/AIQ">https://github.com/mathemajician/AIQ</a></li>
                    <li>An Approximation of the Universal Intelligence Measure by Shane Legg and Joel Veness, 2011</li>
                    </ul></li>
                    <li><a href="https://courses.cs.washington.edu/courses/csep590/06au/projects/history-ai.pdf">History of AI</a>, University of Washington, History of Computing, CSEP 590A</li>
                    <li><a href="https://en.wikipedia.org/wiki/Timeline_of_artificial_intelligence">WP: Timeline of AI</a></li>
                    <li><a href="https://www.quantamagazine.org/why-self-taught-artificial-intelligence-has-trouble-with-the-real-world-20180221/">https://www.quantamagazine.org/why-self-taught-artificial-intelligence-has-trouble-with-the-real-world-20180221/</a></li>
                    <li><a href="http://news.mit.edu/2010/ai-unification">http://news.mit.edu/2010/ai-unification</a></li>
                    <li><a href="http://airesearch.com/">http://airesearch.com/</a></li>
                    <li><a href="https://theconversation.com/understanding-the-four-types-of-ai-from-reactive-robots-to-self-aware-beings-67616">https://theconversation.com/understanding-the-four-types-of-ai-from-reactive-robots-to-self-aware-beings-67616</a></li>
                    <li><a href="https://artificialintelligence.id/">https://artificialintelligence.id/</a></li>
                    <li><a href="https://www.asianscientist.com/2017/09/academia/indonesia-ai-nvidia-binus-kinetica/">https://www.asianscientist.com/2017/09/academia/indonesia-ai-nvidia-binus-kinetica/</a></li>
                    <li><a href="https://arxiv.org/abs/1206.5533">Practical recommendations for gradient-based training of deep architectures</a></li>
                    <li><a href="https://arxiv.org/abs/1604.06737">Entity Embeddings of Categorical Variables</a></li>
                    <li>Google Colab</li>
                    <li><a href="https://qz.com/1172431/artificial-intelligence-ai-should-be-raised-like-children-not-computers/">https://qz.com/1172431/artificial-intelligence-ai-should-be-raised-like-children-not-computers/</a></li>
                    <li><p>RNN, LSTM, GRU</p>
                    <ul>
                    <li>RNN is recurrent neural network.</li>
                    <li>LSTM is a kind of RNN.</li>
                    <li>GRU is a kind of RNN.</li>
                    <li><a href="https://jhui.github.io/2017/03/15/RNN-LSTM-GRU/">https://jhui.github.io/2017/03/15/RNN-LSTM-GRU/</a></li>
                    </ul></li>
                    <li><a href="http://web.mit.edu/tslvr/www/lessons_two_years.html">http://web.mit.edu/tslvr/www/lessons_two_years.html</a></li>
                    <li><a href="https://gallery.mailchimp.com/dc3a7ef4d750c0abfc19202a3/files/93e40657-1adb-4891-94ad-c65dda68061f/Ng_MLY01_02.pdf">https://gallery.mailchimp.com/dc3a7ef4d750c0abfc19202a3/files/93e40657-1adb-4891-94ad-c65dda68061f/Ng_MLY01_02.pdf</a></li>
                    <li><a href="https://www.reddit.com/r/MachineLearning/comments/73n9pm/d_confession_as_an_ai_researcher_seeking_advice/#bottom-comments">https://www.reddit.com/r/MachineLearning/comments/73n9pm/d_confession_as_an_ai_researcher_seeking_advice/#bottom-comments</a></li>
                    <li><a href="http://www.inf.ed.ac.uk/teaching/courses/mlpr/2017/notes/w6b_netflix_prize.html">netflix prize, part of MLPR class notes</a></li>
                    <li><p>Scott M. Lundberg, Su-In Lee: A Unified Approach to Interpreting Model Predictions</p>
                    <ul>
                    <li><a href="http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf">http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf</a></li>
                    <li><a href="https://github.com/slundberg/shap">https://github.com/slundberg/shap</a></li>
                    </ul></li>
                    <li><a href="https://www.datascience.com/blog/introduction-to-bayesian-inference-learn-data-science-tutorials">datascience.com: Introduction to Bayesian Inference</a></li>
                    <li><a href="http://www.fc.uaem.mx/~bruno/material/brooks_87_representation.pdf">1987, Intelligence without representation, Rodney A. Brooks</a></li>
                    <li><a href="http://colah.github.io/posts/2015-08-Backprop/">colah.github.io: Backprop</a></li>
                    <li>google search &quot;ai theory research&quot;</li>
                    <li><a href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.2.4835">2002, PhotoTOC: Automatic Clustering for Browsing Personal Photographs, by John C. Platt, Mary Czerwinski, Brent A. Field</a></li>
                    <li><p>philosophy of learning</p>
                    <ul>
                    <li><a href="http://learning.media.mit.edu/content/publications/EA.Piaget%20_%20Papert.pdf">Piaget's constructivism vs Papert's constructionism</a>, Edith Ackermann</li>
                    </ul></li>
                    <li><a href="https://arxiv.org/abs/1508.01084">2015, Deep Convolutional Networks are Hierarchical Kernel Machines</a></li>
                    <li><a href="https://www.youtube.com/watch?v=F5Z52jl4yHQ">Michio Kaku: Who is right about A.I.: Mark Zuckerberg or Elon Musk?</a></li>
                    <li><a href="https://stats.stackexchange.com/questions/104385/assigning-meaningful-cluster-name-automatically">Stats SE 104385: text processing: assigning meaningful cluster name automatically</a></li>
                    <li>The mathematics of deep learning (a website)</li>
                    <li>Can AI be used to upscale old audio/video recordings? Fix deteriorated pictures, films, documents? Color old pictures, photos, films? &quot;Modernize&quot; past artifacts? Digital restoration of archives?</li>
                    <li><p>brain-computer interface</p>
                    <ul>
                    <li><p>pop science</p>
                    <ul>
                    <li><a href="https://www.youtube.com/watch?v=P29EXskk9oU">How Brain Waves Can Control Physical Objects</a></li>
                    </ul></li>
                    </ul></li>
                    <li><p>machine learning</p>
                    <ul>
                    <li>confusion matrix</li>
                    <li><p>algebra of words</p>
                    <ul>
                    <li><a href="https://medium.com/@erushton214/a-simple-spell-checker-built-from-word-vectors-9f28452b6f26">https://medium.com/@erushton214/a-simple-spell-checker-built-from-word-vectors-9f28452b6f26</a></li>
                    </ul></li>
                    <li><a href="https://www.datasciencecentral.com/profiles/blogs/crisp-dm-a-standard-methodology-to-ensure-a-good-outcome">https://www.datasciencecentral.com/profiles/blogs/crisp-dm-a-standard-methodology-to-ensure-a-good-outcome</a></li>
                    <li><p><a href="http://www.inference.vc/untitled/">ML beyond Curve Fitting: An Intro to Causal Inference and do-Calculus</a></p></li>
                    </ul></li>
                    <li>deepmind wavenet</li>
                    <li><a href="https://openreview.net/pdf?id=ByldLrqlx">deepcoder: learning to write programs</a></li>
                    <li><p>Ramblings, opinions, guesses, hypotheses, conjectures, speculations</p>
                    <ul>
                    <li>AI is approximation (or constrained optimization?) in Sobolev spaces (or ( L<sup>p</sup>() ) spaces?)?</li>
                    <li>Intelligent agents are only possible if the world they live in is structured. If the laws of physics randomly change over time, then intelligent agents are unlikely.</li>
                    <li><p>We should merge machine learning, probability, and statistics?</p>
                    <ul>
                    <li><a href="http://en.wikipedia.org/wiki/Recursive_self_improvement">WP:Recursive self-improvement</a></li>
                    </ul></li>
                    <li><p>World = agent + environment. Environment is everything that the agent does not control directly. The body of an agent is part of the environment, not of the agent.</p></li>
                    </ul></li>
                    <li><a href="http://dl.acm.org/citation.cfm?id=2567715">Dimension independent similarity computation (DISCO)</a></li>
                    <li><a href="http://www.jair.org/">Journal of artificial intelligence research</a> (open access)</li>
                    <li><a href="https://arxiv.org/abs/1802.08195">Adversarial Examples that Fool both Human and Computer Vision</a>, from <a href="https://www.youtube.com/watch?v=AbxPbfODGcs">two minute papers 241</a>.</li>
                    <li><a href="https://www.semanticscholar.org/paper/Machine-Theory-of-Mind-Rabinowitz-Perbet/4a48d7528bf1f81f48be8a644ffb1bcc08f1b2c5">Machine theory of mind</a></li>
                    <li>Ilias Diakonikolas, Daniel Kane and Alistair Stewart. Optimal Learning via the Fourier Transform for Sums of Independent Integer Random Variables</li>
                    <li><a href="https://en.m.wikipedia.org/wiki/List_of_important_publications_in_computer_science#Machine_learning">https://en.m.wikipedia.org/wiki/List_of_important_publications_in_computer_science#Machine_learning</a></li>
                    <li><a href="https://arxiv.org/abs/1704.07441">Detecting English Writing Styles For Non Native Speakers</a></li>
                    <li>&quot;Hicklin envisaged that learning resulted from a dynamic equilibrium between information acquisition and loss.&quot; (<a href="https://onlinelibrary.wiley.com/doi/pdf/10.1002/tea.3660210910">Mathematical modeling of learning, Peter F. W. Preece</a>, 1984)</li>
                    <li>AI research tries to make a system that can optimize a wide variety of goal functions?</li>
                    <li><a href="https://cs.nyu.edu/~mohri/mlbook/">Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar; book; &quot;Foundations of machine learning&quot;</a></li>
                    <li><a href="http://bigthink.com/videos/the-top-3-supplements-for-surviving-the-singularity">http://bigthink.com/videos/the-top-3-supplements-for-surviving-the-singularity</a></li>
                    <li><a href="https://google.github.io/CausalImpact/CausalImpact.html">https://google.github.io/CausalImpact/CausalImpact.html</a></li>
                    <li><p>intelligence testing</p>
                    <ul>
                    <li><p><a href="https://www.youtube.com/watch?v=8YWjSQHfV5U">YT:Jordan Peterson - Example IQ questions and what Career/job fits your IQ</a></p>
                    <ul>
                    <li>problem: no job for people with IQ below 87?</li>
                    <li><a href="https://www.reddit.com/r/JordanPeterson/comments/84qmsj/source_of_83_iq_minimum_for_the_us_military/">R:source for soldier minimum IQ requirement of 85</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Fluid_and_crystallized_intelligence">WP:Fluid and crystallized intelligence</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Raven%27s_Progressive_Matrices">WP:Raven's progressive matrices</a> is a language-neutral visual test for fluid intelligence?</li>
                    </ul></li>
                    </ul></li>
                    <li><a href="https://www.youtube.com/watch?v=GdTBqBnqhaQ">YT:4 Experiments Where the AI Outsmarted Its Creators | Two Minute Papers #242</a></li>
                    <li><a href="https://arxiv.org/abs/1509.06569">Tensorizing Neural Networks</a></li>
                    <li><a href="https://arxiv.org/abs/1502.02367">Gated Feedback Recurrent Neural Networks</a></li>
                    <li>no information <a href="http://syntience.com/">http://syntience.com/</a></li>
                    <li><p><a href="https://www.youtube.com/watch?v=b_6-iVz1R0o">The pattern behind self-deception | Michael Shermer</a>: patternicity, agenticity, pattern over-recognition, false positive, false negative</p>
                    <ul>
                    <li>&quot;false positive&quot; is a much better name than &quot;type 1 error&quot;</li>
                    </ul></li>
                    <li>expected 2018, draft book, &quot;Model-based machine learning&quot;, <a href="http://www.mbmlbook.com/">html</a></li>
                    <li><p>vision (making machines see)</p>
                    <ul>
                    <li>Jim Bednar, <a href="http://homepages.inf.ed.ac.uk/jbednar/demos.html">Orientation Perception Demos</a></li>
                    </ul></li>
                    <li><a href="https://en.wikipedia.org/wiki/Bayesian_approaches_to_brain_function">https://en.wikipedia.org/wiki/Bayesian_approaches_to_brain_function</a></li>
                    <li><p><a href="https://www.youtube.com/watch?v=MvFABFWPBrw">DeepMind Has A Superhuman Level Quake 3 AI Team - YouTube</a></p>
                    <ul>
                    <li>Moby Motion's comment: &quot;Really exciting because of the sparse internal rewards and long term planning. A step towards AI agents that are useful in real life.&quot;</li>
                    </ul></li>
                    <li><p>2018 AI is like autistic savants. They perform one task exceptionally well, but they are bad at everything else.</p>
                    <ul>
                    <li>2018, <a href="https://www.youtube.com/watch?v=eSaShQbUJTQ">DeepMind's AI Takes An IQ Test - YouTube</a></li>
                    </ul></li>
                    <li><p>AI</p>
                    <ul>
                    <li>2007, article, &quot;Self-taught Learning: Transfer Learning from Unlabeled Data&quot;, <a href="https://cs.stanford.edu/people/ang/papers/icml07-selftaughtlearning.pdf">pdf</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Category:Open-source_artificial_intelligence">https://en.wikipedia.org/wiki/Category:Open-source_artificial_intelligence</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Commonsense_knowledge_(artificial_intelligence)">https://en.wikipedia.org/wiki/Commonsense_knowledge_(artificial_intelligence)</a></li>
                    <li>2010, article, <a href="https://news.mit.edu/2010/ai-unification">A grand unified theory of AI - MIT News</a></li>
                    <li>2016, article, <a href="https://ai100.stanford.edu/2016-report/section-i-what-artificial-intelligence/ai-research-trends">AI Research Trends - One Hundred Year Study on Artificial Intelligence (AI100)</a></li>
                    <li><p>sequence learning?</p>
                    <ul>
                    <li><a href="https://devblogs.nvidia.com/deep-learning-nutshell-sequence-learning/">https://devblogs.nvidia.com/deep-learning-nutshell-sequence-learning/</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Sequence_learning">https://en.wikipedia.org/wiki/Sequence_learning</a></li>
                    </ul></li>
                    <li><p>AI perception of time?</p></li>
                    </ul></li>
                    <li><p><a href="https://www.quora.com/Does-the-human-brain-have-an-internal-language">https://www.quora.com/Does-the-human-brain-have-an-internal-language</a></p>
                    <ul>
                    <li>mereological fallacy, confusing the part and the whole</li>
                    </ul></li>
                    <li><a href="https://www.quora.com/Is-the-human-brain-analog-or-digital">https://www.quora.com/Is-the-human-brain-analog-or-digital</a> <a href="https://en.wikipedia.org/wiki/Mereological_essentialism">https://en.wikipedia.org/wiki/Mereological_essentialism</a></li>
                    <li><p>machine learning</p>
                    <ul>
                    <li><a href="https://github.com/Avik-Jain/100-Days-Of-ML-Code">Avik-Jain/100-Days-Of-ML-Code: 100 Days of ML Coding</a></li>
                    </ul></li>
                    <li><p>Justifying consciousness using evolution?</p>
                    <ul>
                    <li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4122207/">The biological function of consciousness</a></li>
                    <li><a href="https://www.quora.com/How-does-sentience-benefit-survival-and-why-is-it-developed">How does sentience benefit survival and why is it developed? - Quora</a></li>
                    </ul></li>
                    <li><a href="https://www.quora.com/How-do-I-publish-artificial-intelligence-research-if-I-am-not-currently-in-academia-or-an-industry-research-setting">https://www.quora.com/How-do-I-publish-artificial-intelligence-research-if-I-am-not-currently-in-academia-or-an-industry-research-setting</a></li>
                    <li><a href="https://www.quora.com/How-does-life-fight-against-entropy">How does life fight against entropy? - Quora</a></li>
                    <li><p>Life and entropy</p>
                    <ul>
                    <li><a href="https://www.quora.com/How-does-life-fight-against-entropy">How does life fight against entropy? - Quora</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Entropy_and_life">WP:Entropy and life</a></li>
                    </ul></li>
                    <li><p>Making machine understand human languages</p>
                    <ul>
                    <li><a href="https://blogs.microsoft.com/ai/microsoft-creates-ai-can-read-document-answer-questions-well-person/">Microsoft creates AI that can read a document and answer questions about it as well as a person - The AI Blog</a></li>
                    </ul></li>
                    <li><a href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html">A (Long) Peek into Reinforcement Learning</a></li>
                    <li><p>Competitions</p>
                    <ul>
                    <li>Kaggle: get paid to solve machine learning problems.</li>
                    </ul></li>
                    <li>HLearn: a machine learning library for Haskell <span class="citation" data-cites="izbicki2013hlearn">[<a href="#ref-izbicki2013hlearn">3</a>]</span></li>
                    <li><a href="https://dzone.com/articles/deep-dive-into-machine-learning">Deep Dive Into Machine Learning - DZone AI</a></li>
                    <li><a href="https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf">https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf</a></li>
                    <li><a href="https://github.com/keras-team/keras">keras-team/keras: Deep Learning for humans</a></li>
                    <li><a href="http://cs230.stanford.edu/proj-spring-2018.html">CS230: Deep Learning - Projects</a></li>
                    <li><a href="http://jonbho.net/2014/09/25/defining-intelligence/">http://jonbho.net/2014/09/25/defining-intelligence/</a></li>
                    <li><a href="https://github.com/HuwCampbell/grenade">HuwCampbell/grenade: Deep Learning in Haskell</a></li>
                    <li><a href="http://www.randomhacks.net/2007/03/03/smart-classification-with-haskell/">Smart classification using Bayesian monads in Haskell - Random Hacks</a></li>
                    </ul>
                    <h2 id="artificial-intelligence-research"><span class="section_number">9</span><span class="section_title">Artificial intelligence research</span></h2>
                    <div class="local_table_of_contents">
                    <ul>
                    <li><span class="section_number">9.1</span><span class="section_title"><a href="#questions">Questions</a></span><span class="word_count">(9w~1m)</span></li>
                    <li><span class="section_number">9.2</span><span class="section_title"><a href="#how-can-i-become-an-ai-researcher">How can I become an AI researcher?</a></span><span class="word_count">(130w~1m)</span></li>
                    <li><span class="section_number">9.3</span><span class="section_title"><a href="#how-are-others-works-progressing">How are others' works progressing?</a></span><span class="word_count">(87w~1m)</span></li>
                    </ul>
                    </div>
                    <h3 id="questions"><span class="section_number">9.1</span><span class="section_title">Questions</span></h3>
                    <ul>
                    <li>What is the best place to do AI research?</li>
                    </ul>
                    <h3 id="how-can-i-become-an-ai-researcher"><span class="section_number">9.2</span><span class="section_title">How can I become an AI researcher?</span></h3>
                    <ul>
                    <li><p>Where are new results announced?</p>
                    <ul>
                    <li><a href="https://en.m.wikipedia.org/wiki/Portal:Artificial_intelligence">Wikipedia AI Portal</a></li>
                    <li>Reddit <a href="https://www.reddit.com/r/artificial/">/r/artificial</a></li>
                    </ul></li>
                    <li><p>Where is more information?</p>
                    <ul>
                    <li><a href="https://en.wikipedia.org/wiki/Artificial_intelligence">Wikipedia: Artificial intelligence</a></li>
                    </ul></li>
                    <li><p>Who are the researchers?</p>
                    <ul>
                    <li><p>See also <a href="https://www.quora.com/Who-is-leading-in-AI-research-among-big-players-like-IBM-Google-Facebook-Apple-and-Microsoft">Quora: Who is leading in AI research among big players like IBM, Google, Facebook, Apple, and Microsoft?</a></p>
                    <ul>
                    <li>Google Brain, OpenAI, FAIR (Facebook AI Research), Microsoft Research, IBM Research</li>
                    </ul></li>
                    <li><p>Geoffrey Hinton, <a href="http://www.cs.toronto.edu/~hinton/">UToronto page</a>, <a href="https://www.reddit.com/r/MachineLearning/comments/2lmo0l/ama_geoffrey_hinton/">Reddit AMA</a>, <a href="https://www.semanticscholar.org/author/Geoffrey-E.-Hinton/1695689">Semantic Scholar influence graph</a></p>
                    <ul>
                    <li>He is trying to find out how the brain works.</li>
                    <li>The idea: If a learning algorithm works on machines, then it might have something to do with how brains work.</li>
                    <li>More interested in physical explanation of how the brain works. Physics first, math second, although his math is OK.</li>
                    </ul></li>
                    <li>Yann LeCun</li>
                    <li>Jürgen Schmidhuber</li>
                    <li>Pedro Domingos</li>
                    <li><p>Demis Hassabis</p>
                    <ul>
                    <li>What is his focus?</li>
                    </ul></li>
                    <li><p>Pamela McCorduck, AI historian</p>
                    <ul>
                    <li>2004 anniversary edition of her 1979 book <a href="http://www.pamelamc.com/html/machines_who_think.html">&quot;Machines who think&quot;</a></li>
                    </ul></li>
                    <li><p>Who else? There are lots of people.</p></li>
                    </ul></li>
                    </ul>
                    <h3 id="how-are-others-works-progressing"><span class="section_number">9.3</span><span class="section_title">How are others' works progressing?</span></h3>
                    <ul>
                    <li><p>How is <a href="https://homes.cs.washington.edu/~pedrod/">Pedro Domingos</a>'s progress of finding the master algorithm unifying the five tribes?</p>
                    <ul>
                    <li><p>Markov logic network unifies probabilists and logicians.</p>
                    <ul>
                    <li>How about the other three tribes?</li>
                    </ul></li>
                    <li><p>Hume's question: How do we justify generalization? Why does generalization work?</p>
                    <ul>
                    <li><p>Does Wolpert answer that in &quot;no free lunch theorem&quot;?</p>
                    <ul>
                    <li><a href="https://en.wikipedia.org/wiki/No_free_lunch_theorem">Wikipedia: No free lunch theorem</a></li>
                    </ul></li>
                    <li><p>I think induction works because our Universe happens to have a structure that is amenable to induction.</p>
                    <ul>
                    <li><p>If induction doesn't work, and evolution is true, then we would have gone extinct long ago, wouldn't we?</p>
                    <ul>
                    <li>What structure is that?</li>
                    </ul></li>
                    </ul></li>
                    </ul></li>
                    </ul></li>
                    </ul>
                    <h2 id="approximation-theory"><span class="section_number">10</span><span class="section_title">Approximation theory</span></h2>
                    <p>We are interested in approximation theory because we want to justify how neural networks work.</p>
                    <ul>
                    <li>2016, article, &quot;Deep vs. shallow networks: An approximation theory perspective&quot;, <a href="https://arxiv.org/abs/1608.03287">pdf available</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Explainable_Artificial_Intelligence">WP:Explainable Artificial Intelligence</a></li>
                    </ul>
                    <p>We should begin by skimming the 1998 book &quot;A Short Course on Approximation Theory&quot; by N. L. Carothers (<a href="http://fourier.math.uoc.gr/~mk/approx1011/carothers.pdf">pdf</a>). Then we should skim the 2017 lecture notes &quot;Lectures on multivariate polynomial approximation&quot; (<a href="http://www.math.unipd.it/~demarchi/MultInterp/LectureNotesMI.pdf">pdf</a>).</p>
                    <p>The phrase &quot;x <em>approximates</em> y&quot; means &quot;x is <em>close</em> to y&quot;, which implies distance, which implies metric space.</p>
                    <p>How close is the approximation? Suppose that the function <span class="math inline">\(g\)</span> approximates the function <span class="math inline">\(f\)</span> in interval <span class="math inline">\(I\)</span>. Then:</p>
                    <ul>
                    <li>The &quot;approximation error at <span class="math inline">\(x\)</span>&quot; is <span class="math inline">\(g(x) - f(x)\)</span>.</li>
                    <li>The &quot;maximum absolute error&quot; is <span class="math inline">\(\max_{x \in I} \abs{g(x) - f(x)}\)</span>.</li>
                    </ul>
                    <p>How do we measure the distance between two <span class="math inline">\(\Real \to \Real\)</span> functions <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span>? There are several ways. Which should we use?</p>
                    <ul>
                    <li>The maximum norm, in interval <span class="math inline">\(I\)</span> is <span class="math inline">\(\max_{x \in I} \abs{f(x) - g(x)}\)</span>. This norm is also called uniform norm, supremum norm, Chebyshev norm, infinity norm, norm-infinity, <span class="math inline">\(L_\infty\)</span>-norm. Why is it called &quot;uniform&quot;? <a href="https://en.wikipedia.org/wiki/Uniform_norm">WP:Uniform norm</a>.</li>
                    <li>What is this norm called? <span class="math inline">\(\int_{x \in I} [f(x)-g(x)]^2 ~ dx\)</span>.</li>
                    </ul>
                    <div class="local_table_of_contents">
                    <ul>
                    <li><span class="section_number">10.1</span><span class="section_title"><a href="#other">Other</a></span><span class="word_count">(664w~4m)</span></li>
                    </ul>
                    </div>
                    <h3 id="other"><span class="section_number">10.1</span><span class="section_title">Other</span></h3>
                    <ul>
                    <li>Courses
                    <ul>
                    <li>2017, <a href="https://www.nada.kth.se/~olofr/Approx/">Approximation Theory, 7.5 ECTS</a></li>
                    <li>2012, syllabus, Drexel University, Math 680-002 (Approximation Theory), <a href="http://www.math.drexel.edu/~foucart/TeachingFiles/S12/Math680Syl.pdf">pdf</a></li>
                    <li>2002, <a href="http://math.ucdenver.edu/~aknyazev/teaching/02/5667/">MATH 5667-001: Introduction to Approximation Theory, CU-Denver, Fall 02</a>.</li>
                    </ul></li>
                    <li>Subfields of approximation theory
                    <ul>
                    <li>Classical approximation theory deals with univariate real functions <span class="math inline">\(\Real \to \Real\)</span>.</li>
                    <li>Multivariate approximation theory deals with multivariate real functions <span class="math inline">\(\Real^m \to \Real^n\)</span>.</li>
                    </ul></li>
                    <li>Scenarios
                    <ul>
                    <li>Suppose we want to approximate the function <span class="math inline">\(f\)</span>, but we don't know the equation for <span class="math inline">\(f\)</span>; we only have a few input-output samples.
                    <ul>
                    <li>Can we approximate <span class="math inline">\(f\)</span>?</li>
                    <li>How do approximation and curve-fitting relate?</li>
                    </ul></li>
                    </ul></li>
                    <li>Overview
                    <ul>
                    <li>What is a multivariate polynomial?</li>
                    <li>Commonly conflated concepts
                    <ul>
                    <li>Approximation is not estimation.
                    <ul>
                    <li>Approximation converges. Estimation doesn't, because the actual value is unknown.</li>
                    <li>Approximation doesn't guess. Estimation does.</li>
                    <li>Approximation has error. Estimation has uncertainty.</li>
                    <li>Approximation is part of analysis. Estimation is part of statistics.</li>
                    </ul></li>
                    </ul></li>
                    </ul></li>
                    <li>The <em>uniform norm</em> is …</li>
                    <li>Best approximation is …</li>
                    <li>Uniform approximation is best approximation in uniform norm.</li>
                    <li><a href="https://en.wikipedia.org/wiki/Approximation_theory#Remez&#39;s_algorithm">https://en.wikipedia.org/wiki/Approximation_theory#Remez's_algorithm</a>
                    <ul>
                    <li><a href="https://en.wikipedia.org/wiki/Remez_algorithm">https://en.wikipedia.org/wiki/Remez_algorithm</a>
                    <ul>
                    <li>Inputs: a function, and an interval.</li>
                    <li>Output: an optimal polynomial approximating the input function in the input interval.</li>
                    </ul></li>
                    </ul></li>
                    <li>What are Bernstein polynomials? What question does the Weierstrass approximation theorem answer?
                    <ul>
                    <li><a href="http://www4.ncsu.edu/~mtchu/Teaching/Lectures/MA530/chapter7.pdf">http://www4.ncsu.edu/~mtchu/Teaching/Lectures/MA530/chapter7.pdf</a></li>
                    </ul></li>
                    <li><a href="https://en.wikipedia.org/wiki/Chebyshev_polynomials">WP:Chebyshev polynomials</a>
                    <ul>
                    <li>Why is it important? How does it relate to best approximation?
                    <ul>
                    <li>&quot;Chebyshev polynomials are important in approximation theory because the roots of the Chebyshev polynomials of the first kind, which are also called Chebyshev nodes, are used as nodes in polynomial interpolation. The resulting interpolation polynomial minimizes the problem of Runge's phenomenon and provides an approximation that is close to the polynomial of best approximation to a continuous function under the maximum norm.&quot;</li>
                    </ul></li>
                    </ul></li>
                    <li>Machine learning as relation approximation
                    <ul>
                    <li>Machine learning, statistical modelling, function approximation, and curve fitting are related.</li>
                    <li>Generalize function approximation to relation approximation.</li>
                    <li>A function can be stated as a relation.</li>
                    <li>A relation can be stated as a function.</li>
                    </ul></li>
                    <li>Consider the least-square solution to an overdetermined system of linear equations. Is such solution a kind of approximation?
                    <ul>
                    <li>There is no exact solution to begin with?</li>
                    <li>Why is it called &quot;least-squares <em>approximation</em>&quot;?</li>
                    <li>How can you approximate something that does not exist?
                    <ul>
                    <li>1.2 approximates 1.23. Both 1.2 and 1.23 exist.</li>
                    <li>Contrarily, there is no X such that AX = B.</li>
                    </ul></li>
                    </ul></li>
                    <li>What are approximation schemes?
                    <ul>
                    <li><a href="https://en.wikipedia.org/wiki/Polynomial-time_approximation_scheme">https://en.wikipedia.org/wiki/Polynomial-time_approximation_scheme</a></li>
                    </ul></li>
                    <li>How do we approximate a function? Is it even possible to approximate arbitrary functions?
                    <ul>
                    <li>If the function is analytic, we can truncate its Taylor series.
                    <ul>
                    <li>Commonly-used differentiable functions are analytic.</li>
                    </ul></li>
                    <li>Chebyshev polynomials?</li>
                    <li>If we have an approximation scheme, we may be able to improve it.
                    <ul>
                    <li><a href="https://en.wikipedia.org/wiki/Series_acceleration">https://en.wikipedia.org/wiki/Series_acceleration</a>
                    <ul>
                    <li><a href="https://en.wikipedia.org/wiki/Aitken%27s_delta-squared_process">https://en.wikipedia.org/wiki/Aitken%27s_delta-squared_process</a></li>
                    </ul></li>
                    </ul></li>
                    <li>google search: machine learning approximation theory
                    <ul>
                    <li><a href="https://math.stackexchange.com/questions/2680158/approximation-theory-for-deep-learning-models-where-to-start">Approximation Theory for Deep Learning Models: Where to Start? - Mathematics Stack Exchange</a></li>
                    <li><a href="http://www.vision.jhu.edu/tutorials/ICCV15-Tutorial-Math-Deep-Learning-Intro-Rene-Joan.pdf">http://www.vision.jhu.edu/tutorials/ICCV15-Tutorial-Math-Deep-Learning-Intro-Rene-Joan.pdf</a></li>
                    <li>2017, slides, &quot;From approximation theory to machine learning: New perspectives in the theory of function spaces and their applications&quot;, <a href="http://npfsa2017.uni-jena.de/l_notes/vybiral.pdf">pdf</a></li>
                    <li>2018, article, &quot;Approximation theory, Numerical Analysis and Deep Learning&quot;, <a href="http://at.yorku.ca/c/b/p/g/30.htm">abstract</a>
                    <ul>
                    <li>&quot;the problem of numerically solving a large class of (high-dimensional) PDEs (such as linear Black-Scholes or diffusion equations) can be cast into a classical supervised learning problem which can then be solved by deep learning methods&quot;</li>
                    </ul></li>
                    </ul></li>
                    </ul></li>
                    <li>Determine whether we need to read these
                    <ul>
                    <li>Very likely
                    <ul>
                    <li>2015, slides, &quot;Best polynomial approximation: multidimensional case&quot;, <a href="https://carma.newcastle.edu.au/meetings/spcom/talks/Sukhorukova-SPCOM_2015.pdf">pdf</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Bernstein_polynomial#Approximating_continuous_functions">https://en.wikipedia.org/wiki/Bernstein_polynomial#Approximating_continuous_functions</a>
                    <ul>
                    <li><a href="https://en.wikipedia.org/wiki/Pointwise_convergence">https://en.wikipedia.org/wiki/Pointwise_convergence</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Uniform_convergence">https://en.wikipedia.org/wiki/Uniform_convergence</a></li>
                    </ul></li>
                    <li><a href="https://en.wikipedia.org/wiki/Approximation">https://en.wikipedia.org/wiki/Approximation</a>
                    <ul>
                    <li><a href="https://en.wikipedia.org/wiki/Approximation_theory">https://en.wikipedia.org/wiki/Approximation_theory</a>
                    <ul>
                    <li>is a branch of <a href="https://en.wikipedia.org/wiki/Functional_analysis">https://en.wikipedia.org/wiki/Functional_analysis</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Approximation_theory#Chebyshev_approximation">https://en.wikipedia.org/wiki/Approximation_theory#Chebyshev_approximation</a></li>
                    </ul></li>
                    <li><a href="https://en.wikipedia.org/wiki/Approximate_computing">https://en.wikipedia.org/wiki/Approximate_computing</a>
                    <ul>
                    <li>example: <a href="https://en.wikipedia.org/wiki/Artificial_neural_network">https://en.wikipedia.org/wiki/Artificial_neural_network</a></li>
                    </ul></li>
                    </ul></li>
                    <li><a href="https://en.wikipedia.org/wiki/Telescoping_series">https://en.wikipedia.org/wiki/Telescoping_series</a></li>
                    </ul></li>
                    <li>Likely
                    <ul>
                    <li>2018, slides, &quot;Deep Learning: Approximation of Functions by Composition&quot;, <a href="http://helper.ipam.ucla.edu/publications/dlt2018/dlt2018_14936.pdf">pdf</a>
                    <ul>
                    <li>classical approximation vs deep learning</li>
                    </ul></li>
                    <li>2013, short survey article draft, &quot;Multivariate approximation&quot;, <a href="http://num.math.uni-goettingen.de/schaback/research/papers/MultApp_01.pdf">pdf</a></li>
                    <li>1995, short introduction, &quot;Multivariate Interpolation and Approximation by Translates of a Basis Function&quot;, <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.45.2194&amp;rep=rep1&amp;type=pdf">pdf</a></li>
                    <li>1989, article, &quot;A Theory of Networks for Approximation and Learning&quot;, <a href="http://www.dtic.mil/docs/citations/ADA212359">pdf available</a>
                    <ul>
                    <li>What is the summary, especially about learning and approximation theory?</li>
                    </ul></li>
                    </ul></li>
                    <li>Unlikely
                    <ul>
                    <li>Survey-like
                    <ul>
                    <li>2006, chapter, &quot;Topics in multivariate approximation theory&quot;, <a href="https://www.researchgate.net/publication/226303661_Topics_in_multivariate_approximation_theory">pdf available</a></li>
                    <li>1982, article, &quot;Topics in multivariate approximation theory&quot;, <a href="http://www.dtic.mil/dtic/tr/fulltext/u2/a116248.pdf">pdf</a></li>
                    <li>1986, &quot;Multivariate Approximation Theory: Selected Topics&quot;, <a href="https://epubs.siam.org/doi/book/10.1137/1.9781611970197">paywall</a></li>
                    </ul></li>
                    <li>Theorem
                    <ul>
                    <li>2017, article, &quot;Multivariate polynomial approximation in the hypercube&quot;, <a href="https://people.maths.ox.ac.uk/trefethen/hypercube_published.pdf">pdf</a></li>
                    </ul></li>
                    <li>2017, article, &quot;Selected open problems in polynomial approximation and potential theory&quot;, <a href="http://drna.padovauniversitypress.it/system/files/papers/BaranCiezEgginkKowalskaNagyPierzcha%C5%82a_DRNA2017.pdf">pdf</a></li>
                    <li>2017, article, &quot;High order approximation theory for Banach space valued functions&quot;, <a href="https://ictp.acad.ro/jnaat/journal/article/view/1112">pdf available</a></li>
                    <li>Articles summarizing people's works
                    <ul>
                    <li>2017, article, &quot;Michael J.D. Powell's work in approximation theory and optimisation&quot;, <a href="https://www.sciencedirect.com/science/article/abs/pii/S0021904517301053">paywall</a></li>
                    <li>2000, article, &quot;Weierstrass and Approximation Theory&quot;, <a href="https://www.sciencedirect.com/science/article/pii/S0021904500935081">paywall</a></li>
                    </ul></li>
                    <li>2013, article, &quot;[1312.5540] Emerging problems in approximation theory for the numerical solution of nonlinear PDEs of integrable type&quot;, <a href="https://arxiv.org/abs/1312.5540">pdf available</a></li>
                    <li>1985, article, &quot;Some problems in approximation theory and numerical analysis - IOPscience&quot;, <a href="http://iopscience.iop.org/article/10.1070/RM1985v040n01ABEH003526">pdf available</a></li>
                    <li>2011, article, &quot;Experiments on Probabilistic Approximations&quot;, <a href="https://people.eecs.ku.edu/~jerzygb/c154-clark.pdf">pdf</a></li>
                    </ul></li>
                    </ul></li>
                    <li>Less relevant overview
                    <ul>
                    <li>Why do we approximate?
                    <ul>
                    <li>Because it is practically inevitable.
                    <ul>
                    <li>Fundamental reason: Because computers are finite.</li>
                    <li>Practical reason: Trade-off between computation time and precision.
                    <ul>
                    <li>The more error we can afford, the faster we can run.
                    <ul>
                    <li>May be related: 2013 monograph &quot;Faster Algorithms via Approximation Theory&quot; <a href="http://theory.epfl.ch/vishnoi/Publications_files/approx-survey.pdf">pdf</a></li>
                    </ul></li>
                    </ul></li>
                    </ul></li>
                    </ul></li>
                    <li>2018 book &quot;Recent Advances in Constructive Approximation Theory&quot; <a href="https://www.springer.com/us/book/9783319921648">paywall</a></li>
                    </ul></li>
                    </ul>
                    <h2 id="approximation-by-truncation"><span class="section_number">11</span><span class="section_title">Approximation by truncation</span></h2>
                    <p>We can approximate a series by <em>truncating</em> it.</p>
                    <p>Suppose that the series <span class="math inline">\(y = x_0 + x_1 + \ldots\)</span> converges.</p>
                    <p>Suppose that the sequence <span class="math inline">\(\langle x_0, x_1, \ldots \rangle\)</span> converges to zero.</p>
                    <p>Pick where to cut. Pick a natural number <span class="math inline">\(n\)</span>.</p>
                    <p>Then the series <span class="math inline">\(x_0 + \ldots + x_n\)</span> approximates the series <span class="math inline">\(y\)</span>. We cut its tail. We take finitely many summands from the beginning.</p>
                    <p>Here come examples: Truncate all the series!</p>
                    <div class="local_table_of_contents">
                    <ul>
                    <li><span class="section_number">11.1</span><span class="section_title"><a href="#power-series-truncation">Power series truncation</a></span><span class="word_count">(258w~2m)</span></li>
                    <li><span class="section_number">11.2</span><span class="section_title"><a href="#iteration-truncation">Iteration truncation</a></span><span class="word_count">(62w~1m)</span></li>
                    </ul>
                    </div>
                    <h3 id="power-series-truncation"><span class="section_number">11.1</span><span class="section_title">Power series truncation</span></h3>
                    <p>Below we truncate a power series.</p>
                    <p>Decimal truncation: <span class="math inline">\(1.2\)</span> approximates <span class="math inline">\(1.23\)</span>. Remember that a decimal number is a series. For example, the number <span class="math inline">\(1.23\)</span> is the power series <span class="math display">\[ \ldots 01.230 \ldots = \ldots + 0 \cdot 10^1 + 1 \cdot 10^0 + 2 \cdot 10^{-1} + 3 \cdot 10^{-2} + 0 \cdot 10^{-3} + \ldots. \]</span></p>
                    <p>Polynomial truncation: <span class="math inline">\(1 + x\)</span> approximates <span class="math inline">\(1 + x + x^2\)</span> for <span class="math inline">\(x\)</span> near zero.</p>
                    <p>Taylor series truncation: <span class="math inline">\(1 + x + \frac{x^2}{2}\)</span> approximates <span class="math inline">\(e^x\)</span> for <span class="math inline">\(x\)</span> near zero. Remember the Taylor series expansion <span class="math inline">\(e^x = \sum_{n \in \Nat} \frac{x^n}{n!}\)</span>.</p>
                    <p>Below we truncate the ratio of two power series.</p>
                    <p>Rational truncation: <span class="math inline">\(12/23\)</span> approximates <span class="math inline">\(123/234\)</span>.</p>
                    <p><a href="https://en.wikipedia.org/wiki/Pad%C3%A9_approximant">WP:Padé approximation</a> is a truncation of a ratio of series.</p>
                    <p>Fourier series truncation: The <a href="https://en.wikipedia.org/wiki/Fourier_series#Example_1:_a_simple_Fourier_series">Wikipedia example</a> animates how a Fourier series converges to the sawtooth function as more terms are added.</p>
                    <p>Digression: Is a (complex) Fourier series a power series? Reminder: A Fourier series looks like <span class="math inline">\(\sum_{k=0}^{\infty} c_k e^{ikt}\)</span>.</p>
                    <p><a href="https://en.wikipedia.org/wiki/Laurent_series">WP:Laurent series</a> truncation?</p>
                    <div class="local_table_of_contents">
                    <ul>
                    <li><span class="section_number">11.1.1</span><span class="section_title"><a href="#digression-what-is-an-analytic-function">Digression: What is an analytic function?</a></span><span class="word_count">(56w~1m)</span></li>
                    <li><span class="section_number">11.1.2</span><span class="section_title"><a href="#digression-what-is-the-relationship-between-polynomial-and-power-series">Digression: What is the relationship between polynomial and power series?</a></span><span class="word_count">(49w~1m)</span></li>
                    </ul>
                    </div>
                    <h4 id="digression-what-is-an-analytic-function"><span class="section_number">11.1.1</span><span class="section_title">Digression: What is an analytic function?</span></h4>
                    <p>A function is <em>analytic</em> iff it can be represented by power series.</p>
                    <p>Formally, a function <span class="math inline">\(f\)</span> is <em>analytic</em> iff for every <span class="math inline">\(x \in \dom(f)\)</span>, we can write <span class="math inline">\(f(x)\)</span> as a power series.</p>
                    <p>See also <a href="https://en.wikipedia.org/wiki/Power_series#Analytic_functions">WP:Definition of &quot;analytic function&quot;</a>.</p>
                    <p>Taylor series expansion is illustrated in the 2015 slides &quot;Taylor Series: Expansions, Approximations and Error&quot; (<a href="https://relate.cs.illinois.edu/course/cs357-f15/file-version/2978ddd5db9824a374db221c47a33f437f2df1da/media/cs357-slides6.pdf">pdf</a>)</p>
                    <h4 id="digression-what-is-the-relationship-between-polynomial-and-power-series"><span class="section_number">11.1.2</span><span class="section_title">Digression: What is the relationship between polynomial and power series?</span></h4>
                    <p>A polynomial is an algebraic expression. It is not a function.</p>
                    <p>Power series is a kind of infinite polynomial.</p>
                    <p><a href="https://en.wikipedia.org/wiki/Formal_power_series">WP:Formal power series</a>: &quot;A formal power series is a generalization of a polynomial, where the number of terms is allowed to be infinite.&quot;</p>
                    <h3 id="iteration-truncation"><span class="section_number">11.2</span><span class="section_title">Iteration truncation</span></h3>
                    <ul>
                    <li><a href="https://en.wikipedia.org/wiki/Iterated_function">WP:Iterated function</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Iterative_method">WP:Iterative method</a></li>
                    <li><a href="http://mathworld.wolfram.com/NewtonsIteration.html">Newton's Iteration</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Methods_of_computing_square_roots#Babylonian_method">WP:Methods of computing square roots, the Babylonian method</a></li>
                    <li>An iteration converges to an attractive fixed point.</li>
                    </ul>
                    <p>Example: Let <span class="math inline">\(f(x) = x + \frac{1}{x}\)</span>.</p>
                    <p>Continued fraction truncation: We know that <span class="math display">\[ 1 + \frac{1}{1 + \frac{1}{1 + \ldots}} = \frac{1 + \sqrt{5}}{2} = \Phi. \]</span> We can truncate that continued fraction to approximate <span class="math inline">\(\Phi\)</span>.</p>
                    <p>Seeing those examples makes me wonder whether all approximations are truncation.</p>
                    <h2 id="automatic-differentiation"><span class="section_number">12</span><span class="section_title">Automatic differentiation?</span></h2>
                    <p>Justin Le, <a href="https://blog.jle.im/entry/purely-functional-typed-models-1.html">A Purely Functional Typed Approach to Trainable Models</a></p>
                    <h2 id="about-defining-consciousness"><span class="section_number">13</span><span class="section_title">About defining consciousness</span></h2>
                    <p>2009, &quot;How to define consciousness—and how not to define consciousness&quot;, <a href="http://cogprints.org/6453/1/How_to_define_consciousness.pdf">pdf</a></p>
                    <h2 id="book-interpretable-machine-learning"><span class="section_number">14</span><span class="section_title">&lt;2018-09-28&gt; Book: &quot;interpretable machine learning&quot;</span></h2>
                    <p><a href="https://christophm.github.io/interpretable-ml-book/">https://christophm.github.io/interpretable-ml-book/</a></p>
                    <h2 id="approximation-theory-and-machine-learning"><span class="section_number">15</span><span class="section_title">Approximation theory and machine learning</span></h2>
                    <p>Conference: &quot;Approximation Theory and Machine Learning&quot;, at Purdue University, September 29 - 30, 2018</p>
                    <ul>
                    <li><a href="http://www.math.purdue.edu/calendar/conferences/machinelearning/">http://www.math.purdue.edu/calendar/conferences/machinelearning/</a></li>
                    <li><a href="http://www.math.purdue.edu/calendar/conferences/machinelearning/abstracts.php">http://www.math.purdue.edu/calendar/conferences/machinelearning/abstracts.php</a></li>
                    </ul>
                    <h2 id="analogizers-recommender-systems-matrices"><span class="section_number">16</span><span class="section_title">Analogizers, recommender systems, matrices</span></h2>
                    <ul>
                    <li><a href="https://medium.com/s/story/spotifys-discover-weekly-how-machine-learning-finds-your-new-music-19a41ab76efe">https://medium.com/s/story/spotifys-discover-weekly-how-machine-learning-finds-your-new-music-19a41ab76efe</a></li>
                    </ul>
                    <h2 id="bibliography" class="unnumbered"><span class="section_number">17</span><span class="section_title">Bibliography</span></h2>
                    <div id="refs" class="references">
                    <div id="ref-GacsVitanyiSolomonoff">
                    <p>[1] Gács, P. and Vitányi, P.M.B. 2011. Raymond J. Solomonoff 1926–2009. <em>IEEE Information Theory Society Newsletter</em>. 61, 1 (2011), 11–16.</p>
                    </div>
                    <div id="ref-hutter2004universal">
                    <p>[2] Hutter, M. 2004. <em>Universal artificial intelligence: Sequential decisions based on algorithmic probability</em>. Springer Science &amp; Business Media.</p>
                    </div>
                    <div id="ref-izbicki2013hlearn">
                    <p>[3] Izbicki, M. 2013. HLearn: a machine learning library for Haskell. <em>Proceedings of the fourteenth symposium on trends in functional programming, brigham young university, utah</em> (2013).</p>
                    </div>
                    <div id="ref-Legg2007Collection">
                    <p>[4] Legg, S. and Hutter, M. 2007. A collection of definitions of intelligence. <em>Frontiers in Artificial Intelligence and applications</em>. 157, (2007), 17.</p>
                    </div>
                    <div id="ref-DefineMachIntel">
                    <p>[5] Legg, S. and Hutter, M. 2007. Universal intelligence: A definition of machine intelligence. (2007).</p>
                    </div>
                    <div id="ref-negnevitsky2005artificial">
                    <p>[6] Negnevitsky, M. 2005. <em>Artificial intelligence: A guide to intelligent systems</em>. Pearson Education.</p>
                    </div>
                    <div id="ref-WdsIntelSlide">
                    <p>[7] Smith, W.D. 2006. Mathematical definition of “intelligence”, after mathematical definition of “intelligence” (and consequences). (2006).</p>
                    </div>
                    <div id="ref-WdsIntel">
                    <p>[8] Smith, W.D. 2006. Mathematical definition of “intelligence” (and consequences). (2006).</p>
                    </div>
                    <div id="ref-SolAlpProb2011">
                    <p>[9] Solomonoff, R.J. 2011. Algorithmic probability – its discovery – its properties and application to strong AI. <em>Randomness through computation: Some answers, more questions</em>. H. Zenil, ed. World Scientific Publishing Company. 1–23.</p>
                    </div>
                    </div>
                </div>
            </div>
        </main>
                        <div id="disqus_thread"></div>
<script>
/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
var disqus_config = function () {
    this.page.url = "https://edom.github.io/intelligence.html";  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = "https://edom.github.io/intelligence.html"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://edom-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
                        <footer class="site-footer h-card">
            <data class="u-url" href="/"></data>
            <div class="wrapper">
                <p>This page was created on 2017-06-22 03:57:00 +0700.</p>
                <p class="rss-subscribe">There is an
                    <a href="/feed.xml">RSS feed</a>, but it's unused because this site is a wiki, not a blog.</p>
                <p>Stop writing books, papers, and blogs!
                    Write a personal wiki instead!
                    Or, even better, contribute to a community wiki.
                </p>
            </div>
        </footer>
    </body>
</html>
