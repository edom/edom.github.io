<!DOCTYPE html>
<html lang="">
    <head>
        <meta charset="UTF-8"/>
        <title>Making intelligence</title>
        <link rel="stylesheet" href="/assets/main.css"/>
        <script>
// Help the reader estimate how much time the reading is going to take.
// Show word count and reading time estimation in TOC entry.
//
// TOC = table of contents
//
// Known issue: This janks: this DOM manipulation is done after the page is rendered.
// If we don't want jank, we have to manipulate the HTML source before it reaches the browser.
// We assume that the user doesn't refresh the page while reading.
// The benefit of fixing that jank is not enough for me to justify trying to fix it.
document.addEventListener("DOMContentLoaded", function () {
    function count_word (string) {
        return string.trim().split(/\s+/).length;
    }
    function show_quantity (count, singular) {
        let plural = singular + "s"; // For this script only.
        return count + " " + ((count == 1) ? singular : plural);
    }
    function create_length_indicator (word, minute) {
        let e = document.createElement("span");
        e.className = "toc_entry__length_indicator";
        e.textContent = " (" + show_quantity(word, "word") + " ~ " + show_quantity(minute, "minute") + ")";
        return e;
    }
    // We assume that readers read this many words per minute with 100% comprehension.
    // This assumption may not hold for dense texts such as philosophy and mathematics.
    const wpm_assumption = 200;
    // We assume a certain Jekyll template.
    let page = document.querySelector("main.page-content");
    if (page === null) {
        console.log("toc_generate_estimate: Impossible: CSS selector \"main.page-content\" does not match anything");
        return;
    }
    let page_title = document.querySelector("header.post-header h1.post-title");
    if (page_title === null) {
        console.log("toc_generate_estimate: Impossible: CSS selector \"header.post-header h1.post-title\" does not match anything");
        return;
    }
    let page_word = count_word(page.textContent);
    let page_minute = Math.ceil(page_word / wpm_assumption);
    page_title.insertAdjacentElement("afterend", create_length_indicator(page_word, page_minute));
    // We violate the HTML specification.
    // The page may have several elements with the same ID.
    // We assume that Org HTML Export generates a DIV element with ID "table-of-contents".
    // We assume that Jekyll Markdown-to-HTML generates a UL element with ID "markdown-toc".
    // This only works for Org HTML Export's TOC.
    let toc_entries = document.querySelectorAll("#table-of-contents a, #text-table-of-contents a");
    toc_entries.forEach((toc_entry_a) => {
        let href = toc_entry_a.getAttribute("href"); // We assume that this is a string like "#org0123456".
        if (href.charAt(0) !== '#') {
            console.log("toc_generate_estimate: Impossible: " + href + " does not begin with hash sign");
            return;
        }
        // We can't just document.querySelector(href) because target_id may contain invalid ID characters such as periods.
        let target_id = href.substring(1);
        let id_escaped = target_id.replace("\"", "\\\"");
        let h_elem = document.querySelector("[id=\"" + id_escaped + "\"]"); // We assume that this is the h1/h2/h3 element referred by the TOC entry.
        if (h_elem === null) { // We assume that this is impossible.
            console.log("toc_generate_estimate: Impossible: " + href + " does not refer to anything");
            return;
        }
        let section = h_elem.parentNode;
        let section_word = count_word(section.textContent);
        let section_minute = Math.ceil(section_word / wpm_assumption);
        toc_entry_a.insertAdjacentElement("afterend", create_length_indicator(section_word, section_minute));
    });
});
        </script>

        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-12628443-6"></script>
<script>
  window['ga-disable-UA-12628443-6'] = window.doNotTrack === "1" || navigator.doNotTrack === "1" || navigator.doNotTrack === "yes" || navigator.msDoNotTrack === "1";
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-12628443-6');
</script>
        
        <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            jax: ["input/TeX","input/MathML","input/AsciiMath",
            "output/CommonHTML"
            ],
            extensions: ["tex2jax.js","mml2jax.js","asciimath2jax.js","MathMenu.js","MathZoom.js","AssistiveMML.js", "a11y/accessibility-menu.js"],
            TeX: {
                extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
                , equationNumbers: {
                    autoNumber: "AMS"
                }
            },
            "CommonHTML": {
                scale: 100
            },
            "fast-preview": {
                disabled: true,
            }
        });
        </script>
        <style>
            /*
            PreviewHTML produces small Times New Roman text.
            PreviewHTML scale doesn't work.
            */
            .MathJax_PHTML { font-size: 110%; }
        </style>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js" async defer></script>
    </head>
    <body>
        <header class="site-header" role="banner">
            <div class="wrapper">
                <a class="site-title" rel="author" href="/">Erik Dominikus's wiki</a>
            </div>
        </header>
    <div style="display:none;">\(
    \renewcommand\emptyset{\varnothing}
    \newcommand\abs[1]{\left|#1\right|}
    \newcommand\dom{\textrm{dom}}
    \newcommand\cod{\textrm{cod}}
    \newcommand\Bernoulli{\textrm{Bernoulli}}
    \newcommand\Binomial{\textrm{Binomial}}
    \newcommand\Expect[1]{\mathbb{E}[#1]}
    \newcommand\Nat{\mathbb{N}}
    \newcommand\Integers{\mathbb{Z}}
    \newcommand\Real{\mathbb{R}}
    \newcommand\Rational{\mathbb{Q}}
    \newcommand\Complex{\mathbb{C}}
    \newcommand\Pr{\mathrm{P}}
    \newcommand\Time{\text{Time}}
    \newcommand\DTime{\text{DTime}}
    \newcommand\NTime{\text{NTime}}
    \newcommand\TimeP{\text{P}}
    \newcommand\TimeNP{\text{NP}}
    \newcommand\TimeExp{\text{ExpTime}}
    \newcommand\norm[1]{\left\lVert#1\right\rVert}
    \newcommand\bbA{\mathbb{A}}
    \newcommand\bbC{\mathbb{C}}
    \newcommand\bbD{\mathbb{D}}
    \newcommand\bbE{\mathbb{E}}
    \newcommand\bbN{\mathbb{N}}
    \newcommand\frakI{\mathfrak{I}}
    % deprecated; use TimeExp
    \newcommand\ExpTime{\text{ExpTime}}
    \newcommand\Compute{\text{Compute}}
    \newcommand\Search{\text{Search}}
    % model theory structure
    \newcommand\struc[1]{\mathcal{#1}}
    \)</div>
        <main class="page-content" aria-label="Content">
            <div class="wrapper">
                <article class="post">
                    <header class="post-header">
                        <h1 class="post-title">Making intelligence</h1>
                    </header>
                </article>
                <div class="post-content">
                    <div class="local_table_of_contents">
                    <ul>
                    <li><span class="section_number">1</span><span class="section_title"><a href="#using-this-document">Using this document</a></span><span class="word_count">(78w~1m)</span></li>
                    <li><span class="section_number">2</span><span class="section_title"><a href="#some-analytic-philosophy">Some analytic philosophy</a></span><span class="word_count">(708w~4m)</span></li>
                    <li><span class="section_number">3</span><span class="section_title"><a href="#mathematics-of-intelligence-activities">Mathematics of intelligence activities</a></span><span class="word_count">(1193w~6m)</span></li>
                    <li><span class="section_number">4</span><span class="section_title"><a href="#building-an-intelligent-system">Building an intelligent system</a></span><span class="word_count">(1098w~6m)</span></li>
                    <li><span class="section_number">5</span><span class="section_title"><a href="#what-else-is-intelligence">What else is intelligence???</a></span><span class="word_count">(942w~5m)</span></li>
                    <li><span class="section_number">6</span><span class="section_title"><a href="#natural-language-processing">Natural language processing?</a></span><span class="word_count">(130w~1m)</span></li>
                    <li><span class="section_number">7</span><span class="section_title"><a href="#philosophical-dead-ends">Philosophical dead-ends?</a></span><span class="word_count">(956w~5m)</span></li>
                    <li><span class="section_number">8</span><span class="section_title"><a href="#more-math">More math?</a></span><span class="word_count">(2754w~14m)</span></li>
                    <li><span class="section_number">9</span><span class="section_title"><a href="#ai-research">AI research???</a></span><span class="word_count">(3043w~16m)</span></li>
                    <li><span class="section_number">10</span><span class="section_title"><a href="#appendices">(Appendices?)</a></span><span class="word_count">(1w~1m)</span></li>
                    <li><span class="section_number">11</span><span class="section_title"><a href="#approximation-theory">Approximation theory?</a></span><span class="word_count">(1214w~7m)</span></li>
                    <li><span class="section_number">12</span><span class="section_title"><a href="#bibliography-1">Bibliography</a></span><span class="word_count">(661w~4m)</span></li>
                    </ul>
                    </div>
                    <h2 id="using-this-document"><span class="section_number">1</span><span class="section_title">Using this document</span></h2>
                    <p>The target audience is …?</p>
                    <p>The goal is to do the last work that we will ever need to do.</p>
                    <p>The structure of this document: We begin with analytic philosophy because we have to understand what words mean. Then …? Finally we try to build the thing.</p>
                    <p>The plan (I'm really clueless):</p>
                    <ol>
                    <li>Learn AI.</li>
                    <li>???</li>
                    <li>PROFIT</li>
                    </ol>
                    <p>Notes to self:</p>
                    <ul>
                    <li>Sections with superfluous question marks should be rewritten.</li>
                    <li>We should not use footnotes for references?<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></li>
                    <li>The writing must still be usable even if all footnotes are removed.</li>
                    </ul>
                    <h2 id="some-analytic-philosophy"><span class="section_number">2</span><span class="section_title">Some analytic philosophy</span></h2>
                    <p>Key ideas???</p>
                    <ul>
                    <li>X <em>learns</em> Y iff X <em>causes</em> itself to <em>get better</em> at Y. (A teacher is a <em>contributing factor</em>, but the student itself is the <em>cause</em>.)</li>
                    <li>Learning is self-improvement.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></li>
                    <li>To <em>learn</em> X is to <em>become more intelligent</em> in X?</li>
                    <li>To be <em>intelligent</em> in X is to be proficient in X. Intelligence is proficiency.</li>
                    <li>Intelligence depends on task/goal/measurement. There is no such thing as general intelligence or absolute intelligence.</li>
                    </ul>
                    <div class="local_table_of_contents">
                    <ul>
                    <li><span class="section_number">2.1</span><span class="section_title"><a href="#analytic-philosophy-is-using-words-carefully">Analytic philosophy is using words carefully</a></span><span class="word_count">(47w~1m)</span></li>
                    <li><span class="section_number">2.2</span><span class="section_title"><a href="#what-is-necessary-for-learning">What is necessary for learning??</a></span><span class="word_count">(11w~1m)</span></li>
                    <li><span class="section_number">2.3</span><span class="section_title"><a href="#we-dont-know-what-intelligence-is">We don't know what intelligence is</a></span><span class="word_count">(128w~1m)</span></li>
                    <li><span class="section_number">2.4</span><span class="section_title"><a href="#intelligence-and-learning">Intelligence and learning</a></span><span class="word_count">(67w~1m)</span></li>
                    <li><span class="section_number">2.5</span><span class="section_title"><a href="#prediction">Prediction</a></span><span class="word_count">(47w~1m)</span></li>
                    <li><span class="section_number">2.6</span><span class="section_title"><a href="#toward-a-unified-theory-of-learning">Toward a unified theory of learning</a></span><span class="word_count">(108w~1m)</span></li>
                    <li><span class="section_number">2.7</span><span class="section_title"><a href="#content-plan">Content plan?</a></span><span class="word_count">(36w~1m)</span></li>
                    <li><span class="section_number">2.8</span><span class="section_title"><a href="#learnability">Learnability</a></span><span class="word_count">(61w~1m)</span></li>
                    <li><span class="section_number">2.9</span><span class="section_title"><a href="#approximation-vs-estimation">Approximation vs estimation</a></span><span class="word_count">(87w~1m)</span></li>
                    <li><span class="section_number">2.10</span><span class="section_title"><a href="#philosophy-science-and-engineering">Philosophy, science, and engineering</a></span><span class="word_count">(56w~1m)</span></li>
                    <li><span class="section_number">2.11</span><span class="section_title"><a href="#interpretability">Interpretability?</a></span><span class="word_count">(5w~1m)</span></li>
                    </ul>
                    </div>
                    <h3 id="analytic-philosophy-is-using-words-carefully"><span class="section_number">2.1</span><span class="section_title">Analytic philosophy is using words carefully</span></h3>
                    <p>By &quot;analytic&quot;, we mean the following. First we find out the generally accepted meaning of a word. Then we infer what that meaning implies. We use only logic and language.</p>
                    <p>An example of analytic philosophy is finding that &quot;bachelor&quot; implies &quot;unmarried&quot; and &quot;wifeless&quot;.</p>
                    <h3 id="what-is-necessary-for-learning"><span class="section_number">2.2</span><span class="section_title">What is necessary for learning??</span></h3>
                    <p>Learning requires changeable <em>internal state</em>.</p>
                    <p>experience? mistakes? memory?</p>
                    <h3 id="we-dont-know-what-intelligence-is"><span class="section_number">2.3</span><span class="section_title">We don't know what intelligence is</span></h3>
                    <p>As of 2018 we still have not agreed on what intelligence is.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> <span class="citation" data-cites="sep-artificial-intelligence">[<a href="#ref-sep-artificial-intelligence">6</a>]</span></p>
                    <p>Intelligent means smart.</p>
                    <p>In politics, intelligence is covert warfare, and is often contrasted against physical power.</p>
                    <p>I think the most general definition is &quot;Intelligence measures an agent's ability to achieve goals in a wide range of environments&quot; <span class="citation" data-cites="DefineMachIntel">[<a href="#ref-DefineMachIntel">17</a>]</span><span class="citation" data-cites="Legg2007Collection">[<a href="#ref-Legg2007Collection">16</a>]</span>. I think it subsumes all other definitions of intelligence in all other fields such as psychology.</p>
                    <p>&quot;Intelligent&quot; means &quot;does something well&quot;?</p>
                    <p>What is the etymology of &quot;intelligence&quot;? Opinions differ.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> The word &quot;intelligent&quot; might come from a Latin word that means &quot;to choose between&quot;.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a></p>
                    <p>As of 2018 I still haven't seen how I can write anything without <em>conflating internal state and external behavior</em>. Thus, for progress, I commit the duck-typing<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a> fallacy: &quot;If it <em>looks</em> intelligent, then it <em>is</em> intelligent.&quot;</p>
                    <h3 id="intelligence-and-learning"><span class="section_number">2.4</span><span class="section_title">Intelligence and learning</span></h3>
                    <p>To <em>learn</em> X is to <em>become more intelligent</em> in X.</p>
                    <p>To learn is to increase intelligence.</p>
                    <p>Both intelligence and learning requires measuring <em>how well</em> something is done.</p>
                    <p>What is the relationship between intelligence and learning? Can we have one without the other? Yes. A system that stops learning after it obtains intelligence is still intelligent. A computer program with sufficiently many conditionals is intelligent, but it never learns.</p>
                    <h3 id="prediction"><span class="section_number">2.5</span><span class="section_title">Prediction</span></h3>
                    <p>To predict is to foretell.<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a> To predict something is to say it before it happens.</p>
                    <p>Prediction is a justified belief whose truth is unknown to the predictor. Thus prediction is <em>almost</em> knowledge (justified true belief).</p>
                    <p>Prediction is extrapolation.</p>
                    <p>Prediction is uncertain. Prediction is probabilistic.</p>
                    <p>Predicting the past is called &quot;counterfactual reasoning&quot;.</p>
                    <h3 id="toward-a-unified-theory-of-learning"><span class="section_number">2.6</span><span class="section_title">Toward a unified theory of learning</span></h3>
                    <p>What is learning?</p>
                    <p>To learn is to avoid repeating past mistakes.</p>
                    <p>What does learning require? What is necessary for learning?</p>
                    <p>Learning requires feedback error signal.</p>
                    <p>These things are similar:</p>
                    <ul>
                    <li>hysteresis</li>
                    <li>memory</li>
                    <li>smoothing</li>
                    <li>infinite-impulse-response filter</li>
                    </ul>
                    <p><em>Optimal reverse prediction</em> unifies supervised and unsupervised learning <span class="citation" data-cites="xu2009optimal">[<a href="#ref-xu2009optimal">39</a>]</span>.</p>
                    <p><span class="citation" data-cites="white2012generalized">[<a href="#ref-white2012generalized">37</a>]</span> generalizes <span class="citation" data-cites="xu2009optimal">[<a href="#ref-xu2009optimal">39</a>]</span> to non-linear predictors.</p>
                    <p>TODO Unify learning, prediction, modeling, approximation, control, hysteresis, memory</p>
                    <p>Hysteresis, memory, and learning?</p>
                    <p>Is <a href="https://en.wikipedia.org/wiki/Hysteresis">hysteresis</a> learning? Is hysteresis memory? Does intelligence require learning?</p>
                    <p>An intelligent system does not have to learn.</p>
                    <p>A non-learning intelligent system will continue to satisfy its goal as long as the system stays in the environments it is familiar with.</p>
                    <p>Is it possible to accomplish the same goal in different environments without learning?</p>
                    <p><a href="https://en.wikipedia.org/wiki/Hysteresis#Models_of_hysteresis">https://en.wikipedia.org/wiki/Hysteresis#Models_of_hysteresis</a></p>
                    <p>Use discrete sequences</p>
                    <p>Gradient descent</p>
                    <p><a href="https://forum.azimuthproject.org/discussion/1538/machine-learning">https://forum.azimuthproject.org/discussion/1538/machine-learning</a></p>
                    <h3 id="content-plan"><span class="section_number">2.7</span><span class="section_title">Content plan?</span></h3>
                    <ul>
                    <li>What is the relationship between intelligence, complexity, and compression?</li>
                    <li>What is the &quot;everything is compression&quot; view of intelligence?</li>
                    <li>Why does AI/ML work?</li>
                    <li>Must we pick an area of interest? Speech recognition? Computer vision? Natural language processing? Speech synthesis?</li>
                    </ul>
                    <h3 id="learnability"><span class="section_number">2.8</span><span class="section_title">Learnability</span></h3>
                    <p>Key ideas:</p>
                    <ul>
                    <li>Smoother functions are more learnable (easier to learn).</li>
                    <li>Convex boundary is more learnable than concave boundary.</li>
                    </ul>
                    <p>A polyhedron is a three-dimensional polygon.</p>
                    <p>A polytope is a higher-dimensional polyhedron.</p>
                    <p>The analogy is polytope : polyhedron : polygon = hypercube : cube : square.</p>
                    <p>The boundary of a cluster is a polytope.</p>
                    <p>A cluster with convex polytope boundary is more learnable than a cluster with concave polytope boundary.</p>
                    <h3 id="approximation-vs-estimation"><span class="section_number">2.9</span><span class="section_title">Approximation vs estimation</span></h3>
                    <p>Differences:</p>
                    <ul>
                    <li>Approximation is part of analysis. Estimation is part of statistics.</li>
                    <li>Approximation does not involve sampling. Estimation involves sampling.</li>
                    <li>Epistemology: Approximation converges to a <em>knowable</em> value. Estimation <em>may</em> converge to a possibly <em>unknowable</em> value (the value exists but it is impractical for us to know what it actually is). Example: we <em>approximate</em> pi, and we <em>estimate</em> the height of all living people on Earth.</li>
                    <li>Epistemology: Approximation does not guess. Estimation does.</li>
                    </ul>
                    <p>Similarities:</p>
                    <ul>
                    <li>Both has a notion of &quot;error&quot;. Approximation has error. Estimation has bias and uncertainty.</li>
                    <li>Both are instances of modeling (simplification).</li>
                    </ul>
                    <h3 id="philosophy-science-and-engineering"><span class="section_number">2.10</span><span class="section_title">Philosophy, science, and engineering</span></h3>
                    <div class="local_table_of_contents">
                    <ul>
                    <li><span class="section_number">2.10.1</span><span class="section_title"><a href="#what-people-do">What people do</a></span><span class="word_count">(53w~1m)</span></li>
                    </ul>
                    </div>
                    <h4 id="what-people-do"><span class="section_number">2.10.1</span><span class="section_title">What people do</span></h4>
                    <ul>
                    <li>Philosophers <strong>seek</strong> the <em>truth</em>.</li>
                    <li>Scientists <strong>find</strong> the <em>truth</em> about <em>reality</em>.</li>
                    <li>Engineers <strong>change</strong> <em>reality</em>.</li>
                    </ul>
                    <p><em>Philosophers</em> ask questions that advance science and engineering.</p>
                    <p><em>Scientists</em> craft falsifiable theories and do theory-falsifying experiments. These experiments discover some truth about reality. This truth gives the philosophers clues about what questions to ask next.</p>
                    <p><em>Engineers</em> builds things based on philosophy and science.</p>
                    <h3 id="interpretability"><span class="section_number">2.11</span><span class="section_title">Interpretability?</span></h3>
                    <p>&lt;2018-09-28&gt; Book: &quot;interpretable machine learning&quot;<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a></p>
                    <h2 id="mathematics-of-intelligence-activities"><span class="section_number">3</span><span class="section_title">Mathematics of intelligence activities</span></h2>
                    <p>What we are going to do here: We mathematically model learning, classification, prediction, approximation, and modeling. We do not care about implementation.</p>
                    <p>Let <span class="math inline">\( R = [-1,1] \)</span> or <span class="math inline">\( R = [0,1] \)</span>?</p>
                    <p>Key ideas:</p>
                    <ul>
                    <li>&quot;Getting better&quot; is modeled by monotonically increasing sequence.</li>
                    <li>A classification is a surjective function. A classifier is an approximation of a classification.</li>
                    </ul>
                    <p>Some people tried something similar<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a><a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a>.</p>
                    <div class="local_table_of_contents">
                    <ul>
                    <li><span class="section_number">3.1</span><span class="section_title"><a href="#getting-better-at-something">Getting better at something</a></span><span class="word_count">(122w~1m)</span></li>
                    <li><span class="section_number">3.2</span><span class="section_title"><a href="#classification-is-surjection">Classification is surjection</a></span><span class="word_count">(105w~1m)</span></li>
                    <li><span class="section_number">3.3</span><span class="section_title"><a href="#prediction-1">Prediction</a></span><span class="word_count">(125w~1m)</span></li>
                    <li><span class="section_number">3.4</span><span class="section_title"><a href="#compression">Compression??</a></span><span class="word_count">(1w~1m)</span></li>
                    <li><span class="section_number">3.5</span><span class="section_title"><a href="#convexity-of-sets-and-functions">Convexity of sets and functions</a></span><span class="word_count">(81w~1m)</span></li>
                    <li><span class="section_number">3.6</span><span class="section_title"><a href="#machine-learning">Machine learning</a></span><span class="word_count">(82w~1m)</span></li>
                    <li><span class="section_number">3.7</span><span class="section_title"><a href="#learner">Learner</a></span><span class="word_count">(208w~2m)</span></li>
                    <li><span class="section_number">3.8</span><span class="section_title"><a href="#ml-and-hilbert-spaces">ML and Hilbert spaces?</a></span><span class="word_count">(54w~1m)</span></li>
                    <li><span class="section_number">3.9</span><span class="section_title"><a href="#bibliography">Bibliography???</a></span><span class="word_count">(4w~1m)</span></li>
                    <li><span class="section_number">3.10</span><span class="section_title"><a href="#colt-measuring-intelligence">COLT: measuring intelligence</a></span><span class="word_count">(285w~2m)</span></li>
                    <li><span class="section_number">3.11</span><span class="section_title"><a href="#trivia-correspondence-between-surjection-partition-and-equivalence">Trivia: Correspondence between surjection, partition, and equivalence</a></span><span class="word_count">(79w~1m)</span></li>
                    </ul>
                    </div>
                    <h3 id="getting-better-at-something"><span class="section_number">3.1</span><span class="section_title">Getting better at something</span></h3>
                    <p>&quot;Better&quot; implies an ordering of goodness.</p>
                    <p>&quot;Get better&quot; implies time.</p>
                    <p>Here we model &quot;get better&quot;.</p>
                    <p>&quot;Get better&quot; means <em>monotonically increasing test score</em>.</p>
                    <p>Let there be a system. Devise a test. Let the system do the test several times. Let the test results be the sequence <span class="math inline">\(X = x_1, x_2, \ldots, x_n\)</span>. We say that the system is <em>getting better</em> at that test iff <span class="math inline">\(x_1 &lt; x_2 &lt; \ldots &lt; x_n\)</span> (that is iff the sequence of test scores is <em>monotonically increasing</em>).</p>
                    <p>How do we formalize &quot;experience&quot;? &quot;Experience&quot; can be modeled by a sequence?</p>
                    <p>A test only measures how good the subject is at <em>doing that test</em>. What justifies our belief that a high test score implies the ability to do things <em>similar</em> to the test?</p>
                    <h3 id="classification-is-surjection"><span class="section_number">3.2</span><span class="section_title">Classification is surjection</span></h3>
                    <p>Key ideas:</p>
                    <ul>
                    <li>A classification is a surjective function.</li>
                    <li>A classifier is an approximation of a classification.</li>
                    </ul>
                    <p>Relationship between classification and prediction: A classifier tries to <em>predict</em> the class of things in a domain.</p>
                    <p>Let <span class="math inline">\(X\)</span> be a set of things that we want to classify.</p>
                    <p>Let <span class="math inline">\(N\)</span> be the set of <em>class indexes</em>. We assume that <span class="math inline">\(N\)</span> is a finite set of some first natural numbers. This set represents class &quot;names&quot;.</p>
                    <p>The <em>class index</em> of <span class="math inline">\(x\)</span> is <span class="math inline">\(c(x)\)</span>.</p>
                    <p>A <em>classification</em> is a surjective function <span class="math inline">\(c : X \to N\)</span>. &quot;Surjective&quot; means that there is no empty class (there is no unused class index).</p>
                    <p>A <em>classifier</em> is an <em>approximation</em> of a classification.</p>
                    <h3 id="prediction-1"><span class="section_number">3.3</span><span class="section_title">Prediction</span></h3>
                    <p>Consider the sequence <span class="math inline">\(x = x_1, \ldots, x_n\)</span>. What is the most likely continuation? Schmidhuber et al. (link?) shows us how to answer this with Solomonoff algorithmic probability<span class="citation" data-cites="solomonoff1996does">[<a href="#ref-solomonoff1996does">30</a>]</span>, which is unfortunately incomputable.</p>
                    <p>Let <span class="math inline">\(a\)</span> be the input type, <span class="math inline">\(b\)</span> be the output type, and <span class="math inline">\(g : a \to b\)</span>. A <em>predictor</em> is a function. Iff <span class="math inline">\(b\)</span> is finite, then <span class="math inline">\(f\)</span> is a <em>classifier</em>. A <em>feature</em> inhabits <span class="math inline">\(a \to \Real\)</span>. A <em>data</em> or an <em>example</em> is a tuple <span class="math inline">\((x,y) : (a,b)\)</span>.</p>
                    <p>A <em>linear predictor</em> is the equation <span class="math inline">\(y = w \cdot f(x)\)</span> where <span class="math inline">\(w\)</span> is the <em>weight vector</em>, <span class="math inline">\(f(x) = (f_1(x),\ldots,f_n(x))\)</span> is the <em>feature vector</em> of <span class="math inline">\(x\)</span>, <span class="math inline">\(f_k(x)\)</span> is the $k$th feature, <span class="math inline">\(x\)</span> is the input, and <span class="math inline">\(y\)</span> is the predicted output. The predictor is linear in <span class="math inline">\(w\)</span>.</p>
                    <h3 id="compression"><span class="section_number">3.4</span><span class="section_title">Compression??</span></h3>
                    <h3 id="convexity-of-sets-and-functions"><span class="section_number">3.5</span><span class="section_title">Convexity of sets and functions</span></h3>
                    <p>Convex is the shape of protruding fat belly.</p>
                    <p>Concave is the shape of pectus excavatum.</p>
                    <p>Let <span class="math inline">\(p, q \in S\)</span> be two points and <span class="math inline">\(L(p,q) \subseteq S\)</span> be the line segment from <span class="math inline">\(p\)</span> to <span class="math inline">\(q\)</span>. The set <span class="math inline">\(S\)</span> is <em>convex</em> iff <span class="math inline">\(\forall p,q \in S : L(p,q) \subseteq S\)</span>.</p>
                    <p>A function <span class="math inline">\(f : \Real \to \Real\)</span> is <em>convex</em> iff the area above its graph is a convex set. That area is <span class="math inline">\(\{(x,y) ~|~ x \in \Real, ~ y &gt; f(x)\}\)</span>.</p>
                    <h3 id="machine-learning"><span class="section_number">3.6</span><span class="section_title">Machine learning</span></h3>
                    <p>Why are there so many machine learning algorithms?</p>
                    <p>Machine learning is finding a function fitting a data list, minimizing error on unseen data. Machine learning is about how program improves with experience.</p>
                    <p>Find a function fitting the data and minimizing the <em>loss function</em>.</p>
                    <p>Given <span class="math inline">\([(x_1,y_1),\ldots,(x_n,y_n)]\)</span>, find <span class="math inline">\(f\)</span> minimizing <span class="math inline">\(\sum_k \norm{f(x_k) - y_k}^2\)</span>.</p>
                    <p>A <em>model</em> is a constrained optimization problem: Given <span class="math inline">\(C\)</span>, compute <span class="math inline">\(\min_{x \in C} f(x)\)</span> or <span class="math inline">\(\argmin_{x \in C} f(x)\)</span>. If <span class="math inline">\(C\)</span> is discrete, use dynamic programming. If <span class="math inline">\(C\)</span> is continuous, use gradient descent.</p>
                    <h3 id="learner"><span class="section_number">3.7</span><span class="section_title">Learner</span></h3>
                    <p>A <em>learner</em> inhabits <span class="math inline">\([(a,b)] \to (a \to b)\)</span>.</p>
                    <p>A <em>loss function</em> inhabits <span class="math inline">\((a,b,\Real^\infty) \to \Real\)</span>.</p>
                    <p>The <em>training loss</em> of <span class="math inline">\(g(x) = w \cdot f(x)\)</span> with respect to <span class="math inline">\(D\)</span> is <span class="math inline">\(\frac{1}{|D|} \sum_{(x,y) \in D} L(x,y,w)\)</span> where <span class="math inline">\(L\)</span> is the loss function.</p>
                    <p>Learning is finding <span class="math inline">\(w\)</span> that minimizes the training loss.</p>
                    <p>Let <span class="math inline">\(y \in \{-1,+1\}\)</span>. The <em>score</em> of <span class="math inline">\(f\)</span> for <span class="math inline">\((x,y)\)</span> is <span class="math inline">\(f(x)\)</span>. The <em>margin</em> of <span class="math inline">\(f\)</span> for <span class="math inline">\((x,y)\)</span> is <span class="math inline">\(f(x) \cdot y\)</span>.</p>
                    <p>Binarization of <span class="math inline">\(f\)</span> is <span class="math inline">\(\sgn \circ f\)</span>.</p>
                    <p>Least-squares linear regression</p>
                    <p>Minimize training loss</p>
                    <p>Gradient descent training with initial weight <span class="math inline">\(w_1\)</span>, iteration count <span class="math inline">\(T\)</span>, and step size <span class="math inline">\(\eta\)</span>: Let <span class="math inline">\(K : \Real^n \to \Real\)</span> be the training loss function. Let <span class="math inline">\(\nabla K\)</span> be the gradient of <span class="math inline">\(K\)</span>. The weight update equation is <span class="math inline">\(w_{t+1} = w_t - \eta \cdot (\nabla K)(w_t)\)</span> where <span class="math inline">\(w_1\)</span> may be random. The training result is <span class="math inline">\(w_T\)</span>.</p>
                    <p>Stochastic gradient descent (SGD) training: <span class="math inline">\(w_{t+1} = w_t - \eta \cdot (\nabla(L~x_t~y_t))(w_t)\)</span>. Note the usage of the loss function <span class="math inline">\(L\)</span> instead of the training loss function <span class="math inline">\(K\)</span>.</p>
                    <p>SGD is <em>online</em> or <em>incremental</em> training.</p>
                    <p>Classification is regression with zero-one loss function. Every classification can be turned into regression by using <em>hinge loss</em> or <em>logistic regression</em>.</p>
                    <p>The <em>logistic function</em> is <span class="math inline">\(f(x) = \frac{1}{1 + e^{-x}}\)</span>.</p>
                    <p>Nearest neighbor with training data list <span class="math inline">\(D\)</span>: <span class="math inline">\(g(x&#39;) = y\)</span> where <span class="math inline">\((x,y) \in D\)</span> minimizing <span class="math inline">\(\norm{f(x&#39;) - f(x)}^2\)</span>.</p>
                    <h3 id="ml-and-hilbert-spaces"><span class="section_number">3.8</span><span class="section_title">ML and Hilbert spaces?</span></h3>
                    <p>Practically all machine learning cases deal with functions that is continuous enough to form a Hilbert space. Every classification problem in the real world can be modeled by as a function <span class="math inline">\(f : R^\infty \to R\)</span>. Consider the case where <span class="math inline">\(R = [0,1]\)</span>. Continuous map from hyperplane <span class="math inline">\(R^n\)</span> to line <span class="math inline">\(R\)</span>.</p>
                    <h3 id="bibliography"><span class="section_number">3.9</span><span class="section_title">Bibliography???</span></h3>
                    <p><span class="citation" data-cites="DeepArch">[<a href="#ref-DeepArch">4</a>]</span></p>
                    <p><span class="citation" data-cites="DeepLearning">[<a href="#ref-DeepLearning">12</a>]</span></p>
                    <p><span class="citation" data-cites="RepLearn">[<a href="#ref-RepLearn">5</a>]</span></p>
                    <p><span class="citation" data-cites="SuttonBartoRein">[<a href="#ref-SuttonBartoRein">33</a>]</span></p>
                    <p>Algorithmic information theory <span class="citation" data-cites="AlgoInfTh">[<a href="#ref-AlgoInfTh">13</a>]</span></p>
                    <h3 id="colt-measuring-intelligence"><span class="section_number">3.10</span><span class="section_title">COLT: measuring intelligence</span></h3>
                    <ul>
                    <li><a href="https://en.wikipedia.org/wiki/Computational_learning_theory">Wikipedia: Computational learning theory</a>
                    <ul>
                    <li>What is the goal of computational learning theory?
                    <ul>
                    <li>&quot;Give a rigorous, computationally detailed and plausible account of how learning can be done.&quot; [Angluin1992]</li>
                    </ul></li>
                    <li>&quot;a subfield of Artificial Intelligence devoted to studying the design and analysis of machine learning algorithms&quot;</li>
                    </ul></li>
                    <li>Supervised learning is extrapolating a function from finite samples. Usually, the function is high-dimensional, and the samples are few.</li>
                    <li>It is simple to measure learning success in perfect information games such as chess. Chess also doesn't require any sensors and motors.</li>
                    </ul>
                    <div class="local_table_of_contents">
                    <ul>
                    <li><span class="section_number">3.10.1</span><span class="section_title"><a href="#what-colt">What COLT?</a></span><span class="word_count">(205w~2m)</span></li>
                    </ul>
                    </div>
                    <h4 id="what-colt"><span class="section_number">3.10.1</span><span class="section_title">What COLT?</span></h4>
                    <ul>
                    <li>2000, György Turán, <a href="https://link.springer.com/article/10.1023%2FA%3A1018948021083">Remarks on COLT</a></li>
                    <li>2016, Krendzelak, Jakab, <a href="https://ieeexplore.ieee.org/document/7802092/">Fundamental principals of Computational Learning Theory</a>
                    <ul>
                    <li>Reading queue:
                    <ul>
                    <li>D. Angluin, C. Smith, &quot;Inductive inference: theory and methods&quot;, A.C.M. Computing Surveys, vol. 15, pp. 237-269, 1983.</li>
                    <li>M. Anthony, N. Biggs, &quot;Computational Learning Theory&quot; in , Cambridge university press, 1992.</li>
                    <li>M.J. Kearns, &quot;The computational Complexity of Machine Learning&quot; in , The MIT Press, May 1990.</li>
                    <li>L.G. Valiant, &quot;A theory of the learnable&quot;, Communications of the A.C.M., vol. 27, no. 11, pp. 1134-1142, 1984.</li>
                    <li>L. Pitt, L.G. Valiant, &quot;Computational limitations on learning from examples&quot;, Journal of the A.C.M., vol. 35, no. 4, pp. 965-984, 1988.</li>
                    </ul></li>
                    </ul></li>
                    <li>helpful slides <a href="https://cs.uwaterloo.ca/~klarson/teaching/W15-486/lectures/22Colt.pdf">https://cs.uwaterloo.ca/~klarson/teaching/W15-486/lectures/22Colt.pdf</a></li>
                    <li>Bertoni et al. http://elearning.unimib.it/pluginfile.php/283303/mod_resource/content/1/Apprendimento_Automatico/Computational_Learning.pdf</li>
                    <li><a href="https://stats.stackexchange.com/questions/142906/what-does-pac-learning-theory-mean">https://stats.stackexchange.com/questions/142906/what-does-pac-learning-theory-mean</a></li>
                    <li><a href="https://pdfs.semanticscholar.org/presentation/fbbd/65646c8a81094864d4e0b0dfb9c1f22181af.pdf">https://pdfs.semanticscholar.org/presentation/fbbd/65646c8a81094864d4e0b0dfb9c1f22181af.pdf</a></li>
                    <li><a href="http://web.cs.iastate.edu/~honavar/colt-tutorial.pdf">http://web.cs.iastate.edu/~honavar/colt-tutorial.pdf</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Probably_approximately_correct_learning#cite_note-valiant-1">https://en.wikipedia.org/wiki/Probably_approximately_correct_learning#cite_note-valiant-1</a> A Theory of the Learnable Leslie G. Valiant 1984 <a href="http://web.mit.edu/6.435/www/Valiant84.pdf">http://web.mit.edu/6.435/www/Valiant84.pdf</a></li>
                    <li>kearns vazirani introduction <a href="ftp://ftp.cis.upenn.edu/pub/cse140/public_html/2002/kvpages.pdf">ftp://ftp.cis.upenn.edu/pub/cse140/public_html/2002/kvpages.pdf</a></li>
                    <li><a href="http://www.cis.upenn.edu/~mkearns/">http://www.cis.upenn.edu/~mkearns/</a> the computational complexity of machine learning <a href="http://www.cis.upenn.edu/~mkearns/papers/thesis.pdf">http://www.cis.upenn.edu/~mkearns/papers/thesis.pdf</a> <a href="https://www.worldscientific.com/worldscibooks/10.1142/10175">https://www.worldscientific.com/worldscibooks/10.1142/10175</a></li>
                    <li>2015 <a href="http://www.cs.tufts.edu/~roni/Teaching/CLT/">http://www.cs.tufts.edu/~roni/Teaching/CLT/</a></li>
                    <li>probably link to this <a href="http://bactra.org/notebooks/learning-theory.html">http://bactra.org/notebooks/learning-theory.html</a></li>
                    <li>semantics-first <a href="https://pdfs.semanticscholar.org/83e7/b615c165209af54dd0fe05c850bb08232625.pdf">https://pdfs.semanticscholar.org/83e7/b615c165209af54dd0fe05c850bb08232625.pdf</a></li>
                    <li>discrete approximation theory see the references of this paper <a href="https://www.worldscientific.com/doi/suppl/10.1142/10175/suppl_file/10175_chap01.pdf">https://www.worldscientific.com/doi/suppl/10.1142/10175/suppl_file/10175_chap01.pdf</a></li>
                    <li><a href="https://profs.info.uaic.ro/~ciortuz/SLIDES/ml7.pdf">https://profs.info.uaic.ro/~ciortuz/SLIDES/ml7.pdf</a></li>
                    </ul>
                    <p>Optimal learning for humans <a href="https://www.kqed.org/mindshift/37289">https://www.kqed.org/mindshift/37289</a></p>
                    <p>Curate from this <a href="https://thesecondprinciple.com/optimal-learning/">https://thesecondprinciple.com/optimal-learning/</a></p>
                    <p>Boston dynamics dog robots</p>
                    <p>Tesla car autopilots</p>
                    <p>Google and Uber self-driving cars</p>
                    <p><a href="https://www.quora.com/Will-we-ever-have-a-rigorous-and-robust-definition-for-intelligence">https://www.quora.com/Will-we-ever-have-a-rigorous-and-robust-definition-for-intelligence</a></p>
                    <p>rigorous definition of intelligence The new ai is general and rigorous, idsia Toward a theory of intelligence,RAND</p>
                    <p>A system responds to a stimulus. Define: a system is <em>adapting</em> to a stimulus if the same stimulus level elicits decreasing response level from the system. The stimulus level has to be increased to maintain the response level.</p>
                    <p>Is learning = adapting? Is intelligence = adaptiveness?</p>
                    <h3 id="trivia-correspondence-between-surjection-partition-and-equivalence"><span class="section_number">3.11</span><span class="section_title">Trivia: Correspondence between surjection, partition, and equivalence</span></h3>
                    <p>(You can skip this.)</p>
                    <p>To <em>partition</em> a set is to split that set into disjoint non-empty subsets.<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a> Each subset is called a <em>partition</em>.</p>
                    <p>The surjective function <span class="math inline">\(c : X \to N\)</span> corresponds to the partitions <span class="math inline">\(P_0, \ldots, P_n\)</span> where <span class="math inline">\(P_k = \{ x ~|~ c(x) = k \}\)</span> is the set of all things in class <span class="math inline">\(k\)</span>. Thus each set partitioning corresponds to a classification (a surjective function).</p>
                    <p>A partition also corresponds to an equivalence relation.</p>
                    <h2 id="building-an-intelligent-system"><span class="section_number">4</span><span class="section_title">Building an intelligent system</span></h2>
                    <p>What should we do? Where should we begin?</p>
                    <p>&lt;2018-12-25&gt; I'm thinking of using Prolog, even for neural networks.</p>
                    <div class="local_table_of_contents">
                    <ul>
                    <li><span class="section_number">4.1</span><span class="section_title"><a href="#software-approaches">Software approaches</a></span><span class="word_count">(7w~1m)</span></li>
                    <li><span class="section_number">4.2</span><span class="section_title"><a href="#hardware-approaches">Hardware approaches</a></span><span class="word_count">(118w~1m)</span></li>
                    <li><span class="section_number">4.3</span><span class="section_title"><a href="#building-classifiers">Building classifiers?</a></span><span class="word_count">(364w~2m)</span></li>
                    <li><span class="section_number">4.4</span><span class="section_title"><a href="#what-intelligence-engineering">What intelligence engineering???</a></span><span class="word_count">(3w~1m)</span></li>
                    <li><span class="section_number">4.5</span><span class="section_title"><a href="#should-i-read-these">Should I read these?</a></span><span class="word_count">(12w~1m)</span></li>
                    <li><span class="section_number">4.6</span><span class="section_title"><a href="#what-is-the-relationship-between-ml-and-statistical-modeling">What is the relationship between ML and statistical modeling?</a></span><span class="word_count">(9w~1m)</span></li>
                    <li><span class="section_number">4.7</span><span class="section_title"><a href="#how-do-we-categorize-ml-algorithms-what-is-the-common-thing">How do we categorize ML algorithms? What is the common thing?</a></span><span class="word_count">(65w~1m)</span></li>
                    <li><span class="section_number">4.8</span><span class="section_title"><a href="#doing-the-last-thing-we-will-ever-need-to-do">Doing the last thing we will ever need to do</a></span><span class="word_count">(186w~1m)</span></li>
                    <li><span class="section_number">4.9</span><span class="section_title"><a href="#machine-learning-sometimes-needs-philosophy">Machine learning sometimes needs philosophy</a></span><span class="word_count">(15w~1m)</span></li>
                    <li><span class="section_number">4.10</span><span class="section_title"><a href="#automating-reasoning">Automating reasoning?</a></span><span class="word_count">(10w~1m)</span></li>
                    <li><span class="section_number">4.11</span><span class="section_title"><a href="#what-are-some-tools-that-i-can-use-to-make-my-computer-learn">What are some tools that I can use to make my computer learn?</a></span><span class="word_count">(17w~1m)</span></li>
                    <li><span class="section_number">4.12</span><span class="section_title"><a href="#which-ai-architecture-has-won-lots-of-ai-contests-lately">Which AI architecture has won lots of AI contests lately?</a></span><span class="word_count">(47w~1m)</span></li>
                    <li><span class="section_number">4.13</span><span class="section_title"><a href="#what-is-the-question">What is the question?</a></span><span class="word_count">(15w~1m)</span></li>
                    <li><span class="section_number">4.14</span><span class="section_title"><a href="#how-might-we-build-a-seed-ai">How might we build a seed AI?</a></span><span class="word_count">(49w~1m)</span></li>
                    <li><span class="section_number">4.15</span><span class="section_title"><a href="#analogizers-recommender-systems-matrices">Analogizers, recommender systems, matrices</a></span><span class="word_count">(4w~1m)</span></li>
                    <li><span class="section_number">4.16</span><span class="section_title"><a href="#designing-a-humanoid">Designing a humanoid?</a></span><span class="word_count">(111w~1m)</span></li>
                    <li><span class="section_number">4.17</span><span class="section_title"><a href="#ai-approaches">AI approaches</a></span><span class="word_count">(63w~1m)</span></li>
                    </ul>
                    </div>
                    <h3 id="software-approaches"><span class="section_number">4.1</span><span class="section_title">Software approaches</span></h3>
                    <p>2018 &quot;One Big Net For Everything&quot;<a href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a></p>
                    <h3 id="hardware-approaches"><span class="section_number">4.2</span><span class="section_title">Hardware approaches</span></h3>
                    <div class="local_table_of_contents">
                    <ul>
                    <li><span class="section_number">4.2.1</span><span class="section_title"><a href="#architecture">Architecture?</a></span><span class="word_count">(62w~1m)</span></li>
                    <li><span class="section_number">4.2.2</span><span class="section_title"><a href="#an-array-of-fitzhugh-nagumo-cells">An array of FitzHugh-Nagumo cells?</a></span><span class="word_count">(26w~1m)</span></li>
                    <li><span class="section_number">4.2.3</span><span class="section_title"><a href="#ecobot-is-a-robot-that-can-feed-itself">EcoBot is a robot that can feed itself</a></span><span class="word_count">(31w~1m)</span></li>
                    </ul>
                    </div>
                    <h4 id="architecture"><span class="section_number">4.2.1</span><span class="section_title">Architecture?</span></h4>
                    <p>Most computers in 2017 have the von Neumann architecture, which suffers from the von Neumann bottleneck (the limited transfer rate between CPU and RAM). This architecture fits programming, but it fits training less, and it does not fit learning. This architecture does not suit machines with billions of sensors. This architecture does not preclude intelligence but the bottleneck incurs a great penalty.</p>
                    <h4 id="an-array-of-fitzhugh-nagumo-cells"><span class="section_number">4.2.2</span><span class="section_title">An array of FitzHugh-Nagumo cells?</span></h4>
                    <p>A FitzHugh-Nagumo cell is an electrical circuit implementing the FitzHugh-Nagumo model. FHN cells can be implemented in Field-programmable Analog Array (FPAA) <span class="citation" data-cites="CircuitFitzHughNagumo">[<a href="#ref-CircuitFitzHughNagumo">40</a>]</span>.</p>
                    <h4 id="ecobot-is-a-robot-that-can-feed-itself"><span class="section_number">4.2.3</span><span class="section_title">EcoBot is a robot that can feed itself</span></h4>
                    <p><a href="https://en.wikipedia.org/wiki/EcoBot">Wikipedia: EcoBot</a>: &quot;a class of energetically autonomous robots that can remain self-sustainable by collecting their energy from material, mostly waste matter, in the environment&quot;</p>
                    <h3 id="building-classifiers"><span class="section_number">4.3</span><span class="section_title">Building classifiers?</span></h3>
                    <div class="local_table_of_contents">
                    <ul>
                    <li><span class="section_number">4.3.1</span><span class="section_title"><a href="#classifiers">Classifiers?</a></span><span class="word_count">(205w~2m)</span></li>
                    <li><span class="section_number">4.3.2</span><span class="section_title"><a href="#nearest-something-classifier">Nearest-something classifier</a></span><span class="word_count">(103w~1m)</span></li>
                    <li><span class="section_number">4.3.3</span><span class="section_title"><a href="#what-mathematics">What mathematics??</a></span><span class="word_count">(11w~1m)</span></li>
                    <li><span class="section_number">4.3.4</span><span class="section_title"><a href="#mathematical-spaces">Mathematical spaces</a></span><span class="word_count">(47w~1m)</span></li>
                    </ul>
                    </div>
                    <h4 id="classifiers"><span class="section_number">4.3.1</span><span class="section_title">Classifiers?</span></h4>
                    <p>We assume that the classes are known. If the classes are not known, then the problem is called &quot;clustering&quot;.</p>
                    <p>Supervised learning. Training:</p>
                    <ul>
                    <li>A sample is a point. A class is a set of samples (points). A class's boundary is a polytope.</li>
                    <li>For each training class, construct a smallest polytope that bounds the class's samples. If the polytope is convex, good.</li>
                    </ul>
                    <p>Classes must not overlap/intersect.</p>
                    <p>We assume that clusters satisfy the cluster hypothesis<a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a>.</p>
                    <p>Traditional classifiers have two phases: training and performance. They no longer learns when they perform. The alternative is called &quot;lifelong learning&quot; or &quot;continual learning&quot;<a href="#fn14" class="footnote-ref" id="fnref14"><sup>14</sup></a><a href="#fn15" class="footnote-ref" id="fnref15"><sup>15</sup></a></p>
                    <p>Given some examples of <span class="math inline">\(c\)</span>, approximate <span class="math inline">\(c\)</span>.</p>
                    <p>Inputs:</p>
                    <ul>
                    <li>Some <em>training pairs</em>: <span class="math inline">\(c&#39; \subset c\)</span>. Every class must be represented.</li>
                    </ul>
                    <p>Outputs:</p>
                    <ul>
                    <li>Estimate <span class="math inline">\(c\)</span>.</li>
                    </ul>
                    <p>The type of a <em>classifier</em> is <span class="math inline">\(a \to b\)</span> where <span class="math inline">\(b\)</span> is countable. Iff <span class="math inline">\(|b| = 2\)</span>, the classifier is <em>binary</em>. Iff <span class="math inline">\(|b|\)</span> is finite, the classifier is <em>multi-class</em>.</p>
                    <p>A <em>quasiclassifier</em> is an inhabitant of <span class="math inline">\(\Real^\infty \to \Real\)</span>. A <em>predicate</em> <span class="math inline">\(p\)</span> turns a quasiclassifier <span class="math inline">\(q\)</span> into a classifier <span class="math inline">\(c~x = p~(q~x)\)</span>.</p>
                    <p>A multiclassifier can be made from binary classifiers.</p>
                    <p>The <em>maximum-margin hyperplane</em> separating the lower training set <span class="math inline">\(L\)</span> and the upper training set <span class="math inline">\(U\)</span> is the hyperplane <span class="math inline">\(h\)</span> such that <span class="math inline">\(\forall a \in U : h~a &gt; 0\)</span>,  <span class="math inline">\(\forall b \in L : h~b &lt; 0\)</span>, and <span class="math inline">\(\dist~h~(U \cup L)\)</span> is maximal.</p>
                    <h4 id="nearest-something-classifier"><span class="section_number">4.3.2</span><span class="section_title">Nearest-something classifier</span></h4>
                    <p>Nearest-centroid classifier<a href="#fn16" class="footnote-ref" id="fnref16"><sup>16</sup></a></p>
                    <p>Nearest-cluster classifier<a href="#fn17" class="footnote-ref" id="fnref17"><sup>17</sup></a><a href="#fn18" class="footnote-ref" id="fnref18"><sup>18</sup></a></p>
                    <p>Cover, T.M., Hart, P.E.: Nearest neighbor pattern classification. IEEE Trans. Inform. Theory IT-13(1), 21–27 (1967)</p>
                    <p>This is mathematically principled. This does not &quot;just work&quot;. This works and we understand why it works.</p>
                    <p>Nearest convex hull classification<span class="citation" data-cites="nalbantov2006nearest">[<a href="#ref-nalbantov2006nearest">21</a>]</span>.</p>
                    <p>Performing:</p>
                    <ul>
                    <li>Find the convex hull is nearest to the input point.</li>
                    </ul>
                    <p>Explainability: It is simple to explain an NCHC's decision. It is not simple to explain a neural network's decision.</p>
                    <p>Why does it classify this as that? Because it is the closest cluster.</p>
                    <p>Geometric learning / analogizer / learning by constructing convex hull / classification by cluster-convex-hull-boundary learning; analogizer</p>
                    <p>Voronoi classifier?</p>
                    <p>Find out the cluster centers. Let the Voronoi diagram be the boundary.</p>
                    <h4 id="what-mathematics"><span class="section_number">4.3.3</span><span class="section_title">What mathematics??</span></h4>
                    <p>Can we apply Aitken's delta-squared process<a href="#fn19" class="footnote-ref" id="fnref19"><sup>19</sup></a><a href="#fn20" class="footnote-ref" id="fnref20"><sup>20</sup></a> to machine learning algorithms?</p>
                    <h4 id="mathematical-spaces"><span class="section_number">4.3.4</span><span class="section_title">Mathematical spaces</span></h4>
                    <ul>
                    <li>What is a metric?</li>
                    <li>What is a norm?</li>
                    <li>What is a measure?</li>
                    <li><a href="https://en.wikipedia.org/wiki/Space_(mathematics)#Three_taxonomic_ranks">https://en.wikipedia.org/wiki/Space_(mathematics)#Three_taxonomic_ranks</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Topological_space#Classification_of_topological_spaces">https://en.wikipedia.org/wiki/Topological_space#Classification_of_topological_spaces</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Functional_analysis">https://en.wikipedia.org/wiki/Functional_analysis</a>
                    <ul>
                    <li>What is a Hilbert space?</li>
                    <li>What is a Banach space?</li>
                    <li>What is a Sobolev space?</li>
                    <li>What is a measure?
                    <ul>
                    <li>What is a Lebesgue measure?
                    <ul>
                    <li>What is an Lp space?
                    <ul>
                    <li><a href="https://en.wikipedia.org/wiki/Lp_space#Lp_spaces">Wikipedia: Lp space</a></li>
                    <li>How is it pronounced?
                    <ul>
                    <li>&quot;Lebesgue space with <span class="math inline">\(p\)</span>-norm&quot;</li>
                    </ul></li>
                    </ul></li>
                    <li>What is a small lp space?</li>
                    </ul></li>
                    </ul></li>
                    </ul></li>
                    </ul>
                    <h3 id="what-intelligence-engineering"><span class="section_number">4.4</span><span class="section_title">What intelligence engineering???</span></h3>
                    <h3 id="should-i-read-these"><span class="section_number">4.5</span><span class="section_title">Should I read these?</span></h3>
                    <ul>
                    <li><a href="https://medium.com/machine-learning-world/learning-path-for-machine-learning-engineer-a7d5dc9de4a4">How To Become A Machine Learning Engineer: Learning Path</a></li>
                    <li><a href="https://dzone.com/guides/artificial-intelligence-machine-learning-and-predi">https://dzone.com/guides/artificial-intelligence-machine-learning-and-predi</a></li>
                    </ul>
                    <h3 id="what-is-the-relationship-between-ml-and-statistical-modeling"><span class="section_number">4.6</span><span class="section_title">What is the relationship between ML and statistical modeling?</span></h3>
                    <h3 id="how-do-we-categorize-ml-algorithms-what-is-the-common-thing"><span class="section_number">4.7</span><span class="section_title">How do we categorize ML algorithms? What is the common thing?</span></h3>
                    <p>There are so many ML algorithms. What is the common thing?</p>
                    <ul>
                    <li>Online vs offline
                    <ul>
                    <li><a href="https://en.wikipedia.org/wiki/Online_machine_learning">Wikipedia: Online machine learning</a></li>
                    </ul></li>
                    <li>Discrete-time model vs continuous-time model
                    <ul>
                    <li>LTI (linear time-invariant) systems</li>
                    </ul></li>
                    <li>Assemble answers from these sources:
                    <ul>
                    <li><a href="https://en.wikipedia.org/wiki/Machine_learning#Approaches">Wikipedia: Machine learning, approaches</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Outline_of_machine_learning#Machine_learning_algorithms">Wikipedia: Outline of machine learning, algorithms</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Outline_of_machine_learning#Machine_learning_methods">Wikipedia: Outline of machine learning, methods</a></li>
                    <li><a href="https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/">A tour of machine learning algorithms</a></li>
                    <li><a href="https://towardsdatascience.com/types-of-machine-learning-algorithms-you-should-know-953a08248861">Types of machine learning algorithms you should know</a></li>
                    <li><a href="https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/">Common machine learning algorithms</a></li>
                    </ul></li>
                    </ul>
                    <h3 id="doing-the-last-thing-we-will-ever-need-to-do"><span class="section_number">4.8</span><span class="section_title">Doing the last thing we will ever need to do</span></h3>
                    <p>In his website<a href="#fn21" class="footnote-ref" id="fnref21"><sup>21</sup></a>, Jürgen Schmidhuber writes that he wants to build something smarter than him and then retire.</p>
                    <p>I want the same thing.</p>
                    <p>Schmidhuber's website floods the reader with too much content. It is hard for an outsider to tell whether he is genius or crazy. But he has lots of credentials.</p>
                    <p>Schmidhuber gave a Reddit mass-interview<a href="#fn22" class="footnote-ref" id="fnref22"><sup>22</sup></a>.</p>
                    <p>Schmidhuber has always been an optimist.<a href="#fn23" class="footnote-ref" id="fnref23"><sup>23</sup></a></p>
                    <p>Isn't Jacques Pitrat's CAIA similar in spirit to what Jürgen Schmidhuber wants? Unfortunately Jacques Pitrat's blog<a href="#fn24" class="footnote-ref" id="fnref24"><sup>24</sup></a> is even harder to understand than Schmidhuber's website.</p>
                    <p>Is Kyndi closest to what we want? &quot;Kyndi serves as a tireless digital assistant, identifying the documents and passages that require human judgment.&quot;<a href="#fn25" class="footnote-ref" id="fnref25"><sup>25</sup></a></p>
                    <p>Kyndi uses Prolog.</p>
                    <p>I need something similar to Kyndi but able to generate interesting questions for itself to answer. I want it to read journal articles and conference proceedings, understand them, and summarize them for me.</p>
                    <p>Concepts:</p>
                    <ul>
                    <li>artificial general intelligence</li>
                    <li>seed AI</li>
                    </ul>
                    <p>We need cooperation, not competititon. Google, DeepMind, Facebook, Tesla, Amazon, OpenAI, Uber, Waymo, and other companies and organizations globally waste massive effort reinventing each other's AI capabilities.</p>
                    <p>Abbreviations:</p>
                    <ul>
                    <li>AI: Artificial Intelligence</li>
                    <li>AGI: Artificial General Intelligence</li>
                    <li>ML: Machine Learning</li>
                    <li>COLT: Computational Learning Theory</li>
                    </ul>
                    <h3 id="machine-learning-sometimes-needs-philosophy"><span class="section_number">4.9</span><span class="section_title">Machine learning sometimes needs philosophy</span></h3>
                    <p>It is <em>sometimes</em> important to explain why a prediction works. <a href="http://blogs.cornell.edu/modelmeanings/2013/12/08/ml-philosophy-and-does-interpretation-matter/">http://blogs.cornell.edu/modelmeanings/2013/12/08/ml-philosophy-and-does-interpretation-matter/</a></p>
                    <h3 id="automating-reasoning"><span class="section_number">4.10</span><span class="section_title">Automating reasoning?</span></h3>
                    <p>What is reasoning? How do we automate reasoning? Prolog?</p>
                    <h3 id="what-are-some-tools-that-i-can-use-to-make-my-computer-learn"><span class="section_number">4.11</span><span class="section_title">What are some tools that I can use to make my computer learn?</span></h3>
                    <ul>
                    <li>Google TensorFlow?</li>
                    <li>Does OpenAI have tools?</li>
                    <li>Facebook?</li>
                    <li>Keras?</li>
                    </ul>
                    <h3 id="which-ai-architecture-has-won-lots-of-ai-contests-lately"><span class="section_number">4.12</span><span class="section_title">Which AI architecture has won lots of AI contests lately?</span></h3>
                    <ul>
                    <li>Is it LSTM RNN?</li>
                    <li>What is LSTM RNN?
                    <ul>
                    <li>&quot;long short-term memory recurrent neural network&quot;</li>
                    <li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></li>
                    <li>&quot;The expression <em>long short-term</em> refers to the fact that LSTM is a model for the <em>short-term memory</em> which can last for a <em>long</em> period of time.&quot; (<a href="https://en.wikipedia.org/wiki/Long_short-term_memory">Wikipedia</a>)</li>
                    </ul></li>
                    </ul>
                    <h3 id="what-is-the-question"><span class="section_number">4.13</span><span class="section_title">What is the question?</span></h3>
                    <ul>
                    <li>How do we make an AI?</li>
                    <li>How do we create a seed AI?</li>
                    </ul>
                    <h3 id="how-might-we-build-a-seed-ai"><span class="section_number">4.14</span><span class="section_title">How might we build a seed AI?</span></h3>
                    <ul>
                    <li>Use off-the-shelf computers.</li>
                    <li>Use supercomputers.</li>
                    <li>Use clusters.</li>
                    <li>Use computers over the Internet.</li>
                    <li>Raise an AI like raising a child.</li>
                    <li>Evolve a system. Create an environment with selection pressure. Run it long enough.
                    <ul>
                    <li><a href="https://en.wikipedia.org/wiki/Evolutionary_robotics">WP: Evolutionary robotics</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Evolutionary_computation">WP: Evolutionary computation</a></li>
                    </ul></li>
                    <li>What is TensorFlow? Keras? CNTK? Theano?
                    <ul>
                    <li>The building blocks of AI? Standardized AI components?</li>
                    </ul></li>
                    </ul>
                    <h3 id="analogizers-recommender-systems-matrices"><span class="section_number">4.15</span><span class="section_title">Analogizers, recommender systems, matrices</span></h3>
                    <ul>
                    <li><a href="https://medium.com/s/story/spotifys-discover-weekly-how-machine-learning-finds-your-new-music-19a41ab76efe">https://medium.com/s/story/spotifys-discover-weekly-how-machine-learning-finds-your-new-music-19a41ab76efe</a></li>
                    </ul>
                    <h3 id="designing-a-humanoid"><span class="section_number">4.16</span><span class="section_title">Designing a humanoid?</span></h3>
                    <p>A humanoid is a human-shaped robot.</p>
                    <p>There are several choices: Make a machine that resembles human, Make a cyborg (a human-machine hybrid with more human part), or Mind upload.</p>
                    <div class="local_table_of_contents">
                    <ul>
                    <li><span class="section_number">4.16.1</span><span class="section_title"><a href="#power-plant">Power plant</a></span><span class="word_count">(57w~1m)</span></li>
                    <li><span class="section_number">4.16.2</span><span class="section_title"><a href="#sensors">Sensors</a></span><span class="word_count">(26w~1m)</span></li>
                    </ul>
                    </div>
                    <h4 id="power-plant"><span class="section_number">4.16.1</span><span class="section_title">Power plant</span></h4>
                    <p>It needs power plant with high power-to-mass and power-to-volume ratio for long-time low-power and short-time burst scenario. High-density sugar biobattery <span class="citation" data-cites="zhu2014high">[<a href="#ref-zhu2014high">41</a>]</span>. A microbial fuel cell capable of converting glucose to electricity at high rate and efficiency <span class="citation" data-cites="rabaey2003microbial">[<a href="#ref-rabaey2003microbial">25</a>]</span>. Sugar beats lithium ion.</p>
                    <p>Distributed processing, distributed energy generation.</p>
                    <p>Citric acid cycle. Oxidative phosphorylation.</p>
                    <p>Biomachine hybrid. A mixture of microbes and machine.</p>
                    <h4 id="sensors"><span class="section_number">4.16.2</span><span class="section_title">Sensors</span></h4>
                    <p>Billions of sensors. Light, sound, heat, itch, touch, gravity.</p>
                    <p>A strong enough brain.</p>
                    <p>How will it sustain itself?</p>
                    <p>How will it sense the world?</p>
                    <p>How will it manipulate the world?</p>
                    <h3 id="ai-approaches"><span class="section_number">4.17</span><span class="section_title">AI approaches</span></h3>
                    <ul>
                    <li>logic, symbolism</li>
                    <li>biology, connectionism</li>
                    <li>probabilistic logic programming</li>
                    </ul>
                    <p>What's trending in 2018??</p>
                    <ul>
                    <li>deep learning (DL)</li>
                    <li>generative adversarial network (GAN)</li>
                    <li>long short-term memory (LSTM)</li>
                    </ul>
                    <p>There are two ways to make an &quot;infinite-layer&quot; neural network:</p>
                    <ul>
                    <li>recurrent neural network (RNN), similar to IIR (infinite-impulse-response) filter in control theory</li>
                    <li>neural ordinary differential equations (NODE), similar to Riemann summation in calculus</li>
                    </ul>
                    <p>How many AI approaches are there? <a href="https://en.wikipedia.org/wiki/Portal:Artificial_intelligence">WP AI Portal</a> lists 4 approaches. Pedro Domingos lists 5 &quot;tribes&quot;.</p>
                    <h2 id="what-else-is-intelligence"><span class="section_number">5</span><span class="section_title">What else is intelligence???</span></h2>
                    <div class="local_table_of_contents">
                    <ul>
                    <li><span class="section_number">5.1</span><span class="section_title"><a href="#intelligence-is-an-ordering-2018-04-26">Intelligence is an ordering (2018-04-26)</a></span><span class="word_count">(186w~1m)</span></li>
                    <li><span class="section_number">5.2</span><span class="section_title"><a href="#intelligence-is-function-optimization-2018-04-27">Intelligence is function optimization (2018-04-27)</a></span><span class="word_count">(27w~1m)</span></li>
                    <li><span class="section_number">5.3</span><span class="section_title"><a href="#what-is-a-mathematical-theory-of-intelligence">What is a mathematical theory of intelligence?</a></span><span class="word_count">(399w~2m)</span></li>
                    <li><span class="section_number">5.4</span><span class="section_title"><a href="#historical-definitions">Historical definitions</a></span><span class="word_count">(28w~1m)</span></li>
                    <li><span class="section_number">5.5</span><span class="section_title"><a href="#what-is-a-neural-network">What is a neural network?</a></span><span class="word_count">(219w~2m)</span></li>
                    <li><span class="section_number">5.6</span><span class="section_title"><a href="#what-is-ai">What is AI?</a></span><span class="word_count">(76w~1m)</span></li>
                    <li><span class="section_number">5.7</span><span class="section_title"><a href="#how-do-we-measure-the-performance-of-a-learning-algorithm">How do we measure the performance of a learning algorithm?</a></span><span class="word_count">(10w~1m)</span></li>
                    </ul>
                    </div>
                    <h3 id="intelligence-is-an-ordering-2018-04-26"><span class="section_number">5.1</span><span class="section_title">Intelligence is an ordering (2018-04-26)</span></h3>
                    <p>This idea goes back at least to 2004 in <span class="citation" data-cites="hutter2004universal">[<a href="#ref-hutter2004universal">14</a>]</span>.</p>
                    <p>Intelligence is an <em>ordering</em> of systems.</p>
                    <p>An order is a transitive antisymmetric relation.</p>
                    <p><em>Intelligence depends on its measurement</em>. Absolute intelligence doesn't exist.</p>
                    <p>The <em>behavior</em> of a system is whatever it exhibits that can be observed from outside.</p>
                    <p>How do we decide which system is more intelligent?</p>
                    <p>Let <span class="math inline">\(A\)</span> be a system.</p>
                    <p>Let <span class="math inline">\(B\)</span> be a system.</p>
                    <p>Let <span class="math inline">\(T\)</span> be a task.</p>
                    <p>Let <span class="math inline">\(S\)</span> be a set of tasks.</p>
                    <p>Let <span class="math inline">\(T(A)\)</span> denote how well system <span class="math inline">\(A\)</span> does task <span class="math inline">\(T\)</span>. This is a number. Higher is better. We can invent any measurement. Our definition of &quot;intelligence&quot; is only as good as this measurement.</p>
                    <p>We say &quot;<span class="math inline">\(A\)</span> is <em><span class="math inline">\(T\)</span>-better</em> than <span class="math inline">\(B\)</span>&quot; iff <span class="math inline">\(T(A) &gt; T(B)\)</span>.</p>
                    <p>We say &quot;<span class="math inline">\(A\)</span> <em><span class="math inline">\(S\)</span>-dominates</em> <span class="math inline">\(B\)</span>&quot; iff <span class="math inline">\(T(A) &gt; T(B)\)</span> for every task <span class="math inline">\(T \in S\)</span>.</p>
                    <p>We define &quot;to be more <span class="math inline">\(S\)</span>-intelligent than&quot; to mean &quot;to <span class="math inline">\(S\)</span>-dominate&quot;.</p>
                    <p>The <span class="math inline">\(S\)</span>-domination relation forms a partial order of all systems.</p>
                    <p>That is how.</p>
                    <p>Example</p>
                    <p>Which is more intelligent, a dog or a rock?</p>
                    <p>That depends on the task set <span class="math inline">\(S\)</span>.</p>
                    <p>It's the rock if ( S = { sit still } ).</p>
                    <p>It's the dog if ( S = { move around } ).</p>
                    <h3 id="intelligence-is-function-optimization-2018-04-27"><span class="section_number">5.2</span><span class="section_title">Intelligence is function optimization (2018-04-27)</span></h3>
                    <p>Let <span class="math inline">\(g\)</span> be a goal function.</p>
                    <p>A system's <span class="math inline">\(g\)</span>-intelligence is how well it optimizes <span class="math inline">\(g\)</span>.</p>
                    <p>What is &quot;how well&quot;?</p>
                    <p>Optimization (extremization) is either minimization or maximization.</p>
                    <h3 id="what-is-a-mathematical-theory-of-intelligence"><span class="section_number">5.3</span><span class="section_title">What is a mathematical theory of intelligence?</span></h3>
                    <p>Here I try an alternative formalization to <span class="citation" data-cites="DefineMachIntel">[<a href="#ref-DefineMachIntel">17</a>]</span>.</p>
                    <p>Let <span class="math inline">\(E\)</span> be a set of <em>environments</em>.</p>
                    <p>Let <span class="math inline">\(G : E \to \Real\)</span> be a <em>goal function</em>. The value of <span class="math inline">\(G(e)\)</span> measures how well the agent performs in environment <span class="math inline">\(e\)</span>.</p>
                    <p>The <em>intelligence</em> of the agent <em>with respect to <span class="math inline">\(G\)</span> across $E$</em> is <span class="math inline">\(\int_E G\)</span>.</p>
                    <p>A <em>performance</em> consists of an agent and an environment.</p>
                    <p>Assumption: The agent cannot modify <span class="math inline">\(G\)</span>.</p>
                    <p>Behavior is a function taking an environment and outputing something.</p>
                    <p>Intelligence is <em>relative</em> to <span class="math inline">\(G\)</span> and <span class="math inline">\(E\)</span>: <em>goal</em> and <em>environment</em>.</p>
                    <p>If we see longevity as intelligence test, then an illiterate farmer who lives to 80 is more intelligent than a scientist who dies at 20, but a rock that has been there for 100 years would even be more intelligent than the farmer.</p>
                    <p>If we see money as intelligence test, then a corrupt politician who steals billions of dollars without getting caught is more intelligent than a honest farmer who only has tens of thousands of dollars.</p>
                    <p>Gaming the system is a sign of intelligence. It is hard to design a goal function that gives the desired outcome without undesired side effects.</p>
                    <p>IQ tests are intelligence measures with small environment set.</p>
                    <p>Lifespan may be an intelligence measure with huge environment set.</p>
                    <p>A human can optimize <em>several</em> goal functions across the same environment set. A human may be asked to clean a floor, to write a report, to run a company, to cook food, and to find the quickest route between home and office, and optimize them all.</p>
                    <p>Some goal functions for humans are (but perhaps not limited to):</p>
                    <ul>
                    <li>Maximize happiness</li>
                    <li>Minimize pain</li>
                    <li>Optimize the level of a chemical in the brain</li>
                    <li>Optimize the time integral of such chemical</li>
                    <li>Maximize the chance of survival</li>
                    </ul>
                    <p>But I don't know the root goal function that explains all those behaviors.</p>
                    <p>What are some mathematical definitions of intelligence?</p>
                    <ul>
                    <li>&quot;Intelligence measures an agent's ability to achieve goals in a wide range of environments.&quot; [Legg2006][Legg2008]</li>
                    <li><a href="https://www.researchgate.net/publication/323203054_Defining_intelligence">Shour2018</a>: &quot;Defining intelligence as a rate of problem solving and using the concept of network entropy enable measurement, comparison and calculation of collective and individual intelligence and of computational capacity.&quot;</li>
                    <li>Tononi integrated information theory. <a href="https://en.wikipedia.org/wiki/Integrated_information_theory">Wikipedia</a>.</li>
                    <li>Schmidhuber, Hutter, and team have used Solomonoff algorithmic probability and Kolmogorov complexity to define a theoretically optimal predictor they call AIXI.
                    <ul>
                    <li>J&quot;urgen Schmidhuber. <a href="http://www.idsia.ch/~juergen/newai/newai.html">Schmidhuber article</a>.</li>
                    <li><a href="http://www.cs.uic.edu/~piotr/cs594/Prashant-UniversalAI.pdf">Prashant's slides</a>. These define &quot;universal&quot; and &quot;optimal&quot;.</li>
                    </ul></li>
                    <li>Marcus Hutter approached intelligence from <em>algorithmic</em> complexity theory (Solomonoff induction) <span class="citation" data-cites="DefineMachIntel">[<a href="#ref-DefineMachIntel">17</a>]</span>.</li>
                    <li>Warren D. Smith approached intelligence from <em>computational</em> complexity theory (NP-completeness) <span class="citation" data-cites="WdsIntel WdsIntelSlide">[<a href="#ref-WdsIntelSlide">28</a>, <a href="#ref-WdsIntel">29</a>]</span></li>
                    </ul>
                    <p><span class="citation" data-cites="Legg2007Collection">[<a href="#ref-Legg2007Collection">16</a>]</span> is a collection of definitions of intelligence.</p>
                    <h3 id="historical-definitions"><span class="section_number">5.4</span><span class="section_title">Historical definitions</span></h3>
                    <p><a href="https://brocku.ca/MeadProject/sup/Boring_1923.html">Edwin Boring in 1923</a> proposed that we start out by defining intelligence as what intelligence tests measure &quot;until further scientific observation allows us to extend the definition&quot;.</p>
                    <h3 id="what-is-a-neural-network"><span class="section_number">5.5</span><span class="section_title">What is a neural network?</span></h3>
                    <p>What is a neural network?</p>
                    <ul>
                    <li>A <em>neuron</em> is a function in <span class="math inline">\(\Real^\infty \to \Real\)</span>.</li>
                    <li>A <em>neural network</em> layer is a function in <span class="math inline">\(\Real^\infty \to \Real^\infty\)</span>.</li>
                    <li>Why do neural networks work?
                    <ul>
                    <li><a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">Wikipedia: Universal approximation theorem</a></li>
                    </ul></li>
                    </ul>
                    <ul>
                    <li>What is statistical learning?</li>
                    <li>What is backpropagation, from functional analysis point of view?</li>
                    <li>Consider endofunctions of infinite-dimensional real tuple space. That is, consider <span class="math inline">\(f, g : \Real^\infty \to \Real^\infty\)</span>.
                    <ul>
                    <li>What is the distance between them?</li>
                    </ul></li>
                    <li>Reductionistically, a brain can be thought as a function in <span class="math inline">\(\Real \to \Real^\infty \to \Real^\infty\)</span>.
                    <ul>
                    <li>The first parameter is time.</li>
                    <li>The second parameter is the sensor signals.</li>
                    <li>The output of the function is the actuator signals.</li>
                    <li>Can we model a brain by such <a href="https://en.wikipedia.org/wiki/Functional_differential_equation">functional differential equation</a> involving <a href="https://en.wikipedia.org/wiki/Functional_derivative">functional derivative</a>s?</li>
                    <li><span class="math inline">\(\norm{f(t+h,x) - f(t,x)} = h \cdot g(t,x)\)</span></li>
                    <li><span class="math inline">\(\norm{f(t+h) - f(t)} = h \cdot g(t)\)</span></li>
                    <li>It seems wrong. Abandon this path. See below.</li>
                    </ul></li>
                    <li>We model the input as a function <span class="math inline">\(x : \Real \to \Real^n\)</span>.</li>
                    <li>We model the output as a function <span class="math inline">\(y : \Real \to \Real^n\)</span>.
                    <ul>
                    <li><span class="math inline">\(\norm{y(t+h) - y(t)} = h \cdot g(t)\)</span></li>
                    <li><span class="math inline">\(y(t+h) - y(t) = h \cdot (dy)(t)\)</span></li>
                    <li><span class="math inline">\(\norm{(dy)(t)} = g(t)\)</span>
                    <ul>
                    <li>There are infinitely many <span class="math inline">\(dy\)</span> that satisfies that. Which one should we choose?</li>
                    </ul></li>
                    <li>If <span class="math inline">\(y : \Real \to \Real^n\)</span> then <span class="math inline">\(dy : \Real \to \Real^n\)</span>.</li>
                    </ul></li>
                    <li>A control system snapshot is a function in <span class="math inline">\(\Real^\infty \to \Real^\infty\)</span>.</li>
                    <li>A control system is a function in <span class="math inline">\(\Real \to \Real^\infty \to \Real^\infty\)</span>.</li>
                    <li>How does <span class="math inline">\(F\)</span> have memory if <span class="math inline">\(F(t) = \int_0^t f(x) ~ dx\)</span>?</li>
                    </ul>
                    <h3 id="what-is-ai"><span class="section_number">5.6</span><span class="section_title">What is AI?</span></h3>
                    <ul>
                    <li>In the 1950s, AI was whatever McCarthy et al. were doing.
                    <ul>
                    <li>&quot;McCarthy coined the term 'artificial intelligence' in 1955, and organized the famous Dartmouth Conference in Summer 1956. This conference started AI as a field.&quot; (<a href="https://en.wikipedia.org/wiki/John_McCarthy_(computer_scientist)">WP: John McCarthy (computer scientist)</a>)</li>
                    <li><a href="https://en.wikipedia.org/wiki/Dartmouth_workshop">WP: Dartmouth workshop</a></li>
                    <li><a href="http://raysolomonoff.com/dartmouth/">Ray Solomonoff's Dartmouth archives</a></li>
                    </ul></li>
                    <li>What are AI approaches? How are we trying to make an AI?
                    <ul>
                    <li>Pedro Domingos categorizes AI approaches into five <em>tribes</em>:
                    <ul>
                    <li>symbolists (symbolic logic)</li>
                    <li>connectionists (neural networks)</li>
                    <li>evolutionaries (genetic algorithms)</li>
                    <li>bayesians (statistical learning, probabilistic inference)</li>
                    <li>analogizers (what is this?)</li>
                    </ul></li>
                    </ul></li>
                    </ul>
                    <h3 id="how-do-we-measure-the-performance-of-a-learning-algorithm"><span class="section_number">5.7</span><span class="section_title">How do we measure the performance of a learning algorithm?</span></h3>
                    <h2 id="natural-language-processing"><span class="section_number">6</span><span class="section_title">Natural language processing?</span></h2>
                    <div class="local_table_of_contents">
                    <ul>
                    <li><span class="section_number">6.1</span><span class="section_title"><a href="#conjectures-about-language-and-logic">Conjectures about language and logic</a></span><span class="word_count">(85w~1m)</span></li>
                    <li><span class="section_number">6.2</span><span class="section_title"><a href="#concept-spaces-word-vectors-concept-vectors-bags-of-words">Concept spaces, word vectors, concept vectors, bags of words</a></span><span class="word_count">(44w~1m)</span></li>
                    </ul>
                    </div>
                    <h3 id="conjectures-about-language-and-logic"><span class="section_number">6.1</span><span class="section_title">Conjectures about language and logic</span></h3>
                    <p>Conjectures:</p>
                    <ul>
                    <li>Natural languages are just <em>surface syntaxes</em> for first-order logic.</li>
                    </ul>
                    <p>It is straightforward to write a Prolog program that parses some limited English. It is still practical to write a Prolog program that parses some richer English with named entity recognition. Prolog definite-clause grammars make parsing easy</p>
                    <p>Another problems:</p>
                    <ul>
                    <li>Which information source should the computer trust?</li>
                    <li>How should the computer reconcile conflicting information?</li>
                    </ul>
                    <p>2011 &quot;Natural Language Processing With Prolog in the IBM Watson System&quot; <a href="https://www.cs.nmsu.edu/ALP/2011/03/natural-language-processing-with-prolog-in-the-ibm-watson-system/">https://www.cs.nmsu.edu/ALP/2011/03/natural-language-processing-with-prolog-in-the-ibm-watson-system/</a></p>
                    <p>If IBM Watson is possible, then a personal search assistant should be possible.</p>
                    <h3 id="concept-spaces-word-vectors-concept-vectors-bags-of-words"><span class="section_number">6.2</span><span class="section_title">Concept spaces, word vectors, concept vectors, bags of words</span></h3>
                    <p>Let Car represent the concept of car. Let Red represent the concept of red. Let Modify(Car,Red) represent the concept of red car. Then Modify(X,Red) - Modify(Y,Red) = X - Y.</p>
                    <p>Modify(X,M) - Modify(Y,M) = X - Y.</p>
                    <p>Literature?</p>
                    <h2 id="philosophical-dead-ends"><span class="section_number">7</span><span class="section_title">Philosophical dead-ends?</span></h2>
                    <div class="local_table_of_contents">
                    <ul>
                    <li><span class="section_number">7.1</span><span class="section_title"><a href="#post-ai-ethical-concerns">Post-AI ethical concerns</a></span><span class="word_count">(84w~1m)</span></li>
                    <li><span class="section_number">7.2</span><span class="section_title"><a href="#dropping-the-artificial">Dropping the &quot;artificial&quot;</a></span><span class="word_count">(105w~1m)</span></li>
                    <li><span class="section_number">7.3</span><span class="section_title"><a href="#intelligence-has-nothing-to-do-with-minds">Intelligence has nothing to do with minds</a></span><span class="word_count">(12w~1m)</span></li>
                    <li><span class="section_number">7.4</span><span class="section_title"><a href="#what-is-so-bad-about-human-extinction">What is so bad about human extinction?</a></span><span class="word_count">(34w~1m)</span></li>
                    <li><span class="section_number">7.5</span><span class="section_title"><a href="#guesses">Guesses</a></span><span class="word_count">(23w~1m)</span></li>
                    <li><span class="section_number">7.6</span><span class="section_title"><a href="#non-prioritized-questions">Non-prioritized questions</a></span><span class="word_count">(29w~1m)</span></li>
                    <li><span class="section_number">7.7</span><span class="section_title"><a href="#making-machines-work"><span class="todo TODO">TODO</span> Making machines work</a></span><span class="word_count">(516w~3m)</span></li>
                    <li><span class="section_number">7.8</span><span class="section_title"><a href="#antinatalism-implies-that-creating-a-sentient-machine-is-immoral">Antinatalism implies that creating a sentient machine is immoral</a></span><span class="word_count">(38w~1m)</span></li>
                    <li><span class="section_number">7.9</span><span class="section_title"><a href="#how-do-we-know-that-a-system-has-learned-something">How do we know that a system has learned something?</a></span><span class="word_count">(31w~1m)</span></li>
                    <li><span class="section_number">7.10</span><span class="section_title"><a href="#where-is-the-ai-bottleneck">Where is the AI bottleneck?</a></span><span class="word_count">(92w~1m)</span></li>
                    </ul>
                    </div>
                    <h3 id="post-ai-ethical-concerns"><span class="section_number">7.1</span><span class="section_title">Post-AI ethical concerns</span></h3>
                    <div class="local_table_of_contents">
                    <ul>
                    <li><span class="section_number">7.1.1</span><span class="section_title"><a href="#some-ai-ethics-questions">Some AI ethics questions</a></span><span class="word_count">(56w~1m)</span></li>
                    <li><span class="section_number">7.1.2</span><span class="section_title"><a href="#there-are-only-two-possible-worlds-after-ai">There are only two possible worlds after AI</a></span><span class="word_count">(27w~1m)</span></li>
                    </ul>
                    </div>
                    <h4 id="some-ai-ethics-questions"><span class="section_number">7.1.1</span><span class="section_title">Some AI ethics questions</span></h4>
                    <ul>
                    <li>What is the problem with Asimov's three laws of robotics?</li>
                    <li>Will the rich monopolize AI?</li>
                    <li>What should we do if everything is free? What should we do if we don't have to work to eat?</li>
                    </ul>
                    <p>Who should a machine trust when there is a conflict of belief?</p>
                    <p>Trust is discussed in <a href="social.html">file:social.html</a>. Perhaps it should be refactored.</p>
                    <h4 id="there-are-only-two-possible-worlds-after-ai"><span class="section_number">7.1.2</span><span class="section_title">There are only two possible worlds after AI</span></h4>
                    <p>The optimistic case: Machine does all work. Food is free.</p>
                    <p>The pessimistic case: Some elites use AI to oppress everyone else.</p>
                    <h3 id="dropping-the-artificial"><span class="section_number">7.2</span><span class="section_title">Dropping the &quot;artificial&quot;</span></h3>
                    <p>&quot;Artificial&quot; simply means &quot;man-made&quot;.</p>
                    <p>Being man-made is not a problem in and of itself.</p>
                    <p><em>The problem is that we don't understand the consequences of our actions.</em></p>
                    <p>Why should we care whether something is man-made?</p>
                    <p>A man-made helium atom is indistinguishable from a naturally occurring helium atom.</p>
                    <p>Soylent is man-made. One has survived eating only Soylent for a month. The problem with Soylent is not that it is man-made. The problem is that our jaws may shrink if we don't chew.</p>
                    <p>DDT is man-made. The problem is not that it is man-made. The problem is that it poisons humans. The problem is that we spray it without understanding the consequences.</p>
                    <h3 id="intelligence-has-nothing-to-do-with-minds"><span class="section_number">7.3</span><span class="section_title">Intelligence has nothing to do with minds</span></h3>
                    <p>&quot;Intelligent&quot; simply means &quot;good at something&quot;.</p>
                    <h3 id="what-is-so-bad-about-human-extinction"><span class="section_number">7.4</span><span class="section_title">What is so bad about human extinction?</span></h3>
                    <p>Extinction is not bad in and of itself; it is the suffering that is undesirable. But if we can go extinct without suffering, why shouldn't we go extinct?</p>
                    <h3 id="guesses"><span class="section_number">7.5</span><span class="section_title">Guesses</span></h3>
                    <p>In the future, there are only two kinds of jobs: telling machines to do things, and being told to do things by machines.</p>
                    <h3 id="non-prioritized-questions"><span class="section_number">7.6</span><span class="section_title">Non-prioritized questions</span></h3>
                    <ul>
                    <li>What is AI? Why should I care?
                    <ul>
                    <li>AI is the way for us to become gods.</li>
                    </ul></li>
                    <li>What is a cyborg?</li>
                    <li>If human goal function is survival, then why exists suicide?
                    <ul>
                    <li>Evolutionary noise?</li>
                    </ul></li>
                    </ul>
                    <p><a href="https://en.wikipedia.org/wiki/Universal_Darwinism">https://en.wikipedia.org/wiki/Universal_Darwinism</a></p>
                    <h3 id="making-machines-work"><span class="section_number">7.7</span><span class="section_title"><span class="todo TODO">TODO</span> Making machines work</span></h3>
                    <p>There are several ways to make machines work: program them, train them, or make them learn. Programming and training produce inflexible machines that cannot do things that they are not programmed or trained for.</p>
                    <div class="local_table_of_contents">
                    <ul>
                    <li><span class="section_number">7.7.1</span><span class="section_title"><a href="#delayed-signal-thought-experiment">Delayed signal thought experiment</a></span><span class="word_count">(75w~1m)</span></li>
                    <li><span class="section_number">7.7.2</span><span class="section_title"><a href="#signs-of-intelligence">Signs of intelligence</a></span><span class="word_count">(109w~1m)</span></li>
                    <li><span class="section_number">7.7.3</span><span class="section_title"><a href="#related-concepts">Related concepts</a></span><span class="word_count">(21w~1m)</span></li>
                    <li><span class="section_number">7.7.4</span><span class="section_title"><a href="#interesting-idea">Interesting idea</a></span><span class="word_count">(140w~1m)</span></li>
                    <li><span class="section_number">7.7.5</span><span class="section_title"><a href="#cybernetics">Cybernetics</a></span><span class="word_count">(122w~1m)</span></li>
                    <li><span class="section_number">7.7.6</span><span class="section_title"><a href="#supervised-classification-problems">Supervised classification problems</a></span><span class="word_count">(14w~1m)</span></li>
                    <li><span class="section_number">7.7.7</span><span class="section_title"><a href="#classification-involving-sequence-or-time">Classification involving sequence or time</a></span><span class="word_count">(5w~1m)</span></li>
                    </ul>
                    </div>
                    <h4 id="delayed-signal-thought-experiment"><span class="section_number">7.7.1</span><span class="section_title">Delayed signal thought experiment</span></h4>
                    <p>Imagine that you install something in your brain that delays the signal to your left hand by one hand, so your left hand does what you want it to do, but one second after when you want it to do that. Would you still think your left hand is a part of your self?</p>
                    <p>If a machine does not have any way of sensing touch, even indirectly, then it will never experience touch.</p>
                    <h4 id="signs-of-intelligence"><span class="section_number">7.7.2</span><span class="section_title">Signs of intelligence</span></h4>
                    <p>Imitation and survival?</p>
                    <p>Imitation implies intelligence? For <span class="math inline">\(a\)</span> to be able to imitate <span class="math inline">\(b\)</span>, <span class="math inline">\(a\)</span> has to have a model of <span class="math inline">\(b\)</span>.</p>
                    <p>If the only goal is to survive, then wouldn't the best strategy be make as many copies as many as possible?</p>
                    <p>Make copies, as fast as possible, as many as possible.</p>
                    <p>Arrange for the species to maximize the number of copies that live at the same time.</p>
                    <p>Make an organism as fit as possible. Make an organism survives as many environments as possible, including the environments it did not originally evolve from. A sign of intelligence is that the organism can perform well in environments it had never encountered before.</p>
                    <h4 id="related-concepts"><span class="section_number">7.7.3</span><span class="section_title">Related concepts</span></h4>
                    <p>intelligence, learning, self, consciousness, sentience, life, perception, adaptivity, adaptability, adaptation, control, language, thought, feeling, reasoning, discovery, recursion, feedback, computation, computability.</p>
                    <h4 id="interesting-idea"><span class="section_number">7.7.4</span><span class="section_title">Interesting idea</span></h4>
                    <p>Strategy 1: Given two nouns <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, find a verb <span class="math inline">\(v\)</span> such that the sentence <span class="math inline">\(a~v~b\)</span> makes sense. Strategy 2: Given two nouns <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, pick which of these two sentences make sense: &quot;<span class="math inline">\(a\)</span> requires <span class="math inline">\(b\)</span>,&quot; or &quot;<span class="math inline">\(a\)</span> does not require <span class="math inline">\(b\)</span>.&quot;</p>
                    <p>An early 'intelligence' is chemotaxis. Chemotaxis is random walk that is biased by the gradient. (Cite?) The deterministic version of that is gradient following algorithm. The goal is to minimize the concentration of the chemical at the location of the cell.</p>
                    <p>Control system. Homeostasis.</p>
                    <p>Deduction: Given premises, infer conclusion. Induction: Given a few premises and a conclusion, infer a rule.</p>
                    <p>Probabilistic logic. Generalize boolean <span class="math inline">\(\{0,1\}\)</span> to probability, real unit interval, <span class="math inline">\([0,1]\)</span>. Boolean logic is a special case of probabilistic logic. <span class="math inline">\(p~(x \wedge y) = \min~(p~x)~(p~y)\)</span>. Fuzzy logic?</p>
                    <p>&quot;To organize is to create capabilities by intentionally imposing order and structure.&quot; <span class="citation" data-cites="Organ">[<a href="#ref-Organ">10</a>]</span></p>
                    <h4 id="cybernetics"><span class="section_number">7.7.5</span><span class="section_title">Cybernetics</span></h4>
                    <p>How can we apply systems theory to management? <span class="citation" data-cites="SystemManage">[<a href="#ref-SystemManage">19</a>]</span></p>
                    <p>Ashby's optical mobile homeostat <span class="citation" data-cites="BattleHom">[<a href="#ref-BattleHom">3</a>]</span> <span class="citation" data-cites="BattleThree">[<a href="#ref-BattleThree">2</a>]</span></p>
                    <p>Braintenberg vehicles</p>
                    <p>A Gödel machine improves itself. It proves that the improvement it makes indeed makes it better. <span class="citation" data-cites="GodelMachImpl">[<a href="#ref-GodelMachImpl">32</a>]</span></p>
                    <p><a href="http://people.idsia.ch/~juergen/goedelmachine.html">http://people.idsia.ch/~juergen/goedelmachine.html</a></p>
                    <p><a href="http://people.idsia.ch/~juergen/selfreflection.pdf">http://people.idsia.ch/~juergen/selfreflection.pdf</a></p>
                    <p><a href="http://people.idsia.ch/~juergen/metalearner.html">http://people.idsia.ch/~juergen/metalearner.html</a></p>
                    <p>Steinberg and Salter (1982) wrote that intelligence is &quot;goal-directed adaptive behavior&quot;. This suggests that an intelligent system is purposeful and adaptive, in the sense we defined above. <a href="https://en.wikipedia.org/wiki/Intelligence#Definitions">https://en.wikipedia.org/wiki/Intelligence#Definitions</a></p>
                    <p>Intelligence maximizes future freedom? <a href="https://www.ted.com/talks/alex_wissner_gross_a_new_equation_for_intelligence/transcript?language=en#t-121478">https://www.ted.com/talks/alex_wissner_gross_a_new_equation_for_intelligence/transcript?language=en#t-121478</a></p>
                    <p><span class="citation" data-cites="PickeringCyber">[<a href="#ref-PickeringCyber">24</a>]</span> <span class="citation" data-cites="GoertzelAgi">[<a href="#ref-GoertzelAgi">11</a>]</span> <span class="citation" data-cites="SlomanTuringIrrelevance">[<a href="#ref-SlomanTuringIrrelevance">27</a>]</span></p>
                    <p>Giulio Tononi, integrated information theory (not to be confused with information integration theory)</p>
                    <p>Nils J. Nilsson modeled a world and an agent as finite-state machines <span class="citation" data-cites="NilsLogicAi">[<a href="#ref-NilsLogicAi">23</a>]</span>. He used explicit sense type, action type, and memory type. William Ross Ashby used the phase space of a continuous dynamical system, where time is a real number, to describe an agent's behavior <span class="citation" data-cites="AshbyBrain">[<a href="#ref-AshbyBrain">1</a>]</span>.</p>
                    <h4 id="supervised-classification-problems"><span class="section_number">7.7.6</span><span class="section_title">Supervised classification problems</span></h4>
                    <p>AI shines in supervised classification problems. Machine vision.</p>
                    <p>Digit recognition is classification problem.</p>
                    <h4 id="classification-involving-sequence-or-time"><span class="section_number">7.7.7</span><span class="section_title">Classification involving sequence or time</span></h4>
                    <h3 id="antinatalism-implies-that-creating-a-sentient-machine-is-immoral"><span class="section_number">7.8</span><span class="section_title">Antinatalism implies that creating a sentient machine is immoral</span></h3>
                    <p>It is immoral to force a sentient being to exist.</p>
                    <p>Humans are smart enough to arrive at antinatalism, but they still fuck and have babies. Nobody seems to give a fuck.</p>
                    <h3 id="how-do-we-know-that-a-system-has-learned-something"><span class="section_number">7.9</span><span class="section_title">How do we know that a system has learned something?</span></h3>
                    <p>The only way to know whether the system has learning something is by testing it with samples the system has never seen?</p>
                    <h3 id="where-is-the-ai-bottleneck"><span class="section_number">7.10</span><span class="section_title">Where is the AI bottleneck?</span></h3>
                    <p>What is preventing us from creating the AI?</p>
                    <p>There are inconclusive discussions<a href="#fn26" class="footnote-ref" id="fnref26"><sup>26</sup></a>.</p>
                    <p>Where is the bottleneck: philosophy, science, or engineering?</p>
                    <p>Is our hardware not powerful enough?</p>
                    <p>Is our software not efficient enough?</p>
                    <p>Is our knowledge not enough? Are we clueless?</p>
                    <p>Are we doing the wrong experiments?</p>
                    <p>Reading: 2012, &quot;Philosophy will be the key that unlocks artificial intelligence&quot;, David Deutsch, in The Guardian<a href="#fn27" class="footnote-ref" id="fnref27"><sup>27</sup></a>. That is an abridged version of the 2012 article &quot;Creative blocks&quot; in Aeon magazine<a href="#fn28" class="footnote-ref" id="fnref28"><sup>28</sup></a>.</p>
                    <p>Why has AI mastered chess, but not real life? Because chess search space is much smaller than real-life search space?</p>
                    <h2 id="more-math"><span class="section_number">8</span><span class="section_title">More math?</span></h2>
                    <div class="local_table_of_contents">
                    <ul>
                    <li><span class="section_number">8.1</span><span class="section_title"><a href="#automatic-differentiation">Automatic differentiation?</a></span><span class="word_count">(11w~1m)</span></li>
                    <li><span class="section_number">8.2</span><span class="section_title"><a href="#adversarial-learning">Adversarial learning</a></span><span class="word_count">(15w~1m)</span></li>
                    <li><span class="section_number">8.3</span><span class="section_title"><a href="#habituation">Habituation</a></span><span class="word_count">(53w~1m)</span></li>
                    <li><span class="section_number">8.4</span><span class="section_title"><a href="#human-as-a-feedback-system">Human as a feedback system?</a></span><span class="word_count">(414w~3m)</span></li>
                    <li><span class="section_number">8.5</span><span class="section_title"><a href="#agent-models">Agent models?</a></span><span class="word_count">(731w~4m)</span></li>
                    <li><span class="section_number">8.6</span><span class="section_title"><a href="#aiml-taxonomy">AI/ML taxonomy?</a></span><span class="word_count">(227w~2m)</span></li>
                    <li><span class="section_number">8.7</span><span class="section_title"><a href="#clean-up-system.org"><span class="todo TODO">TODO</span> clean up system.org</a></span><span class="word_count">(1308w~7m)</span></li>
                    </ul>
                    </div>
                    <h3 id="automatic-differentiation"><span class="section_number">8.1</span><span class="section_title">Automatic differentiation?</span></h3>
                    <p>Justin Le, <a href="https://blog.jle.im/entry/purely-functional-typed-models-1.html">A Purely Functional Typed Approach to Trainable Models</a></p>
                    <h3 id="adversarial-learning"><span class="section_number">8.2</span><span class="section_title">Adversarial learning</span></h3>
                    <ul>
                    <li>How do we learn amid lies, deception, disinformation, misinformation?
                    <ul>
                    <li>Related to adversarial learning? <a href="https://en.wikipedia.org/wiki/Adversarial_machine_learning">https://en.wikipedia.org/wiki/Adversarial_machine_learning</a> ?</li>
                    </ul></li>
                    </ul>
                    <h3 id="habituation"><span class="section_number">8.3</span><span class="section_title">Habituation</span></h3>
                    <ul>
                    <li>TODO s/adapt/habituate</li>
                    <li>Let <span class="math inline">\(f(t,x)\)</span> be the system's response intensity for stimulus intensity <span class="math inline">\(x\)</span> at time <span class="math inline">\(t\)</span>. We say the system is <em>habituating</em> between the time <span class="math inline">\(t_1\)</span> and <span class="math inline">\(t_2\)</span> iff <span class="math inline">\(f(t_1,x) &gt; f(t_2,x)\)</span> for all stimulus intensity <span class="math inline">\(x\)</span>.</li>
                    <li>&quot;The habituation process is a form of adaptive behavior (or neuroplasticity) that is classified as non-associative learning.&quot; <a href="https://en.wikipedia.org/wiki/Habituation">https://en.wikipedia.org/wiki/Habituation</a></li>
                    </ul>
                    <h3 id="human-as-a-feedback-system"><span class="section_number">8.4</span><span class="section_title">Human as a feedback system?</span></h3>
                    <div class="local_table_of_contents">
                    <ul>
                    <li><span class="section_number">8.4.1</span><span class="section_title"><a href="#human-behavior-as-a-special-case-of-the-general-feedback-equation">Human behavior as a special case of the general feedback equation</a></span><span class="word_count">(97w~1m)</span></li>
                    <li><span class="section_number">8.4.2</span><span class="section_title"><a href="#hardwiring-the-concept-of-time">Hardwiring the concept of time</a></span><span class="word_count">(20w~1m)</span></li>
                    <li><span class="section_number">8.4.3</span><span class="section_title"><a href="#a-brain-at-a-given-time-is-an-array-function.">A brain at a given time is an array function.</a></span><span class="word_count">(62w~1m)</span></li>
                    <li><span class="section_number">8.4.4</span><span class="section_title"><a href="#an-array-iself-is-also-a-function.">An array iself is also a function.</a></span><span class="word_count">(36w~1m)</span></li>
                    <li><span class="section_number">8.4.5</span><span class="section_title"><a href="#each-brain-has-a-maximand.">Each brain has a maximand.</a></span><span class="word_count">(31w~1m)</span></li>
                    <li><span class="section_number">8.4.6</span><span class="section_title"><a href="#consider-functions-of-length-one-arrays.">Consider functions of length-one arrays.</a></span><span class="word_count">(12w~1m)</span></li>
                    <li><span class="section_number">8.4.7</span><span class="section_title"><a href="#how-do-we-relate-vector-functions-and-intelligence">How do we relate vector functions and intelligence?</a></span><span class="word_count">(8w~1m)</span></li>
                    <li><span class="section_number">8.4.8</span><span class="section_title"><a href="#how-does-feedback-happen-in-the-brain">How does feedback happen in the brain?</a></span><span class="word_count">(49w~1m)</span></li>
                    <li><span class="section_number">8.4.9</span><span class="section_title"><a href="#the-brain-is-a-recurrence-relation.">The brain is a recurrence relation.</a></span><span class="word_count">(71w~1m)</span></li>
                    <li><span class="section_number">8.4.10</span><span class="section_title"><a href="#the-brain-evolved-from-simpler-nervous-systems.">The brain evolved from simpler nervous systems.</a></span><span class="word_count">(33w~1m)</span></li>
                    </ul>
                    </div>
                    <h4 id="human-behavior-as-a-special-case-of-the-general-feedback-equation"><span class="section_number">8.4.1</span><span class="section_title">Human behavior as a special case of the general feedback equation</span></h4>
                    <p>Let <span class="math inline">\(x ~ t\)</span> be the input vector at time <span class="math inline">\(t\)</span>; this vector has at least some billions of elements. The function <span class="math inline">\(x\)</span> represents the state of all sensors at a given time.</p>
                    <p>Let <span class="math inline">\(y ~ t\)</span> be the control vector at time <span class="math inline">\(t\)</span>; this vector is also big.</p>
                    <p>Let <span class="math inline">\(z~t\)</span> be the output vector at time <span class="math inline">\(t\)</span>.</p>
                    <p>The environment feeds back a part of the output to the input. Can the agent determine the response function?</p>
                    <p>The feedback forms memory, but see &quot;Memory without feedback in a neural network&quot;. <a href="https://www.ncbi.nlm.nih.gov/pubmed/19249281">https://www.ncbi.nlm.nih.gov/pubmed/19249281</a></p>
                    <h4 id="hardwiring-the-concept-of-time"><span class="section_number">8.4.2</span><span class="section_title">Hardwiring the concept of time</span></h4>
                    <p>We can transform a non-temporal behavior <span class="math inline">\(f~x = y\)</span> into a temporal behavior <span class="math inline">\(f&#39;~t = y&#39;\)</span>?</p>
                    <h4 id="a-brain-at-a-given-time-is-an-array-function."><span class="section_number">8.4.3</span><span class="section_title">A brain at a given time is an array function.</span></h4>
                    <p>A brain at a given time is an array function having type <span class="math inline">\(\Real^\infty \to \Real^\infty\)</span>. Each component of the input array is a signal from a sensor. Each component of the output array goes to an actuator.</p>
                    <p>Since the brain is finite, there must be infinitely many zeros in the input and output arrays.</p>
                    <h4 id="an-array-iself-is-also-a-function."><span class="section_number">8.4.4</span><span class="section_title">An array iself is also a function.</span></h4>
                    <p>An <span class="math inline">\(E\)</span>-array is a function having type <span class="math inline">\(\Nat \to E\)</span>. The input is an index. The output is the value of the component at that index. Subscripting denotes function application.</p>
                    <h4 id="each-brain-has-a-maximand."><span class="section_number">8.4.5</span><span class="section_title">Each brain has a maximand.</span></h4>
                    <p>Such maximand is a hidden function. The brain always tries to maximize the maximand.</p>
                    <p>A differential change in brain tries to increase the maximand. The brain follows gradient.</p>
                    <h4 id="consider-functions-of-length-one-arrays."><span class="section_number">8.4.6</span><span class="section_title">Consider functions of length-one arrays.</span></h4>
                    <p>Let <span class="math inline">\(h\)</span> be a differential change in brain.</p>
                    <h4 id="how-do-we-relate-vector-functions-and-intelligence"><span class="section_number">8.4.7</span><span class="section_title">How do we relate vector functions and intelligence?</span></h4>
                    <h4 id="how-does-feedback-happen-in-the-brain"><span class="section_number">8.4.8</span><span class="section_title">How does feedback happen in the brain?</span></h4>
                    <p>Feedback is due to environment and the physical laws. When we move our hand, we see it, because the light reflected by our hand now reaches our eyes.</p>
                    <p>The next input depends on the previous input. <span class="math display">\[\begin{aligned}
                        y_k &amp;= b~x_k
                        \\
                        x_{k+1} &amp;= f~x_k~y_k\end{aligned}\]</span></p>
                    <h4 id="the-brain-is-a-recurrence-relation."><span class="section_number">8.4.9</span><span class="section_title">The brain is a recurrence relation.</span></h4>
                    <p>This pictures the brain as a parallel dataflow computer with clock period of a few microseconds.</p>
                    <p><a href="https://en.wikipedia.org/wiki/Dataflow_architecture">https://en.wikipedia.org/wiki/Dataflow_architecture</a></p>
                    <p>Let <span class="math inline">\(m\)</span> be memory, <span class="math inline">\(x\)</span> be senses, and <span class="math inline">\(y\)</span> be actuators. <span class="math display">\[\begin{aligned}
                        m_{t+1} &amp;= f~x_t~m_t
                        \\
                        y_{t+1} &amp;= g~x_t~m_t\end{aligned}\]</span></p>
                    <p>There is also a version with implicit time. <span class="math display">\[\begin{aligned}
                        m&#39; &amp;= f~x~m
                        \\
                        y&#39; &amp;= g~x~m\end{aligned}\]</span></p>
                    <p>There is also a continuous version. <span class="math display">\[\begin{aligned}
                        m_{t+h} &amp;= h \cdot f~x_t~m_t
                        \\
                        y_{t+h} &amp;= h \cdot g~x_t~m_t\end{aligned}\]</span></p>
                    <h4 id="the-brain-evolved-from-simpler-nervous-systems."><span class="section_number">8.4.10</span><span class="section_title">The brain evolved from simpler nervous systems.</span></h4>
                    <p>Nervous systems are control systems.</p>
                    <p>Nervous systems must have provided some evolutionary benefit; otherwise natural selection would have phased them out.</p>
                    <p>Bacterial chemotaxis detects chemical concentration difference.</p>
                    <p>Nematode. Caenorhabditis elegans.</p>
                    <h3 id="agent-models"><span class="section_number">8.5</span><span class="section_title">Agent models?</span></h3>
                    <div class="local_table_of_contents">
                    <ul>
                    <li><span class="section_number">8.5.1</span><span class="section_title"><a href="#explicit-world-mono-unary-algebra">Explicit-world mono-unary algebra</a></span><span class="word_count">(507w~3m)</span></li>
                    <li><span class="section_number">8.5.2</span><span class="section_title"><a href="#limitations">Limitations</a></span><span class="word_count">(34w~1m)</span></li>
                    <li><span class="section_number">8.5.3</span><span class="section_title"><a href="#implicit-world-discrete-dynamical-system-model">Implicit-world discrete dynamical system model</a></span><span class="word_count">(74w~1m)</span></li>
                    <li><span class="section_number">8.5.4</span><span class="section_title"><a href="#dynamical-systems">Dynamical systems</a></span><span class="word_count">(2w~1m)</span></li>
                    <li><span class="section_number">8.5.5</span><span class="section_title"><a href="#measuring-the-intelligence-of-a-phase-space-trajectory">Measuring the intelligence of a phase space trajectory</a></span><span class="word_count">(39w~1m)</span></li>
                    <li><span class="section_number">8.5.6</span><span class="section_title"><a href="#discrete-agent-model">Discrete agent model</a></span><span class="word_count">(79w~1m)</span></li>
                    </ul>
                    </div>
                    <h4 id="explicit-world-mono-unary-algebra"><span class="section_number">8.5.1</span><span class="section_title">Explicit-world mono-unary algebra</span></h4>
                    <p>A <em>world</em> <span class="math inline">\(W\)</span> is a mono-unary algebra <span class="math inline">\((\fun{Sta}~W, ~ \fun{law}~W)\)</span> where <span class="math inline">\(\fun{Sta}~W\)</span> is the <em>world state type</em> and <span class="math inline">\(\fun{law}~W : \fun{Sta}~W \to \fun{Sta}~W\)</span> is the <em>world law</em>.</p>
                    <p>An <em>agent</em> <span class="math inline">\(A\)</span> is a mono-unary algebra <span class="math inline">\((\fun{Sta}~A, ~ \fun{law}~A)\)</span> where <span class="math inline">\(\fun{Sta}~A\)</span> is the <em>agent state type</em> and <span class="math inline">\(\fun{law}~A : \fun{Sta}~A \to \fun{Sta}~A\)</span> is the <em>agent law</em>.</p>
                    <p>The <em>environment</em> is the world minus the agent. Something is a part of the agent if and only if the agent can directly control that part. Otherwise it is a part of the environment.</p>
                    <p>The function <span class="math inline">\(\fun{sense}: \fun{Sta}~W \to \fun{Sta}~A\)</span> defines how the agent perceives the world. This function is not invertible because <span class="math inline">\(\fun{Sta}~W \supset \fun{Sta}~A\)</span>. This means that <em>there exists part of the world that the agent cannot sense.</em></p>
                    <p>An agent exists in a world. The <span class="math inline">\(\fun{sense}\)</span> function must be a homomorphism from the world to the agent. The <span class="math inline">\(\fun{sense}\)</span> function must satisfy this equation: <span class="math display">\[\fun{sense}\circ \fun{law}~W = \fun{law}~A \circ \fun{sense}.\]</span></p>
                    <p>That equation relates the actual world law and what the law looks like from the agent's point of view. The agent can never know the world law. The agent can only discover something homomorphic to that law. That means <em>we can never know the laws of nature</em>. We will never know the reality. We can only know something homomorphic to the laws of nature.</p>
                    <p>Starting from a state <span class="math inline">\(x\)</span>, the agent forms the sequence <span class="math inline">\(\fun{orbit}~A~x = \fun{iterate}~(\fun{law}~A)~x = (x, ~ f~x, ~ f^2~x, \ldots)\)</span> where <span class="math inline">\(f = \fun{law}~A\)</span>. We define such <span class="math inline">\(\fun{orbit}~A~x\)</span> to be a member of the type <span class="math inline">\(\fun{Orbit}~A\)</span>. We use the notation <span class="math inline">\(\fun{InfSeq}~A\)</span> to mean the space of infinite sequences where each element has type <span class="math inline">\(A\)</span>. We have <span class="math inline">\(\fun{Orbit}~A \subset \fun{InfSeq}~A\)</span>. We consider the type <span class="math inline">\(\fun{Orbit}~A = \{ \fun{orbit}~A~x ~|~ x : \fun{Sta}~A \}\)</span>, the set of all orbits of <span class="math inline">\(A\)</span>. We have a judge function that judges an orbit. This function is <span class="math inline">\(\fun{judge}: \fun{Orbit}~A \to \Real\)</span>.</p>
                    <p>Now we assume that every <span class="math inline">\(x : \fun{Sta}~A\)</span> is distributed uniformly. Define <span class="math inline">\(p~r\)</span> as the probability of finding an <span class="math inline">\(x\)</span> where <span class="math inline">\(\fun{judge}~x \le r\)</span>. The shape of the distribution <span class="math inline">\(p\)</span> describes the intelligence of the agent.</p>
                    <p>The function <span class="math inline">\(\fun{penalty}: \fun{Sta}~A \to \Real\)</span> defines the undesirability of an agent state. Alternatively, the function <span class="math inline">\(\fun{reward}: \fun{Sta}~A \to \Real\)</span> defines the desirability of an agent state. The function measures how bad or how good the agent performs. This is the agent's hidden objective function. This is hardwired. This is arbitrary. The agent doesn't have to be aware of this. An intelligent agent acts to make its <span class="math inline">\(\fun{penalty}~x\)</span> as close to zero as possible in the long term for as many <span class="math inline">\(x\)</span> as possible, where <span class="math inline">\(x\)</span> is an agent state.</p>
                    <p>The agent displays an intelligent behavior if it can minimize the long-term penalty from lots of starting states. The most intelligent agent is the one that minimizes its lifelong sum of penalty?</p>
                    <p>An agent has input and output.</p>
                    <p>An <em>agent logic</em> is a function of type <span class="math inline">\((M,I) \to (M,O)\)</span> where <span class="math inline">\(M\)</span> is the memory type, <span class="math inline">\(I\)</span> is the input type, and <span class="math inline">\(O\)</span> is the output type. We assume that the world remembers the agent memory.</p>
                    <h4 id="limitations"><span class="section_number">8.5.2</span><span class="section_title">Limitations</span></h4>
                    <p>The agent is not omniscient. The agent does not know everything. The agent can only perceive a small part of the world. The agent has physical limitations. The agent cannot know the whole world.</p>
                    <h4 id="implicit-world-discrete-dynamical-system-model"><span class="section_number">8.5.3</span><span class="section_title">Implicit-world discrete dynamical system model</span></h4>
                    <p>Let <span class="math inline">\(w\)</span> be a world. Let <span class="math inline">\(a\)</span> be an agent in world <span class="math inline">\(w\)</span>. Let <span class="math inline">\(x~t\)</span> be the input of the agent at time <span class="math inline">\(t\)</span>. Let <span class="math inline">\(y~t\)</span> be the output of the agent at time <span class="math inline">\(t\)</span>. Let <span class="math inline">\(m~t\)</span> be the memory of the agent at time <span class="math inline">\(t\)</span>.</p>
                    <p>We assume that the agent needs one time step to compute the output. <span class="math display">\[\begin{aligned}
                        y~(t+1) &amp;= Y~(x~t)~(m~t)~t
                        \\
                        m~(t+1) &amp;= M~(x~t)~(m~t)~t
                        \\
                        x~(t+1) &amp;= X~(x~t)~(y~t)~t\end{aligned}\]</span></p>
                    <h4 id="dynamical-systems"><span class="section_number">8.5.4</span><span class="section_title">Dynamical systems</span></h4>
                    <h4 id="measuring-the-intelligence-of-a-phase-space-trajectory"><span class="section_number">8.5.5</span><span class="section_title">Measuring the intelligence of a phase space trajectory</span></h4>
                    <p>We can think of a human as a dynamical system.</p>
                    <p>Given two phase space trajectories, which is more intelligent? Why? The most intelligent is the most homeostatic, the most stabilizing, the most controlling.</p>
                    <h4 id="discrete-agent-model"><span class="section_number">8.5.6</span><span class="section_title">Discrete agent model</span></h4>
                    <p>Let <span class="math inline">\(w\)</span> be the world law, and <span class="math inline">\(s\)</span> be short-term goal function. At time <span class="math inline">\(t\)</span>, let <span class="math inline">\(x_t\)</span> be agent input, <span class="math inline">\(y_t\)</span> be agent output, <span class="math inline">\(a_t\)</span> be the agent logic, Assume that these equations hold:</p>
                    <p><span class="math display">\[\begin{aligned}
                    y_t &amp;= a_t(x_t)
                    \\ x_{t+1} &amp;= w(x_t,y_t)
                    \\ g_t &amp;= s(x_t).\end{aligned}\]</span></p>
                    <p>Rearranging and simplifying gives:</p>
                    <p><span class="math display">\[\begin{aligned}
                    \\ g_{t+1} &amp;= s(x_{t+1})
                    \\ &amp;= s(w(x_t,y_t))
                    \\ &amp;= s(w(x_t,a_t(x_t)))\end{aligned}\]</span></p>
                    <p>The goal function takes <span class="math inline">\(e_N\)</span> and outputs a number, where <span class="math inline">\(N\)</span> is a constant.</p>
                    <p>The agent wants to maximize <span class="math inline">\(\sum_{t=0}^\infty g_t\)</span>.</p>
                    <h3 id="aiml-taxonomy"><span class="section_number">8.6</span><span class="section_title">AI/ML taxonomy?</span></h3>
                    <p>What should the categories be?</p>
                    <p>Artificial intelligence is constrained optimization. Brain minimizes free energy <span class="citation" data-cites="friston2006free friston2010free">[<a href="#ref-friston2010free">7</a>, <a href="#ref-friston2006free">8</a>]</span>.</p>
                    <p>Generate vs discriminative.</p>
                    <p>Type type of an <em>expert system</em> is <span class="math inline">\(Facts \to Query \to Answer\)</span>. Decision tree. Linearized decision tree.</p>
                    <p>A learning algorithm is <em>stable</em> iff its generalization error is bounded.</p>
                    <div class="local_table_of_contents">
                    <ul>
                    <li><span class="section_number">8.6.1</span><span class="section_title"><a href="#hyperplane-classifier">Hyperplane classifier</a></span><span class="word_count">(45w~1m)</span></li>
                    <li><span class="section_number">8.6.2</span><span class="section_title"><a href="#support-vector-machine">Support vector machine</a></span><span class="word_count">(121w~1m)</span></li>
                    <li><span class="section_number">8.6.3</span><span class="section_title"><a href="#humes-problem-of-induction">Hume's problem of induction</a></span><span class="word_count">(7w~1m)</span></li>
                    <li><span class="section_number">8.6.4</span><span class="section_title"><a href="#statistics">Statistics?</a></span><span class="word_count">(14w~1m)</span></li>
                    </ul>
                    </div>
                    <h4 id="hyperplane-classifier"><span class="section_number">8.6.1</span><span class="section_title">Hyperplane classifier</span></h4>
                    <p>Let <span class="math inline">\(h\)</span> be a hyperplane. Define <span class="math inline">\(m : \Real^\infty \to \{0,1\}\)</span>, the <em>hard linear binary classifier</em> of <span class="math inline">\(h\)</span>, as <span class="math inline">\(m~x = [h~x \ge 0]\)</span> where <span class="math inline">\([x]\)</span> is 1 iff <span class="math inline">\(x\)</span> is true or 0 iff <span class="math inline">\(x\)</span> is false. Soft classifier: define <span class="math inline">\(m~x = \tanh^{-1}~(h~x)\)</span>.</p>
                    <h4 id="support-vector-machine"><span class="section_number">8.6.2</span><span class="section_title">Support vector machine</span></h4>
                    <p>A training point <span class="math inline">\(x\)</span> is a support of <span class="math inline">\(h\)</span> iff it is the closest point to <span class="math inline">\(h\)</span> among all points in the class of <span class="math inline">\(x\)</span>.</p>
                    <p>Alternative formulation: An upper level is a hyperplane <span class="math inline">\(h_u\)</span> such that <span class="math inline">\(\forall a \in U : h_u~a &gt; 0\)</span>. A lower level is a hyperplane <span class="math inline">\(h_l\)</span> such that <span class="math inline">\(\forall b \in L : h_l~b &lt; 0\)</span>. Let <span class="math inline">\(h_u\)</span> and <span class="math inline">\(h_l\)</span> be parallel. Maximize the distance between <span class="math inline">\(h_u\)</span> and <span class="math inline">\(h_l\)</span>. Then <span class="math inline">\(h_u\)</span> is the upper margin and <span class="math inline">\(h_l\)</span> is the lower margin. Define <span class="math inline">\(h\)</span> as the hyperplane exactly between <span class="math inline">\(h_u\)</span> and <span class="math inline">\(h_l\)</span>.</p>
                    <p>Define <span class="math inline">\(m : \Real^\infty \to \{0,1\}\)</span>, the <em>support vector machine</em> (SVM) of <span class="math inline">\(h\)</span>, as <span class="math inline">\(m~x = [h~x \ge 0]\)</span>. Such SVM is a binary classifier.</p>
                    <h4 id="humes-problem-of-induction"><span class="section_number">8.6.3</span><span class="section_title">Hume's problem of induction</span></h4>
                    <p>Justification for induction? <a href="https://en.wikipedia.org/wiki/Rule_of_succession">https://en.wikipedia.org/wiki/Rule_of_succession</a></p>
                    <h4 id="statistics"><span class="section_number">8.6.4</span><span class="section_title">Statistics?</span></h4>
                    <p>Correlation hints causation.</p>
                    <p>Mathematical-Statistical Learning Theory <a href="https://ocw.mit.edu/courses/mathematics/18-657-mathematics-of-machine-learning-fall-2015/">https://ocw.mit.edu/courses/mathematics/18-657-mathematics-of-machine-learning-fall-2015/</a></p>
                    <p><a href="https://ocw.mit.edu/courses/mathematics/18-655-mathematical-statistics-spring-2016/">https://ocw.mit.edu/courses/mathematics/18-655-mathematical-statistics-spring-2016/</a></p>
                    <p>Convex Optimization, Boyd &amp; Vandenberghe <a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf</a></p>
                    <p>CMU Statistics</p>
                    <p><a href="http://www.stat.cmu.edu/~siva/700/main.html">http://www.stat.cmu.edu/~siva/700/main.html</a></p>
                    <p><a href="http://www.stat.cmu.edu/~larry/=stat705/">http://www.stat.cmu.edu/~larry/=stat705/</a></p>
                    <p><a href="http://www.stat.cmu.edu/~larry/=sml/">http://www.stat.cmu.edu/~larry/=sml/</a></p>
                    <p><a href="http://www.cs.cmu.edu/~10702/">http://www.cs.cmu.edu/~10702/</a></p>
                    <p><a href="https://www.stat.berkeley.edu/~statlearning/publications/index.html">https://www.stat.berkeley.edu/~statlearning/publications/index.html</a></p>
                    <p><a href="https://github.com/bblais/Statistical-Inference-for-Everyone">https://github.com/bblais/Statistical-Inference-for-Everyone</a></p>
                    <p><a href="https://en.wikipedia.org/wiki/Checking_whether_a_coin_is_fair">https://en.wikipedia.org/wiki/Checking_whether_a_coin_is_fair</a></p>
                    <p>Bayesian Updating <a href="http://statweb.stanford.edu/~serban/116/bayes.pdf">http://statweb.stanford.edu/~serban/116/bayes.pdf</a></p>
                    <p><a href="https://en.wikipedia.org/wiki/Wikipedia:WikiProject_Mathematics">https://en.wikipedia.org/wiki/Wikipedia:WikiProject_Mathematics</a></p>
                    <h3 id="clean-up-system.org"><span class="section_number">8.7</span><span class="section_title"><span class="todo TODO">TODO</span> clean up system.org</span></h3>
                    <p>This chapter defines <em>system</em>. Later chapters discuss interesting systems. We classify systems, hoping to gain some insight. We can classify systems into two big classes: <em>time-dependent</em> (time-variant, temporal) and <em>time-independent</em> (time-invariant, atemporal).</p>
                    <div class="local_table_of_contents">
                    <ul>
                    <li><span class="section_number">8.7.1</span><span class="section_title"><a href="#what-is-a-system">What is a system?</a></span><span class="word_count">(138w~1m)</span></li>
                    <li><span class="section_number">8.7.2</span><span class="section_title"><a href="#ignoring-degenerate-feedback-feedforward">Ignoring degenerate feedback: feedforward</a></span><span class="word_count">(53w~1m)</span></li>
                    <li><span class="section_number">8.7.3</span><span class="section_title"><a href="#finding-feedback-the-inverse-fixed-point-problem">Finding feedback: the inverse fixed point problem</a></span><span class="word_count">(161w~1m)</span></li>
                    <li><span class="section_number">8.7.4</span><span class="section_title"><a href="#feedback-based-on-differentiability-preserving-map">Feedback based on differentiability-preserving map</a></span><span class="word_count">(119w~1m)</span></li>
                    <li><span class="section_number">8.7.5</span><span class="section_title"><a href="#measuring-feedback">Measuring feedback</a></span><span class="word_count">(66w~1m)</span></li>
                    <li><span class="section_number">8.7.6</span><span class="section_title"><a href="#linear-feedback-and-function-classes">Linear feedback and function classes</a></span><span class="word_count">(30w~1m)</span></li>
                    <li><span class="section_number">8.7.7</span><span class="section_title"><a href="#temporal-systems">Temporal systems</a></span><span class="word_count">(93w~1m)</span></li>
                    <li><span class="section_number">8.7.8</span><span class="section_title"><a href="#first-order-system">First-order system</a></span><span class="word_count">(67w~1m)</span></li>
                    <li><span class="section_number">8.7.9</span><span class="section_title"><a href="#chaining-temporal-systems">Chaining temporal systems</a></span><span class="word_count">(51w~1m)</span></li>
                    <li><span class="section_number">8.7.10</span><span class="section_title"><a href="#stateless-and-stateful-systems">Stateless and stateful systems</a></span><span class="word_count">(77w~1m)</span></li>
                    <li><span class="section_number">8.7.11</span><span class="section_title"><a href="#property">Property</a></span><span class="word_count">(23w~1m)</span></li>
                    <li><span class="section_number">8.7.12</span><span class="section_title"><a href="#constraint">Constraint</a></span><span class="word_count">(13w~1m)</span></li>
                    <li><span class="section_number">8.7.13</span><span class="section_title"><a href="#parameterfamily">Parameter/family</a></span><span class="word_count">(10w~1m)</span></li>
                    <li><span class="section_number">8.7.14</span><span class="section_title"><a href="#measure">Measure</a></span><span class="word_count">(21w~1m)</span></li>
                    <li><span class="section_number">8.7.15</span><span class="section_title"><a href="#temporal-measure">Temporal measure</a></span><span class="word_count">(15w~1m)</span></li>
                    <li><span class="section_number">8.7.16</span><span class="section_title"><a href="#system-space">System space</a></span><span class="word_count">(6w~1m)</span></li>
                    <li><span class="section_number">8.7.17</span><span class="section_title"><a href="#system-endofunction">System endofunction</a></span><span class="word_count">(4w~1m)</span></li>
                    <li><span class="section_number">8.7.18</span><span class="section_title"><a href="#output-input-gradient">Output-input gradient</a></span><span class="word_count">(25w~1m)</span></li>
                    <li><span class="section_number">8.7.19</span><span class="section_title"><a href="#minimand">Minimand</a></span><span class="word_count">(87w~1m)</span></li>
                    <li><span class="section_number">8.7.20</span><span class="section_title"><a href="#constrained-system">Constrained system</a></span><span class="word_count">(28w~1m)</span></li>
                    <li><span class="section_number">8.7.21</span><span class="section_title"><a href="#optimizing-system">Optimizing system</a></span><span class="word_count">(52w~1m)</span></li>
                    <li><span class="section_number">8.7.22</span><span class="section_title"><a href="#purposeful-system">Purposeful system</a></span><span class="word_count">(90w~1m)</span></li>
                    <li><span class="section_number">8.7.23</span><span class="section_title"><a href="#how-do-we-measure-how-well-a-system-serves-its-purpose">How do we measure how well a system serves its purpose?</a></span><span class="word_count">(21w~1m)</span></li>
                    <li><span class="section_number">8.7.24</span><span class="section_title"><a href="#what-is-an-intelligent-system">What is an intelligent system?</a></span><span class="word_count">(47w~1m)</span></li>
                    </ul>
                    </div>
                    <h4 id="what-is-a-system"><span class="section_number">8.7.1</span><span class="section_title">What is a system?</span></h4>
                    <p>We define a system as an input, a state, and an output. The input is <span class="math inline">\(x\)</span>, the output is <span class="math inline">\(y\)</span>, and an equation relates them. The state is implied by the equation. Such equation can be written <span class="math inline">\(f~y~x = y\)</span>. The equation can be quite arbitrary. The terms <span class="math inline">\(f,x,y\)</span> may appear on both sides of the equation.</p>
                    <p>A system is <span class="math inline">\((x,y,f)\)</span> where <span class="math inline">\(y=f~y~x\)</span>.</p>
                    <p>An <em>invariant</em> of a system is a property that stays the same throughout the evolution of the system.</p>
                    <p>The behavior of a system is its output, especially the observable part of the output.</p>
                    <p>Composition</p>
                    <p>Continuous system</p>
                    <p>Discrete system</p>
                    <p>Finite system</p>
                    <p>An embedded system is a system in another system. The outer system feeds the inner system's output back to the inner system's input, possibly with some change.</p>
                    <p>Don't confuse this with embedded systems in computer engineering.</p>
                    <p>How do we measure system complexity?</p>
                    <h4 id="ignoring-degenerate-feedback-feedforward"><span class="section_number">8.7.2</span><span class="section_title">Ignoring degenerate feedback: feedforward</span></h4>
                    <p>Every function <span class="math inline">\(f\)</span> is a special case of the general feedback equation <span class="math inline">\(f(x) = g(f,x)\)</span> where <span class="math inline">\(g\)</span> is an identity function. This suggests that feedforward is a degenerate case of feedback. To simplify the writing, from this point on, we always assume that a feedback is non-degenerate unless written otherwise.</p>
                    <h4 id="finding-feedback-the-inverse-fixed-point-problem"><span class="section_number">8.7.3</span><span class="section_title">Finding feedback: the inverse fixed point problem</span></h4>
                    <p>Given <span class="math inline">\(f\)</span>, find a <span class="math inline">\(g\)</span> such that <span class="math inline">\(f(x) = g(f,x)\)</span> and <span class="math inline">\(g\)</span> is not an identity function.</p>
                    <p>The forward fixed point problem: Given <span class="math inline">\(f\)</span>, find an <span class="math inline">\(x\)</span> such that <span class="math inline">\(x=f(x)\)</span>.</p>
                    <p>The inverse fixed point problem is &quot;Given <span class="math inline">\(x\)</span>, find an <span class="math inline">\(f\)</span> such that <span class="math inline">\(x = f(x)\)</span> and <span class="math inline">\(f\)</span> is not an identity function.&quot; This problem arises when we want to determine if <span class="math inline">\(f\)</span> has a feedback.</p>
                    <p>Example of non-feedback: linear functions. Consider a function of the form <span class="math inline">\(f(x) = a \cdot x + b\)</span> where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are non-zero constants. The only <span class="math inline">\(g\)</span> that satisfies <span class="math inline">\(g(f) = f\)</span> is the identity function <span class="math inline">\(g(x)=x\)</span>.</p>
                    <p>Example of feedback: functional equation. Consider a function of the form <span class="math inline">\(f(x) = x \cdot f(x-1)\)</span>.</p>
                    <p>Recursive functions are special cases of feedback. Searching in list. <span class="math inline">\(f(N,e) = 0\)</span>. <span class="math inline">\(f(C,h,t,e) = h \equiv e \vee f(t,e)\)</span>. <span class="math inline">\(g\)</span> is the Y-combinator.</p>
                    <p>We have a problem: there are infinitely many wildly discontinuous functions satisfying that. We want smooth functions.</p>
                    <h4 id="feedback-based-on-differentiability-preserving-map"><span class="section_number">8.7.4</span><span class="section_title">Feedback based on differentiability-preserving map</span></h4>
                    <p>We want a map that preserves differentiability. Formally, given <span class="math inline">\(f=g(f)\)</span>, we want <span class="math inline">\(g\)</span> to have the property that iff <span class="math inline">\(f\)</span> is differentiable then <span class="math inline">\(g(f)\)</span> is also differentiable. Surely if <span class="math inline">\(f\)</span> is differentiable and <span class="math inline">\(g\)</span> is differentiable then <span class="math inline">\(g(f)\)</span> is also differentiable? Surely if <span class="math inline">\(f\)</span> is differentiable <span class="math inline">\(g\)</span> is a polynomial then <span class="math inline">\(g(f)\)</span> is differentiable?</p>
                    <p>We begin with the generalized differential on a field: <span class="math inline">\(g~(f+h) = g~f + h \cdot d~g~f\)</span> where <span class="math inline">\((f + g)~x = f~x + g~x\)</span> and <span class="math inline">\((f \cdot g)~x = f~x \cdot g~x\)</span>. Thus <span class="math inline">\(h \cdot d~g~f = g~(f+h) - g~f\)</span>. This is like computing the gradient of a vector function, but the vector is infinite-dimensional.</p>
                    <p>Is it time to learn topology? Smooth manifolds?</p>
                    <h4 id="measuring-feedback"><span class="section_number">8.7.5</span><span class="section_title">Measuring feedback</span></h4>
                    <p>Given a system <span class="math inline">\(f(x) = g(f,x)\)</span>, we're interested in measuring how much feedback it has.</p>
                    <p>Assume that <span class="math inline">\(f\)</span> is a vector. We can measure the feedback by measuring <span class="math inline">\(d_f ~ g\)</span>: the differential of <span class="math inline">\(g\)</span> with respect to <span class="math inline">\(f\)</span>. Using non-standard analysis, we define the gradient <span class="math inline">\(d f\)</span> as something satisfying <span class="math inline">\(f(x + h) = f(x) + h \cdot (d f)(x)\)</span> where <span class="math inline">\(h\)</span> is an infinitesimal.</p>
                    <h4 id="linear-feedback-and-function-classes"><span class="section_number">8.7.6</span><span class="section_title">Linear feedback and function classes</span></h4>
                    <p>If <span class="math inline">\(f\)</span> is linear and <span class="math inline">\(g\)</span> is linear, then <span class="math inline">\(f \circ g\)</span> is linear. A linear feedback does not add anything interesting to a linear function.</p>
                    <h4 id="temporal-systems"><span class="section_number">8.7.7</span><span class="section_title">Temporal systems</span></h4>
                    <p>A temporal system, a time-dependent system, or a time-variant system is a system that depends on time. With time, we can define more interesting systems.</p>
                    <p>A temporal system is a function whose type is <span class="math inline">\((T \to X) \to T \to Y\)</span>. <span class="math display">\[\SysTmp~T~X~Y = (T \to X) \to (T \to Y)\]</span></p>
                    <p>We can see a temporal system as a transformation of time functions. <span class="math inline">\((T \to X) \to (T \to Y)\)</span>.</p>
                    <p>Example: <span class="math inline">\(f~x~t = (x~t)^2\)</span>.</p>
                    <p>Example: <span class="math display">\[f~x~t = x~t + \int_0^t (s - f~x~t) \cdot dt\]</span>.</p>
                    <p>A temporal system is <span class="math inline">\((x,y,f,T)\)</span> that satisfies <span class="math inline">\(\forall t \in T : f~y~x~t\)</span>.</p>
                    <h4 id="first-order-system"><span class="section_number">8.7.8</span><span class="section_title">First-order system</span></h4>
                    <p>The previous section talks about second-order system.</p>
                    <p>This is a first-order system: <span class="math inline">\(\SysTmp~T~X~Y = X \to T \to Y\)</span>.</p>
                    <p><span class="math inline">\(\SysTmp~T~X~Y = T \to X \to Y\)</span>.</p>
                    <p>There are two points of view: <span class="math inline">\(d_x~f\)</span> and <span class="math inline">\(d_t~f\)</span>.</p>
                    <p>First-order system should be more analyzable.</p>
                    <p>Continuous-time and discrete-time system?</p>
                    <p>In the above definition, <span class="math inline">\(T\)</span> is the time type. If <span class="math inline">\(T = \R\)</span> we call the system continuous-time. If <span class="math inline">\(T = \N\)</span> we call the system discrete-time.</p>
                    <h4 id="chaining-temporal-systems"><span class="section_number">8.7.9</span><span class="section_title">Chaining temporal systems</span></h4>
                    <p>We can feed the output temporal system <span class="math inline">\(f\)</span> to the input of the temporal system <span class="math inline">\(g\)</span> this produces the temporal system <span class="math inline">\(h\)</span> where <span class="math inline">\(h~x~t = g~(f~x)~t\)</span>, or <span class="math inline">\(h~x = g~(f~x)\)</span> after eta-conversion, or <span class="math inline">\(h = g \circ f\)</span>. It turns out that system composition is just plain function composition.</p>
                    <h4 id="stateless-and-stateful-systems"><span class="section_number">8.7.10</span><span class="section_title">Stateless and stateful systems</span></h4>
                    <p>A system is stateless iff the same input always gives the same output. There is no way to tell apart a system that has state but doesn't use it and a system that really has no state.</p>
                    <p>A stateless system is a temporal system that satisfies <span class="math inline">\(\forall t : \forall u: x~t = x~u \implies y~t = y~u\)</span>.</p>
                    <p>In a stateful system, the same input can give different outputs, depending on time.</p>
                    <p>Why do we define those?</p>
                    <h4 id="property"><span class="section_number">8.7.11</span><span class="section_title">Property</span></h4>
                    <p>If <span class="math inline">\(p\)</span> is a predicate that is always true for a system, then <span class="math inline">\(p\)</span> is a property of that system. <span class="math inline">\(\SysTmp~T~X~Y \to \{0,1\}\)</span></p>
                    <h4 id="constraint"><span class="section_number">8.7.12</span><span class="section_title">Constraint</span></h4>
                    <p>A constraint of <span class="math inline">\(S\)</span> is a property of <span class="math inline">\(S\)</span> that is always true.</p>
                    <h4 id="parameterfamily"><span class="section_number">8.7.13</span><span class="section_title">Parameter/family</span></h4>
                    <p>Parameterized system.</p>
                    <p><span class="math inline">\(P \to \SysTmp~T~X~Y\)</span></p>
                    <p>System parameter.</p>
                    <p>Family of systems.</p>
                    <p>Indexed family of systems.</p>
                    <h4 id="measure"><span class="section_number">8.7.14</span><span class="section_title">Measure</span></h4>
                    <p>Categorical inverse of parameter. (Whatever categorical inverse means.)</p>
                    <p>From type theory point of view, parametrization is the inverse of measurement.</p>
                    <p><span class="math inline">\(\SysTmp~T~X~Y \to M\)</span></p>
                    <h4 id="temporal-measure"><span class="section_number">8.7.15</span><span class="section_title">Temporal measure</span></h4>
                    <p><span class="math inline">\(m : \SysTmp~T~X~Y \to (T \to M)\)</span></p>
                    <p>Find <span class="math inline">\(s\)</span> that minimizes <span class="math inline">\(m~s~t\)</span> as <span class="math inline">\(t\)</span> grows.</p>
                    <h4 id="system-space"><span class="section_number">8.7.16</span><span class="section_title">System space</span></h4>
                    <p>Like function space. Metric space.</p>
                    <h4 id="system-endofunction"><span class="section_number">8.7.17</span><span class="section_title">System endofunction</span></h4>
                    <p><span class="math inline">\(\SysTmp~T~X~Y \to \SysTmp~T~X~Y\)</span>.</p>
                    <h4 id="output-input-gradient"><span class="section_number">8.7.18</span><span class="section_title">Output-input gradient</span></h4>
                    <p><span class="math inline">\(f : \SysTmp~T~X~Y\)</span></p>
                    <p><span class="math inline">\(f~(x+h) - f~x = h \cdot d~f~x\)</span> but <span class="math inline">\(h\)</span> is a function.</p>
                    <p><span class="math inline">\(m\)</span>-adaptivity</p>
                    <p><span class="math inline">\((m~f~(x+h) - m~f~x) / h\)</span></p>
                    <p>Reversal: <span class="math inline">\(\SysTmp~T~X~Y \to \SysTmp~T~Y~X\)</span></p>
                    <p>Time-reversible/Time-symmetric: <span class="math inline">\(f~x~t = f~x~(-t)\)</span></p>
                    <h4 id="minimand"><span class="section_number">8.7.19</span><span class="section_title">Minimand</span></h4>
                    <p>The minimand is the thing that is to be minimized. It's an English word. The minimand of a temporal system is a function that is minimized as time goes by.</p>
                    <p>Recall that a temporal system has type <span class="math inline">\((T \to X) \to T \to Y\)</span>. A minimand is a function that has type <span class="math inline">\((T \to X) \to T \to M\)</span>.</p>
                    <p>The function <span class="math inline">\(g\)</span> is a minimand of a temporal system <span class="math inline">\(s\)</span> iff <span class="math inline">\(g~s~t \xrightarrow[t \to \infty]{} 0\)</span>.</p>
                    <p>There's always a trivial minimand: <span class="math inline">\(g~s~t = 0\)</span>.</p>
                    <p>Does every system have a non-trivial minimand?</p>
                    <h4 id="constrained-system"><span class="section_number">8.7.20</span><span class="section_title">Constrained system</span></h4>
                    <p>Constrained system: a system whose equation is subject to constraints (which can be inequalities). Every system is constrained; the definition requires it. So why bother defining this?</p>
                    <h4 id="optimizing-system"><span class="section_number">8.7.21</span><span class="section_title">Optimizing system</span></h4>
                    <p>A system is <em>optimizing</em> iff it optimizes a function. We call this function a <em>goal function</em>. The purpose of the system is to minimize the goal function.</p>
                    <p>A goal is something that a system wants to reach. This implies that the definition of goal involves time. The goal function is usually hidden.</p>
                    <h4 id="purposeful-system"><span class="section_number">8.7.22</span><span class="section_title">Purposeful system</span></h4>
                    <p>We also call a purposeful system an optimizing system.</p>
                    <p><em>Purpose requires time.</em></p>
                    <p>Let <span class="math inline">\(x\)</span> be a function of time. Let the equation <span class="math inline">\(f~x~t = y\)</span> govern the system. Let <span class="math inline">\(g~x\)</span> be a function of time. The system is <em>purposeful</em> iff <span class="math inline">\(g~x~t\)</span> approaches zero as <span class="math inline">\(t\)</span> grows, for some non-trivial <span class="math inline">\(g\)</span>. We say that <span class="math inline">\(g\)</span> is a <em>purpose</em> or a <em>goal</em> of the system. The goal function may represent the sensed error with respect to a setpoint.</p>
                    <p>A purposeful system doesn't have to be adaptive. A simple thermostat is purposeful but not adaptive.</p>
                    <h4 id="how-do-we-measure-how-well-a-system-serves-its-purpose"><span class="section_number">8.7.23</span><span class="section_title">How do we measure how well a system serves its purpose?</span></h4>
                    <p>is like measuring the rate of convergence of an approximation scheme.</p>
                    <h4 id="what-is-an-intelligent-system"><span class="section_number">8.7.24</span><span class="section_title">What is an intelligent system?</span></h4>
                    <p>Stable system: See stability theory. Lyapunov.</p>
                    <p>How do we measure how adaptive a system is?</p>
                    <p>An adaptive system is a system that adapts.</p>
                    <p>Adaptation implies change.</p>
                    <p>Adapt means &quot;fit, adjust&quot;.</p>
                    <p>Adaptive with respect to what?</p>
                    <p>Chaotic system: Small change in input causes large change in output. See chaos theory.</p>
                    <h2 id="ai-research"><span class="section_number">9</span><span class="section_title">AI research???</span></h2>
                    <div class="local_table_of_contents">
                    <ul>
                    <li><span class="section_number">9.1</span><span class="section_title"><a href="#surveys-reviews-positions-and-expositions">Surveys, reviews, positions, and expositions?</a></span><span class="word_count">(25w~1m)</span></li>
                    <li><span class="section_number">9.2</span><span class="section_title"><a href="#readings">Readings?</a></span><span class="word_count">(106w~1m)</span></li>
                    <li><span class="section_number">9.3</span><span class="section_title"><a href="#what-are-some-expository-works-in-ai">What are some expository works in AI?</a></span><span class="word_count">(20w~1m)</span></li>
                    <li><span class="section_number">9.4</span><span class="section_title"><a href="#what-are-the-trends-in-ai">What are the trends in AI?</a></span><span class="word_count">(46w~1m)</span></li>
                    <li><span class="section_number">9.5</span><span class="section_title"><a href="#should-we-read-this">Should we read this?</a></span><span class="word_count">(15w~1m)</span></li>
                    <li><span class="section_number">9.6</span><span class="section_title"><a href="#habituation-1">Habituation</a></span><span class="word_count">(11w~1m)</span></li>
                    <li><span class="section_number">9.7</span><span class="section_title"><a href="#selected-threads-from-rartificial">Selected threads from /r/artificial?</a></span><span class="word_count">(22w~1m)</span></li>
                    <li><span class="section_number">9.8</span><span class="section_title"><a href="#history-questions">History questions</a></span><span class="word_count">(25w~1m)</span></li>
                    <li><span class="section_number">9.9</span><span class="section_title"><a href="#undigested-information">Undigested information</a></span><span class="word_count">(948w~5m)</span></li>
                    <li><span class="section_number">9.10</span><span class="section_title"><a href="#where-is-more-information">Where is more information?</a></span><span class="word_count">(6w~1m)</span></li>
                    <li><span class="section_number">9.11</span><span class="section_title"><a href="#where-are-new-results-announced">Where are new results announced?</a></span><span class="word_count">(8w~1m)</span></li>
                    <li><span class="section_number">9.12</span><span class="section_title"><a href="#people">People??</a></span><span class="word_count">(229w~2m)</span></li>
                    <li><span class="section_number">9.13</span><span class="section_title"><a href="#how-can-i-become-an-ai-researcher">How can I become an AI researcher?</a></span><span class="word_count">(14w~1m)</span></li>
                    <li><span class="section_number">9.14</span><span class="section_title"><a href="#university-courses">University courses</a></span><span class="word_count">(39w~1m)</span></li>
                    <li><span class="section_number">9.15</span><span class="section_title"><a href="#other-resources">Other resources</a></span><span class="word_count">(26w~1m)</span></li>
                    <li><span class="section_number">9.16</span><span class="section_title"><a href="#clean-up-oldindex.xml"><span class="todo TODO">TODO</span> clean up oldindex.xml</a></span><span class="word_count">(111w~1m)</span></li>
                    <li><span class="section_number">9.17</span><span class="section_title"><a href="#fields-of-study-related-to-intelligence"><span class="todo TODO">TODO</span> Fields of study related to intelligence</a></span><span class="word_count">(1100w~6m)</span></li>
                    <li><span class="section_number">9.18</span><span class="section_title"><a href="#wonderings"><span class="todo TODO">TODO</span> Wonderings</a></span><span class="word_count">(111w~1m)</span></li>
                    <li><span class="section_number">9.19</span><span class="section_title"><a href="#reading-list-1">Reading list?</a></span><span class="word_count">(150w~1m)</span></li>
                    <li><span class="section_number">9.20</span><span class="section_title"><a href="#independent-scholar-citizen-science">Independent scholar? Citizen science?</a></span><span class="word_count">(4w~1m)</span></li>
                    <li><span class="section_number">9.21</span><span class="section_title"><a href="#what-is-the-relationship-between-ai-and-ml">What is the relationship between AI and ML?</a></span><span class="word_count">(46w~1m)</span></li>
                    </ul>
                    </div>
                    <h3 id="surveys-reviews-positions-and-expositions"><span class="section_number">9.1</span><span class="section_title">Surveys, reviews, positions, and expositions?</span></h3>
                    <ul>
                    <li>Google query: most recent mathematical ai book</li>
                    <li><a href="http://eliassi.org/COLTSurveyArticle.pdf">http://eliassi.org/COLTSurveyArticle.pdf</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Computational_learning_theory#Surveys">WP: COLT surveys</a></li>
                    <li><a href="http://www.cs.ox.ac.uk/people/varun.kanade/teaching/CLT-HT2018/lectures/">COLT lecture 2018</a></li>
                    <li>Book: &quot;An Introduction to Computational Learning Theory&quot; by Kearns and Vazirani</li>
                    <li><a href="https://mitpress.mit.edu/books/introduction-computational-learning-theory">https://mitpress.mit.edu/books/introduction-computational-learning-theory</a></li>
                    </ul>
                    <h3 id="readings"><span class="section_number">9.2</span><span class="section_title">Readings?</span></h3>
                    <ul>
                    <li><p>Read about universal intelligence</p>
                    <ul>
                    <li><p>Pamela McCorduck's &quot;Machines who think&quot; for some history</p>
                    <ul>
                    <li><a href="https://en.wikipedia.org/wiki/Timeline_of_artificial_intelligence">WP: Timeline of artificial intelligence</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Progress_in_artificial_intelligence">WP: Progress in artificial intelligence</a></li>
                    </ul></li>
                    <li>[Hutter2005Book]</li>
                    <li><p><a href="http://www.hutter1.net/ai/uaibook.htm">hutter1.net…uaibook.htm</a></p>
                    <ul>
                    <li>He formulated the &quot;degree of intelligence&quot; in 2005</li>
                    <li>(edited) &quot;AIXI […] learns by eliminating Turing machines […] once they become inconsistent with the progressing history.&quot;</li>
                    </ul></li>
                    <li><a href="http://www.hutter1.net/ai/suaibook.pdf">Presentation, 393 slides</a></li>
                    <li><a href="http://users.cecs.anu.edu.au/~ssanner/MLSS2010/Hutter1.pdf">Slides</a>, maybe a draft of the above.</li>
                    <li>Shane Legg's PhD thesis &quot;Machine super intelligence&quot; [Legg2008]</li>
                    <li><a href="http://www.vetta.org/documents/universal_intelligence_abstract_ai50.pdf">Legg and Hutter: A formal definition of intelligence for artificial systems</a></li>
                    <li><p>2005 Negnevitsky AI book <span class="citation" data-cites="negnevitsky2005artificial">[<a href="#ref-negnevitsky2005artificial">22</a>]</span>?</p></li>
                    </ul></li>
                    <li><p>COLT</p>
                    <ul>
                    <li><p>Should we read this?</p>
                    <ul>
                    <li><a href="https://arxiv.org/abs/1405.1513">Ibrahim Alabdulmohsin: A Mathematical Theory of Learning</a></li>
                    <li>1999: <a href="http://www.cis.syr.edu/people/royer/stl2e/">Sanjay Jain et al.: Systems that learn</a></li>
                    <li><a href="https://www.quora.com/What-are-the-best-math-books-for-machine-learning">https://www.quora.com/What-are-the-best-math-books-for-machine-learning</a></li>
                    <li><a href="https://machinelearningwithvick.quora.com/Learning-about-machine-learning">https://machinelearningwithvick.quora.com/Learning-about-machine-learning</a></li>
                    <li><a href="http://web.archive.org/web/20101102210231/http://measuringmeasures.com/blog/2010/1/15/learning-about-statistical-learning.html">http://web.archive.org/web/20101102210231/http://measuringmeasures.com/blog/2010/1/15/learning-about-statistical-learning.html</a></li>
                    <li><a href="https://www.quora.com/Which-are-the-best-books-to-get-the-Math-background-for-Machine-Learning">https://www.quora.com/Which-are-the-best-books-to-get-the-Math-background-for-Machine-Learning</a></li>
                    <li><a href="https://www.quora.com/How-do-I-learn-mathematics-for-machine-learning?share=1">https://www.quora.com/How-do-I-learn-mathematics-for-machine-learning?share=1</a></li>
                    </ul></li>
                    <li><p><a href="http://emis.ams.org/journals/TAC/reprints/articles/22/tr22.pdf">http://emis.ams.org/journals/TAC/reprints/articles/22/tr22.pdf</a></p>
                    <ul>
                    <li><a href="https://www.quora.com/What-are-some-survey-papers-on-artificial-intelligence-and-deep-learning">https://www.quora.com/What-are-some-survey-papers-on-artificial-intelligence-and-deep-learning</a></li>
                    <li><a href="http://people.idsia.ch/~juergen/deep-learning-conspiracy.html">http://people.idsia.ch/~juergen/deep-learning-conspiracy.html</a></li>
                    <li><a href="https://arxiv.org/abs/1404.7828">Jürgen Schmidhuber: &quot;Deep Learning in Neural Networks: An Overview&quot;</a></li>
                    <li><a href="http://www.ijircce.com/upload/2017/june/107_A%20Survey.pdf">http://www.ijircce.com/upload/2017/june/107_A%20Survey.pdf</a></li>
                    </ul></li>
                    </ul></li>
                    </ul>
                    <p>Should we read these?</p>
                    <p>2017, <a href="https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/building-machines-that-learn-and-think-for-themselves/E28DBFEC380D4189FB7754B50066A96F">Building machines that learn and think for themselves</a></p>
                    <h3 id="what-are-some-expository-works-in-ai"><span class="section_number">9.3</span><span class="section_title">What are some expository works in AI?</span></h3>
                    <ul>
                    <li><a href="https://www.sciencedirect.com/science/article/pii/S1574013717300606">The evolution of sentiment analysis—A review of research topics, venues, and top cited papers</a></li>
                    </ul>
                    <h3 id="what-are-the-trends-in-ai"><span class="section_number">9.4</span><span class="section_title">What are the trends in AI?</span></h3>
                    <ul>
                    <li><a href="https://twitter.com/michael_nielsen/status/983502409325395969">Michael Nielsen's tweet</a>: &quot;I meet lots of people who tell me fatalistically (&amp; often despondently) that it's near impossible to do important work on neural nets today, unless you have huge compute and huge data sets.&quot;
                    <ul>
                    <li><a href="https://arxiv.org/abs/1712.00409">Deep Learning Scaling is Predictable, Empirically</a></li>
                    </ul></li>
                    </ul>
                    <h3 id="should-we-read-this"><span class="section_number">9.5</span><span class="section_title">Should we read this?</span></h3>
                    <ul>
                    <li><a href="http://www.cs.cmu.edu/~16831-f12/notes/F11/16831_lecture15_shorvath.pdf">Boosting: Gradient descent in function space</a></li>
                    <li><a href="http://alessio.guglielmi.name/res/cos/">Alessio Guglielmi's deep inference</a></li>
                    <li><a href="https://arxiv.org/abs/1412.1044">Problem theory, Ramón Casares</a></li>
                    </ul>
                    <h3 id="habituation-1"><span class="section_number">9.6</span><span class="section_title">Habituation</span></h3>
                    <ul>
                    <li><a href="https://www.sciencedaily.com/releases/2016/04/160427081533.htm">A single-celled organism capable of learning</a>: protists may learn by habituation</li>
                    </ul>
                    <h3 id="selected-threads-from-rartificial"><span class="section_number">9.7</span><span class="section_title">Selected threads from /r/artificial?</span></h3>
                    <ul>
                    <li><a href="https://www.reddit.com/r/artificial/comments/8begcv/what_are_some_of_the_best_books_on_artificial/">What are some of the best books on AI/ML?</a></li>
                    <li><a href="https://www.reddit.com/r/artificial/comments/8bzrmd/math_phd_want_to_learn_more_about_ai_what_to_read/">Math PhD. Want to learn more about AI. What to read?</a></li>
                    </ul>
                    <h3 id="history-questions"><span class="section_number">9.8</span><span class="section_title">History questions</span></h3>
                    <ul>
                    <li>Why was Raymond J. Solomonoff <span class="citation" data-cites="SolAlpProb2011 GacsVitanyiSolomonoff">[<a href="#ref-GacsVitanyiSolomonoff">9</a>, <a href="#ref-SolAlpProb2011">31</a>]</span> interested in predicting sequences of bits? What was he interested in? What was he trying to do?</li>
                    </ul>
                    <h3 id="undigested-information"><span class="section_number">9.9</span><span class="section_title">Undigested information</span></h3>
                    <ul>
                    <li><a href="https://kevinbinz.com/2017/08/13/ml-five-tribes/">kevinbinz.com: Five Tribes of Machine Learning</a>, part of <a href="https://kevinbinz.com/2017/05/09/sequence-machine-learning/">machine learning sequence</a>, some contents from Pedro Domingos's book &quot;The master algorithm&quot;</li>
                    <li><a href="http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html">Introducing state of the art text classification with universal language models</a></li>
                    <li><p>Summary of Pedro Domingos's book &quot;The master algorithm&quot;</p>
                    <ul>
                    <li>Sparse autoencoders (p. 116).</li>
                    <li>&quot;A nugget of knowledge so incontestable, so fundamental, that we can build all induction on top of it&quot; (p. 64) in Chapter 9.</li>
                    <li>Induction is the inverse of deduction, as subtraction is the inverse of addition. (Is this a quote from the book?)</li>
                    <li>EM (expectation maximization) algorithm (p. 209).</li>
                    <li>Metalearning (p. 237).</li>
                    <li>A classifier that classifies by combining the output of subclassifiers.</li>
                    <li><a href="http://homes.cs.washington.edu/~pedrod/papers/mlj05.pdf">Markov logic network</a> (p. 246) named <a href="Alchemy"><a href="http://alchemy.cs.washington.edu/">http://alchemy.cs.washington.edu/</a></a> (p. 250)</li>
                    </ul></li>
                    <li>Harvard University the graduate school of arts and sciences: <a href="http://sitn.hms.harvard.edu/flash/2017/history-artificial-intelligence/">Rockwell Anyoha: History of AI</a></li>
                    <li><a href="http://jacques.pitrat.pagesperso-orange.fr/">Jacques Pitrat</a> and his CAIA, bootstrapping AI with AI.</li>
                    <li><a href="http://www.hutter1.net/ai/uaibook.htm">Marcus Hutter book: Universal Artificial Intelligence: Sequential Decisions based on Algorithmic Probability</a> and the <a href="http://www.hutter1.net/ai/suaibook.pdf">slides</a>.</li>
                    <li><p><a href="http://math.bu.edu/people/mkon/V5Fin.pdf">Mark A. Kon, Louise A. Raphael, Daniel A. Williams: Extending Girosi's approximation estimates for functions in Sobolev spaces via statistical learning theory</a></p>
                    <ul>
                    <li>&quot;Girosi [8] established an interesting connection between statistical learning theory (SLT) and approximation theory, showing that SLT methods can be used to prove results of a purely approximation theoretic nature.&quot;</li>
                    </ul></li>
                    <li>Speech synthesizer using hidden Markov model? Someone must have done it. Find the paper.</li>
                    <li>ISIR (International Society for Intelligence Research) human intelligence research <a href="http://www.isironline.org/resources/teaching-pages/">teaching pages</a>.</li>
                    <li><a href="https://en.wikipedia.org/wiki/Artificial_life">https://en.wikipedia.org/wiki/Artificial_life</a></li>
                    <li>What is the simplest life form? (2008) <a href="https://www.quora.com/What-is-the-simplest-life-form">https://www.quora.com/What-is-the-simplest-life-form</a></li>
                    <li><a href="https://stats.stackexchange.com/questions/142906/what-does-pac-learning-theory-mean">https://stats.stackexchange.com/questions/142906/what-does-pac-learning-theory-mean</a></li>
                    <li><p><a href="https://brenocon.com/blog/2008/12/statistics-vs-machine-learning-fight/">https://brenocon.com/blog/2008/12/statistics-vs-machine-learning-fight/</a></p>
                    <ul>
                    <li>YC thread for that <a href="https://news.ycombinator.com/item?id=4927168">https://news.ycombinator.com/item?id=4927168</a></li>
                    </ul></li>
                    <li><a href="https://www.quora.com/What-are-the-most-important-foundational-papers-in-artificial-intelligence-machine-learning">Quora: What are the most important, foundational papers in artificial intelligence/machine learning?</a></li>
                    <li>JAIR (Journal of Artificial Intelligence Research): <a href="https://www.jair.org/index.php/jair/navigationMenu/view/IJCAIJAIR">IJCAI-JAIR awards</a></li>
                    <li>Schmidhuber, <a href="http://people.idsia.ch/~juergen/fastestuniverse.pdf">The Fastest Way of Computing All Universes</a></li>
                    <li><p><a href="http://raysolomonoff.com/dartmouth/">Dartmouth AI archives</a></p>
                    <ul>
                    <li><a href="http://raysolomonoff.com/publications/indinf56.pdf">Solomonoff, &quot;An inductive inference machine&quot;</a></li>
                    </ul></li>
                    <li><p>Shane Legg, Joel Veness: algorithmic intelligence quotient</p>
                    <ul>
                    <li><a href="https://github.com/mathemajician/AIQ">https://github.com/mathemajician/AIQ</a></li>
                    <li>An Approximation of the Universal Intelligence Measure by Shane Legg and Joel Veness, 2011</li>
                    </ul></li>
                    <li><a href="https://courses.cs.washington.edu/courses/csep590/06au/projects/history-ai.pdf">History of AI</a>, University of Washington, History of Computing, CSEP 590A</li>
                    <li><a href="https://en.wikipedia.org/wiki/Timeline_of_artificial_intelligence">WP: Timeline of AI</a></li>
                    <li><a href="https://www.quantamagazine.org/why-self-taught-artificial-intelligence-has-trouble-with-the-real-world-20180221/">https://www.quantamagazine.org/why-self-taught-artificial-intelligence-has-trouble-with-the-real-world-20180221/</a></li>
                    <li><a href="http://news.mit.edu/2010/ai-unification">http://news.mit.edu/2010/ai-unification</a></li>
                    <li><a href="http://airesearch.com/">http://airesearch.com/</a></li>
                    <li><a href="https://theconversation.com/understanding-the-four-types-of-ai-from-reactive-robots-to-self-aware-beings-67616">https://theconversation.com/understanding-the-four-types-of-ai-from-reactive-robots-to-self-aware-beings-67616</a></li>
                    <li><a href="https://artificialintelligence.id/">https://artificialintelligence.id/</a></li>
                    <li><a href="https://www.asianscientist.com/2017/09/academia/indonesia-ai-nvidia-binus-kinetica/">https://www.asianscientist.com/2017/09/academia/indonesia-ai-nvidia-binus-kinetica/</a></li>
                    <li><a href="https://arxiv.org/abs/1206.5533">Practical recommendations for gradient-based training of deep architectures</a></li>
                    <li><a href="https://arxiv.org/abs/1604.06737">Entity Embeddings of Categorical Variables</a></li>
                    <li>Google Colab</li>
                    <li><a href="https://qz.com/1172431/artificial-intelligence-ai-should-be-raised-like-children-not-computers/">https://qz.com/1172431/artificial-intelligence-ai-should-be-raised-like-children-not-computers/</a></li>
                    <li><p>RNN, LSTM, GRU</p>
                    <ul>
                    <li>RNN is recurrent neural network.</li>
                    <li>LSTM is a kind of RNN.</li>
                    <li>GRU is a kind of RNN.</li>
                    <li><a href="https://jhui.github.io/2017/03/15/RNN-LSTM-GRU/">https://jhui.github.io/2017/03/15/RNN-LSTM-GRU/</a></li>
                    </ul></li>
                    <li><a href="http://web.mit.edu/tslvr/www/lessons_two_years.html">http://web.mit.edu/tslvr/www/lessons_two_years.html</a></li>
                    <li><a href="https://gallery.mailchimp.com/dc3a7ef4d750c0abfc19202a3/files/93e40657-1adb-4891-94ad-c65dda68061f/Ng_MLY01_02.pdf">https://gallery.mailchimp.com/dc3a7ef4d750c0abfc19202a3/files/93e40657-1adb-4891-94ad-c65dda68061f/Ng_MLY01_02.pdf</a></li>
                    <li><a href="https://www.reddit.com/r/MachineLearning/comments/73n9pm/d_confession_as_an_ai_researcher_seeking_advice/#bottom-comments">https://www.reddit.com/r/MachineLearning/comments/73n9pm/d_confession_as_an_ai_researcher_seeking_advice/#bottom-comments</a></li>
                    <li><a href="http://www.inf.ed.ac.uk/teaching/courses/mlpr/2017/notes/w6b_netflix_prize.html">netflix prize, part of MLPR class notes</a></li>
                    <li><p>Scott M. Lundberg, Su-In Lee: A Unified Approach to Interpreting Model Predictions</p>
                    <ul>
                    <li><a href="http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf">http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf</a></li>
                    <li><a href="https://github.com/slundberg/shap">https://github.com/slundberg/shap</a></li>
                    </ul></li>
                    <li><a href="https://www.datascience.com/blog/introduction-to-bayesian-inference-learn-data-science-tutorials">datascience.com: Introduction to Bayesian Inference</a></li>
                    <li><a href="http://www.fc.uaem.mx/~bruno/material/brooks_87_representation.pdf">1987, Intelligence without representation, Rodney A. Brooks</a></li>
                    <li><a href="http://colah.github.io/posts/2015-08-Backprop/">colah.github.io: Backprop</a></li>
                    <li>google search &quot;ai theory research&quot;</li>
                    <li><a href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.2.4835">2002, PhotoTOC: Automatic Clustering for Browsing Personal Photographs, by John C. Platt, Mary Czerwinski, Brent A. Field</a></li>
                    <li><p>philosophy of learning</p>
                    <ul>
                    <li><a href="http://learning.media.mit.edu/content/publications/EA.Piaget%20_%20Papert.pdf">Piaget's constructivism vs Papert's constructionism</a>, Edith Ackermann</li>
                    </ul></li>
                    <li><a href="https://arxiv.org/abs/1508.01084">2015, Deep Convolutional Networks are Hierarchical Kernel Machines</a></li>
                    <li><a href="https://www.youtube.com/watch?v=F5Z52jl4yHQ">Michio Kaku: Who is right about A.I.: Mark Zuckerberg or Elon Musk?</a></li>
                    <li><a href="https://stats.stackexchange.com/questions/104385/assigning-meaningful-cluster-name-automatically">Stats SE 104385: text processing: assigning meaningful cluster name automatically</a></li>
                    <li>The mathematics of deep learning (a website)</li>
                    <li>Can AI be used to upscale old audio/video recordings? Fix deteriorated pictures, films, documents? Color old pictures, photos, films? &quot;Modernize&quot; past artifacts? Digital restoration of archives?</li>
                    <li><p>brain-computer interface</p>
                    <ul>
                    <li><p>pop science</p>
                    <ul>
                    <li><a href="https://www.youtube.com/watch?v=P29EXskk9oU">How Brain Waves Can Control Physical Objects</a></li>
                    </ul></li>
                    </ul></li>
                    <li><p>machine learning</p>
                    <ul>
                    <li>confusion matrix</li>
                    <li><p>algebra of words</p>
                    <ul>
                    <li><a href="https://medium.com/@erushton214/a-simple-spell-checker-built-from-word-vectors-9f28452b6f26">https://medium.com/@erushton214/a-simple-spell-checker-built-from-word-vectors-9f28452b6f26</a></li>
                    </ul></li>
                    <li><a href="https://www.datasciencecentral.com/profiles/blogs/crisp-dm-a-standard-methodology-to-ensure-a-good-outcome">https://www.datasciencecentral.com/profiles/blogs/crisp-dm-a-standard-methodology-to-ensure-a-good-outcome</a></li>
                    <li><p><a href="http://www.inference.vc/untitled/">ML beyond Curve Fitting: An Intro to Causal Inference and do-Calculus</a></p></li>
                    </ul></li>
                    <li>deepmind wavenet</li>
                    <li><a href="https://openreview.net/pdf?id=ByldLrqlx">deepcoder: learning to write programs</a></li>
                    <li><p>Ramblings, opinions, guesses, hypotheses, conjectures, speculations</p>
                    <ul>
                    <li>AI is approximation (or constrained optimization?) in Sobolev spaces (or ( L^p() ) spaces?)?</li>
                    <li>Intelligent agents are only possible if the world they live in is structured. If the laws of physics randomly change over time, then intelligent agents are unlikely.</li>
                    <li><p>We should merge machine learning, probability, and statistics?</p>
                    <ul>
                    <li><a href="http://en.wikipedia.org/wiki/Recursive_self_improvement">WP:Recursive self-improvement</a></li>
                    </ul></li>
                    <li><p>World = agent + environment. Environment is everything that the agent does not control directly. The body of an agent is part of the environment, not of the agent.</p></li>
                    </ul></li>
                    <li><a href="http://dl.acm.org/citation.cfm?id=2567715">Dimension independent similarity computation (DISCO)</a></li>
                    <li><a href="http://www.jair.org/">Journal of artificial intelligence research</a> (open access)</li>
                    <li><a href="https://arxiv.org/abs/1802.08195">Adversarial Examples that Fool both Human and Computer Vision</a>, from <a href="https://www.youtube.com/watch?v=AbxPbfODGcs">two minute papers 241</a>.</li>
                    <li><a href="https://www.semanticscholar.org/paper/Machine-Theory-of-Mind-Rabinowitz-Perbet/4a48d7528bf1f81f48be8a644ffb1bcc08f1b2c5">Machine theory of mind</a></li>
                    <li>Ilias Diakonikolas, Daniel Kane and Alistair Stewart. Optimal Learning via the Fourier Transform for Sums of Independent Integer Random Variables</li>
                    <li><a href="https://en.m.wikipedia.org/wiki/List_of_important_publications_in_computer_science#Machine_learning">https://en.m.wikipedia.org/wiki/List_of_important_publications_in_computer_science#Machine_learning</a></li>
                    <li><a href="https://arxiv.org/abs/1704.07441">Detecting English Writing Styles For Non Native Speakers</a></li>
                    <li>&quot;Hicklin envisaged that learning resulted from a dynamic equilibrium between information acquisition and loss.&quot; (<a href="https://onlinelibrary.wiley.com/doi/pdf/10.1002/tea.3660210910">Mathematical modeling of learning, Peter F. W. Preece</a>, 1984)</li>
                    <li>AI research tries to make a system that can optimize a wide variety of goal functions?</li>
                    <li><a href="https://cs.nyu.edu/~mohri/mlbook/">Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar; book; &quot;Foundations of machine learning&quot;</a></li>
                    <li><a href="http://bigthink.com/videos/the-top-3-supplements-for-surviving-the-singularity">http://bigthink.com/videos/the-top-3-supplements-for-surviving-the-singularity</a></li>
                    <li><a href="https://google.github.io/CausalImpact/CausalImpact.html">https://google.github.io/CausalImpact/CausalImpact.html</a></li>
                    <li><p>intelligence testing</p>
                    <ul>
                    <li><p><a href="https://www.youtube.com/watch?v=8YWjSQHfV5U">YT:Jordan Peterson - Example IQ questions and what Career/job fits your IQ</a></p>
                    <ul>
                    <li>problem: no job for people with IQ below 87?</li>
                    <li><a href="https://www.reddit.com/r/JordanPeterson/comments/84qmsj/source_of_83_iq_minimum_for_the_us_military/">R:source for soldier minimum IQ requirement of 85</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Fluid_and_crystallized_intelligence">WP:Fluid and crystallized intelligence</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Raven%27s_Progressive_Matrices">WP:Raven's progressive matrices</a> is a language-neutral visual test for fluid intelligence?</li>
                    </ul></li>
                    </ul></li>
                    <li><a href="https://www.youtube.com/watch?v=GdTBqBnqhaQ">YT:4 Experiments Where the AI Outsmarted Its Creators | Two Minute Papers #242</a></li>
                    <li><a href="https://arxiv.org/abs/1509.06569">Tensorizing Neural Networks</a></li>
                    <li><a href="https://arxiv.org/abs/1502.02367">Gated Feedback Recurrent Neural Networks</a></li>
                    <li>no information <a href="http://syntience.com/">http://syntience.com/</a></li>
                    <li><p><a href="https://www.youtube.com/watch?v=b_6-iVz1R0o">The pattern behind self-deception | Michael Shermer</a>: patternicity, agenticity, pattern over-recognition, false positive, false negative</p>
                    <ul>
                    <li>&quot;false positive&quot; is a much better name than &quot;type 1 error&quot;</li>
                    </ul></li>
                    <li>expected 2018, draft book, &quot;Model-based machine learning&quot;, <a href="http://www.mbmlbook.com/">html</a></li>
                    <li><p>vision (making machines see)</p>
                    <ul>
                    <li>Jim Bednar, <a href="http://homepages.inf.ed.ac.uk/jbednar/demos.html">Orientation Perception Demos</a></li>
                    </ul></li>
                    <li><a href="https://en.wikipedia.org/wiki/Bayesian_approaches_to_brain_function">https://en.wikipedia.org/wiki/Bayesian_approaches_to_brain_function</a></li>
                    <li><p><a href="https://www.youtube.com/watch?v=MvFABFWPBrw">DeepMind Has A Superhuman Level Quake 3 AI Team - YouTube</a></p>
                    <ul>
                    <li>Moby Motion's comment: &quot;Really exciting because of the sparse internal rewards and long term planning. A step towards AI agents that are useful in real life.&quot;</li>
                    </ul></li>
                    <li><p>2018 AI is like autistic savants. They perform one task exceptionally well, but they are bad at everything else.</p>
                    <ul>
                    <li>2018, <a href="https://www.youtube.com/watch?v=eSaShQbUJTQ">DeepMind's AI Takes An IQ Test - YouTube</a></li>
                    </ul></li>
                    <li><p>AI</p>
                    <ul>
                    <li>2007, article, &quot;Self-taught Learning: Transfer Learning from Unlabeled Data&quot;, <a href="https://cs.stanford.edu/people/ang/papers/icml07-selftaughtlearning.pdf">pdf</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Category:Open-source_artificial_intelligence">https://en.wikipedia.org/wiki/Category:Open-source_artificial_intelligence</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Commonsense_knowledge_(artificial_intelligence)">https://en.wikipedia.org/wiki/Commonsense_knowledge_(artificial_intelligence)</a></li>
                    <li>2010, article, <a href="https://news.mit.edu/2010/ai-unification">A grand unified theory of AI - MIT News</a></li>
                    <li>2016, article, <a href="https://ai100.stanford.edu/2016-report/section-i-what-artificial-intelligence/ai-research-trends">AI Research Trends - One Hundred Year Study on Artificial Intelligence (AI100)</a></li>
                    <li><p>sequence learning?</p>
                    <ul>
                    <li><a href="https://devblogs.nvidia.com/deep-learning-nutshell-sequence-learning/">https://devblogs.nvidia.com/deep-learning-nutshell-sequence-learning/</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Sequence_learning">https://en.wikipedia.org/wiki/Sequence_learning</a></li>
                    </ul></li>
                    <li><p>AI perception of time?</p></li>
                    </ul></li>
                    <li><p><a href="https://www.quora.com/Does-the-human-brain-have-an-internal-language">https://www.quora.com/Does-the-human-brain-have-an-internal-language</a></p>
                    <ul>
                    <li>mereological fallacy, confusing the part and the whole</li>
                    </ul></li>
                    <li><a href="https://www.quora.com/Is-the-human-brain-analog-or-digital">https://www.quora.com/Is-the-human-brain-analog-or-digital</a> <a href="https://en.wikipedia.org/wiki/Mereological_essentialism">https://en.wikipedia.org/wiki/Mereological_essentialism</a></li>
                    <li><p>machine learning</p>
                    <ul>
                    <li><a href="https://github.com/Avik-Jain/100-Days-Of-ML-Code">Avik-Jain/100-Days-Of-ML-Code: 100 Days of ML Coding</a></li>
                    </ul></li>
                    <li><p>Justifying consciousness using evolution?</p>
                    <ul>
                    <li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4122207/">The biological function of consciousness</a></li>
                    <li><a href="https://www.quora.com/How-does-sentience-benefit-survival-and-why-is-it-developed">How does sentience benefit survival and why is it developed? - Quora</a></li>
                    </ul></li>
                    <li><a href="https://www.quora.com/How-do-I-publish-artificial-intelligence-research-if-I-am-not-currently-in-academia-or-an-industry-research-setting">https://www.quora.com/How-do-I-publish-artificial-intelligence-research-if-I-am-not-currently-in-academia-or-an-industry-research-setting</a></li>
                    <li><a href="https://www.quora.com/How-does-life-fight-against-entropy">How does life fight against entropy? - Quora</a></li>
                    <li><p>Life and entropy</p>
                    <ul>
                    <li><a href="https://www.quora.com/How-does-life-fight-against-entropy">How does life fight against entropy? - Quora</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Entropy_and_life">WP:Entropy and life</a></li>
                    </ul></li>
                    <li><p>Making machine understand human languages</p>
                    <ul>
                    <li><a href="https://blogs.microsoft.com/ai/microsoft-creates-ai-can-read-document-answer-questions-well-person/">Microsoft creates AI that can read a document and answer questions about it as well as a person - The AI Blog</a></li>
                    </ul></li>
                    <li><a href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html">A (Long) Peek into Reinforcement Learning</a></li>
                    <li><p>Competitions</p>
                    <ul>
                    <li>Kaggle: get paid to solve machine learning problems.</li>
                    </ul></li>
                    <li>HLearn: a machine learning library for Haskell <span class="citation" data-cites="izbicki2013hlearn">[<a href="#ref-izbicki2013hlearn">15</a>]</span></li>
                    <li><a href="https://dzone.com/articles/deep-dive-into-machine-learning">Deep Dive Into Machine Learning - DZone AI</a></li>
                    <li><a href="https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf">https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf</a></li>
                    <li><a href="https://github.com/keras-team/keras">keras-team/keras: Deep Learning for humans</a></li>
                    <li><a href="http://cs230.stanford.edu/proj-spring-2018.html">CS230: Deep Learning - Projects</a></li>
                    <li><a href="http://jonbho.net/2014/09/25/defining-intelligence/">http://jonbho.net/2014/09/25/defining-intelligence/</a></li>
                    <li><a href="https://github.com/HuwCampbell/grenade">HuwCampbell/grenade: Deep Learning in Haskell</a></li>
                    <li><a href="http://www.randomhacks.net/2007/03/03/smart-classification-with-haskell/">Smart classification using Bayesian monads in Haskell - Random Hacks</a></li>
                    </ul>
                    <h3 id="where-is-more-information"><span class="section_number">9.10</span><span class="section_title">Where is more information?</span></h3>
                    <ul>
                    <li><a href="https://en.wikipedia.org/wiki/Artificial_intelligence">Wikipedia: Artificial intelligence</a></li>
                    </ul>
                    <h3 id="where-are-new-results-announced"><span class="section_number">9.11</span><span class="section_title">Where are new results announced?</span></h3>
                    <ul>
                    <li><a href="https://en.m.wikipedia.org/wiki/Portal:Artificial_intelligence">Wikipedia AI Portal</a></li>
                    <li>Reddit <a href="https://www.reddit.com/r/artificial/">/r/artificial</a></li>
                    </ul>
                    <h3 id="people"><span class="section_number">9.12</span><span class="section_title">People??</span></h3>
                    <div class="local_table_of_contents">
                    <ul>
                    <li><span class="section_number">9.12.1</span><span class="section_title"><a href="#who-are-aiml-researchers-what-do-they-focus-on-and-what-are-they-doing">Who are AI/ML researchers, what do they focus on, and what are they doing?</a></span><span class="word_count">(220w~2m)</span></li>
                    <li><span class="section_number">9.12.2</span><span class="section_title"><a href="#what-is-the-best-place-to-do-ai-research">What is the best place to do AI research?</a></span><span class="word_count">(10w~1m)</span></li>
                    </ul>
                    </div>
                    <h4 id="who-are-aiml-researchers-what-do-they-focus-on-and-what-are-they-doing"><span class="section_number">9.12.1</span><span class="section_title">Who are AI/ML researchers, what do they focus on, and what are they doing?</span></h4>
                    <p><a href="https://en.wikipedia.org/wiki/Portal:Artificial_intelligence">WP AI Portal</a> lists several leading AI researchers.</p>
                    <p>Does Geoffrey Hinton specialize in image recognition?</p>
                    <p>Who are the researchers?</p>
                    <ul>
                    <li>See also <a href="https://www.quora.com/Who-is-leading-in-AI-research-among-big-players-like-IBM-Google-Facebook-Apple-and-Microsoft">Quora: Who is leading in AI research among big players like IBM, Google, Facebook, Apple, and Microsoft?</a>
                    <ul>
                    <li>Google Brain, OpenAI, FAIR (Facebook AI Research), Microsoft Research, IBM Research</li>
                    </ul></li>
                    <li>Geoffrey Hinton, <a href="http://www.cs.toronto.edu/~hinton/">UToronto page</a>, <a href="https://www.reddit.com/r/MachineLearning/comments/2lmo0l/ama_geoffrey_hinton/">Reddit AMA</a>, <a href="https://www.semanticscholar.org/author/Geoffrey-E.-Hinton/1695689">Semantic Scholar influence graph</a>
                    <ul>
                    <li>He is trying to find out how the brain works.</li>
                    <li>The idea: If a learning algorithm works on machines, then it might have something to do with how brains work.</li>
                    <li>More interested in physical explanation of how the brain works. Physics first, math second, although his math is OK.</li>
                    </ul></li>
                    <li>Yann LeCun</li>
                    <li>Jürgen Schmidhuber</li>
                    <li>Pedro Domingos</li>
                    <li>Demis Hassabis
                    <ul>
                    <li>What is his focus?</li>
                    </ul></li>
                    <li>Pamela McCorduck, AI historian
                    <ul>
                    <li>2004 anniversary edition of her 1979 book <a href="http://www.pamelamc.com/html/machines_who_think.html">&quot;Machines who think&quot;</a></li>
                    </ul></li>
                    <li>Who else? There are lots of people.</li>
                    </ul>
                    <p>How is <a href="https://homes.cs.washington.edu/~pedrod/">Pedro Domingos</a>'s progress of finding the master algorithm unifying the five tribes?</p>
                    <ul>
                    <li>Markov logic network unifies probabilists and logicians.
                    <ul>
                    <li>How about the other three tribes?</li>
                    </ul></li>
                    <li>Hume's question: How do we justify generalization? Why does generalization work?
                    <ul>
                    <li>Does Wolpert answer that in &quot;no free lunch theorem&quot;?
                    <ul>
                    <li><a href="https://en.wikipedia.org/wiki/No_free_lunch_theorem">Wikipedia: No free lunch theorem</a></li>
                    </ul></li>
                    <li>I think induction works because our Universe happens to have a structure that is amenable to induction.
                    <ul>
                    <li>If induction doesn't work, and evolution is true, then we would have gone extinct long ago, wouldn't we?
                    <ul>
                    <li>What structure is that?</li>
                    </ul></li>
                    </ul></li>
                    </ul></li>
                    </ul>
                    <h4 id="what-is-the-best-place-to-do-ai-research"><span class="section_number">9.12.2</span><span class="section_title">What is the best place to do AI research?</span></h4>
                    <p>Swiss? IDSIA?</p>
                    <h3 id="how-can-i-become-an-ai-researcher"><span class="section_number">9.13</span><span class="section_title">How can I become an AI researcher?</span></h3>
                    <p>Where do I begin? How do I begin?</p>
                    <h3 id="university-courses"><span class="section_number">9.14</span><span class="section_title">University courses</span></h3>
                    <p>For a course with computer science background, see Stanford University CS221 (Artificial Intelligence: Principles and Techniques) Autumn 2016 <span class="citation" data-cites="LiangCs221">[<a href="#ref-LiangCs221">18</a>]</span>. For a course with mathematics background, see Massachusetts Institute of Technology 18.657 (Mathematics of Machine Learning) Fall 2015 <span class="citation" data-cites="rigollet2015ocw">[<a href="#ref-rigollet2015ocw">26</a>]</span>.</p>
                    <h3 id="other-resources"><span class="section_number">9.15</span><span class="section_title">Other resources</span></h3>
                    <p>Corpuses, datasets, training sets: MNIST handwritten digit dataset.</p>
                    <p>OpenAI. Let an AI learn in an accurate-enough physical simulation, then move it into the real world.</p>
                    <p>OpenCog <a href="http://opencog.org/about/">http://opencog.org/about/</a></p>
                    <h3 id="clean-up-oldindex.xml"><span class="section_number">9.16</span><span class="section_title"><span class="todo TODO">TODO</span> clean up oldindex.xml</span></h3>
                    <p><a href="https://medium.com/deeper-learning/a-glossary-of-deep-learning-9cb6292e087e">https://medium.com/deeper-learning/a-glossary-of-deep-learning-9cb6292e087e</a></p>
                    <p>Lecture 2 of CS221: Artificial Intelligence: Principles and Techniques</p>
                    <p>Neural network <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">https://en.wikipedia.org/wiki/Universal_approximation_theorem</a></p>
                    <p>Create an AI for automatically finding data from the Internet? Machine-aided human summarization (MAHS) Human-aided machine summarization (HAMS) <a href="https://en.wikipedia.org/wiki/Automatic_summarization">https://en.wikipedia.org/wiki/Automatic_summarization</a></p>
                    <p>Stanford Autumn 2016</p>
                    <p>Machine Learning, Tom Mitchell, McGraw-Hill <a href="http://cs229.stanford.edu/">http://cs229.stanford.edu/</a></p>
                    <p><a href="http://www.cs.cmu.edu/~tom/mlbook-chapter-slides.html">http://www.cs.cmu.edu/~tom/mlbook-chapter-slides.html</a></p>
                    <p>Undergraduate Computer Science point of view <a href="https://www.cs.princeton.edu/courses/archive/fall16/cos402/">https://www.cs.princeton.edu/courses/archive/fall16/cos402/</a></p>
                    <p>Graduate <a href="http://www.cs.cmu.edu/afs/cs/Web/People/15780/">http://www.cs.cmu.edu/afs/cs/Web/People/15780/</a></p>
                    <p><a href="http://homes.cs.washington.edu/~pedrod/">http://homes.cs.washington.edu/~pedrod/</a></p>
                    <p>Metric Learning: A Survey <a href="http://web.cse.ohio-state.edu/~kulis/pubs/ftml_metric_learning.pdf">http://web.cse.ohio-state.edu/~kulis/pubs/ftml_metric_learning.pdf</a></p>
                    <p>Distance Metric Learning: A Comprehensive Survey <a href="https://www.cs.cmu.edu/~liuy/frame_survey_v2.pdf">https://www.cs.cmu.edu/~liuy/frame_survey_v2.pdf</a></p>
                    <p>Learning Deep Architectures for AI <a href="http://www.iro.umontreal.ca/~bengioy/papers/ftml_book.pdf">http://www.iro.umontreal.ca/~bengioy/papers/ftml_book.pdf</a></p>
                    <p><a href="https://en.wikipedia.org/wiki/Similarity_learning">https://en.wikipedia.org/wiki/Similarity_learning</a></p>
                    <p>Essentials of Machine Learning Algorithms (with Python and R Codes) <a href="https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/">https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/</a></p>
                    <p><a href="http://www.cs.cmu.edu/">http://www.cs.cmu.edu/</a>~./15381/</p>
                    <p><a href="http://stanford.edu/~cpiech/cs221/">http://stanford.edu/~cpiech/cs221/</a></p>
                    <p><a href="http://www.cs.princeton.edu/courses/archive/fall15/cos402/">http://www.cs.princeton.edu/courses/archive/fall15/cos402/</a></p>
                    <p><a href="https://grid.cs.gsu.edu/~cscyqz/courses/ai/aiLectures.html">https://grid.cs.gsu.edu/~cscyqz/courses/ai/aiLectures.html</a></p>
                    <p><a href="https://www.cs.utexas.edu/users/novak/cs381kcontents.html">https://www.cs.utexas.edu/users/novak/cs381kcontents.html</a></p>
                    <p><a href="https://www.cs.utexas.edu/users/novak/cs343index.html">https://www.cs.utexas.edu/users/novak/cs343index.html</a></p>
                    <p><a href="http://www.cse.unsw.edu.au/~billw/cs9414/notes.html">http://www.cse.unsw.edu.au/~billw/cs9414/notes.html</a></p>
                    <p>Why deep learning works</p>
                    <p><a href="http://www.vision.jhu.edu/tutorials/ICCV15-Tutorial-Math-Deep-Learning-Intro-Rene-Joan.pdf">http://www.vision.jhu.edu/tutorials/ICCV15-Tutorial-Math-Deep-Learning-Intro-Rene-Joan.pdf</a></p>
                    <p><a href="http://www.vision.jhu.edu/tutorials/ICCV15-Tutorial-Math-Deep-Learning.htm">http://www.vision.jhu.edu/tutorials/ICCV15-Tutorial-Math-Deep-Learning.htm</a></p>
                    <p><a href="https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/">https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/</a></p>
                    <p>Neural Networks, Manifolds, and Topology <a href="http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/">http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/</a></p>
                    <p>Deep Learning, NLP, and Representations <a href="http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/">http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/</a></p>
                    <p>Machine Learning A Probabilistic Perspective Kevin P. Murphy Table of Contents <a href="http://www.cs.ubc.ca/~murphyk/MLbook/pml-toc-22may12.pdf">http://www.cs.ubc.ca/~murphyk/MLbook/pml-toc-22may12.pdf</a></p>
                    <p>The following is a list of free, open source books on machine learning, statistics, data-mining, etc. <a href="https://github.com/josephmisiti/awesome-machine-learning/blob/master/books.md">https://github.com/josephmisiti/awesome-machine-learning/blob/master/books.md</a></p>
                    <h3 id="fields-of-study-related-to-intelligence"><span class="section_number">9.17</span><span class="section_title"><span class="todo TODO">TODO</span> Fields of study related to intelligence</span></h3>
                    <p>AI is about making something that is as intelligent as a human brain without caring about how human brain works. Cognitive neuroscience is about how a human brain works.</p>
                    <div class="local_table_of_contents">
                    <ul>
                    <li><span class="section_number">9.17.1</span><span class="section_title"><a href="#connectionism">Connectionism</a></span><span class="word_count">(1w~1m)</span></li>
                    <li><span class="section_number">9.17.2</span><span class="section_title"><a href="#brain-is-a-vector-function">Brain is a vector function</a></span><span class="word_count">(5w~1m)</span></li>
                    <li><span class="section_number">9.17.3</span><span class="section_title"><a href="#machine-learning-1">Machine learning</a></span><span class="word_count">(9w~1m)</span></li>
                    <li><span class="section_number">9.17.4</span><span class="section_title"><a href="#connectionism-1">Connectionism</a></span><span class="word_count">(1w~1m)</span></li>
                    <li><span class="section_number">9.17.5</span><span class="section_title"><a href="#computational-neuroscience">Computational neuroscience</a></span><span class="word_count">(2w~1m)</span></li>
                    <li><span class="section_number">9.17.6</span><span class="section_title"><a href="#cognitive-neuroscience">Cognitive neuroscience</a></span><span class="word_count">(69w~1m)</span></li>
                    <li><span class="section_number">9.17.7</span><span class="section_title"><a href="#imagination-is-as-real-as-perception">Imagination is as real as perception</a></span><span class="word_count">(63w~1m)</span></li>
                    <li><span class="section_number">9.17.8</span><span class="section_title"><a href="#materials-looking-for-a-place-to-belong">Materials looking for a place to belong</a></span><span class="word_count">(124w~1m)</span></li>
                    <li><span class="section_number">9.17.9</span><span class="section_title"><a href="#genetic-algorithm">Genetic algorithm</a></span><span class="word_count">(147w~1m)</span></li>
                    <li><span class="section_number">9.17.10</span><span class="section_title"><a href="#draft">Draft</a></span><span class="word_count">(65w~1m)</span></li>
                    <li><span class="section_number">9.17.11</span><span class="section_title"><a href="#how-do-we-determine-how-intelligent-something-is">How do we determine how intelligent something is?</a></span><span class="word_count">(18w~1m)</span></li>
                    <li><span class="section_number">9.17.12</span><span class="section_title"><a href="#the-brain-at-a-time-is-a-big-array-function.">The brain at a time is a big array function.</a></span><span class="word_count">(23w~1m)</span></li>
                    <li><span class="section_number">9.17.13</span><span class="section_title"><a href="#control-needs-feedback.">Control needs feedback.</a></span><span class="word_count">(14w~1m)</span></li>
                    <li><span class="section_number">9.17.14</span><span class="section_title"><a href="#consciousness">Consciousness</a></span><span class="word_count">(79w~1m)</span></li>
                    <li><span class="section_number">9.17.15</span><span class="section_title"><a href="#intelligence-is-a-spectrum.">Intelligence is a spectrum.</a></span><span class="word_count">(64w~1m)</span></li>
                    <li><span class="section_number">9.17.16</span><span class="section_title"><a href="#intelligence-is-control.">Intelligence is control.</a></span><span class="word_count">(12w~1m)</span></li>
                    <li><span class="section_number">9.17.17</span><span class="section_title"><a href="#intelligence-is-relative.">Intelligence is relative.</a></span><span class="word_count">(10w~1m)</span></li>
                    <li><span class="section_number">9.17.18</span><span class="section_title"><a href="#intelligence-needs-the-ability-to-adapt.">Intelligence needs the ability to adapt.</a></span><span class="word_count">(6w~1m)</span></li>
                    <li><span class="section_number">9.17.19</span><span class="section_title"><a href="#every-software-system-is-a-state-machine.">Every software system is a state machine.</a></span><span class="word_count">(7w~1m)</span></li>
                    <li><span class="section_number">9.17.20</span><span class="section_title"><a href="#currys-y-combinator-makes-a-fixed-point-equation">Curry's Y combinator makes a fixed point equation</a></span><span class="word_count">(8w~1m)</span></li>
                    <li><span class="section_number">9.17.21</span><span class="section_title"><a href="#what-are-the-limits-of-intelligence">What are the limits of intelligence?</a></span><span class="word_count">(6w~1m)</span></li>
                    <li><span class="section_number">9.17.22</span><span class="section_title"><a href="#phase-space-learning">Phase-space learning?</a></span><span class="word_count">(75w~1m)</span></li>
                    <li><span class="section_number">9.17.23</span><span class="section_title"><a href="#what-are-the-ways-of-describing-a-system">What are the ways of describing a system?</a></span><span class="word_count">(34w~1m)</span></li>
                    <li><span class="section_number">9.17.24</span><span class="section_title"><a href="#what-fields-does-this-book-depend-on">What fields does this book depend on?</a></span><span class="word_count">(52w~1m)</span></li>
                    <li><span class="section_number">9.17.25</span><span class="section_title"><a href="#supervised-to-unsupervised">Supervised to unsupervised</a></span><span class="word_count">(15w~1m)</span></li>
                    <li><span class="section_number">9.17.26</span><span class="section_title"><a href="#approximation-to-optimization">Approximation to optimization</a></span><span class="word_count">(13w~1m)</span></li>
                    <li><span class="section_number">9.17.27</span><span class="section_title"><a href="#optimal-clustering">Optimal clustering</a></span><span class="word_count">(11w~1m)</span></li>
                    <li><span class="section_number">9.17.28</span><span class="section_title"><a href="#optimal-approximation">Optimal approximation</a></span><span class="word_count">(65w~1m)</span></li>
                    <li><span class="section_number">9.17.29</span><span class="section_title"><a href="#meta-approximation">Meta-approximation</a></span><span class="word_count">(63w~1m)</span></li>
                    <li><span class="section_number">9.17.30</span><span class="section_title"><a href="#chemotaxis-is-an-example-of-optimization.">Chemotaxis is an example of optimization.</a></span><span class="word_count">(34w~1m)</span></li>
                    </ul>
                    </div>
                    <h4 id="connectionism"><span class="section_number">9.17.1</span><span class="section_title">Connectionism</span></h4>
                    <h4 id="brain-is-a-vector-function"><span class="section_number">9.17.2</span><span class="section_title">Brain is a vector function</span></h4>
                    <h4 id="machine-learning-1"><span class="section_number">9.17.3</span><span class="section_title">Machine learning</span></h4>
                    <p>Machine learning makes machine do things from examples.</p>
                    <h4 id="connectionism-1"><span class="section_number">9.17.4</span><span class="section_title">Connectionism</span></h4>
                    <h4 id="computational-neuroscience"><span class="section_number">9.17.5</span><span class="section_title">Computational neuroscience</span></h4>
                    <h4 id="cognitive-neuroscience"><span class="section_number">9.17.6</span><span class="section_title">Cognitive neuroscience</span></h4>
                    <p>Cognitive neuroscience tries to understand how brains work. The organism with central nervous system with the fewest neurons is . You can create your own virtual online at <span class="citation" data-cites="openworm">[<a href="#ref-openworm">42</a>]</span>.</p>
                    <p>In rats, neuron firing rate encodes posterior probability (expected value)? (Cite?)</p>
                    <p>Neural coding tries to find out how neurons encode information. Are neurons digital, analog, or both? Spike train?</p>
                    <p>Digit-recognizing neural network performs generalization/induction.</p>
                    <p>Decoding mental states from brain activity in humans <span class="citation" data-cites="haynes2006decoding">[<span class="citeproc-not-found" data-reference-id="haynes2006decoding"><strong>???</strong></span>]</span></p>
                    <h4 id="imagination-is-as-real-as-perception"><span class="section_number">9.17.7</span><span class="section_title">Imagination is as real as perception</span></h4>
                    <p>Imagining a thing excites the same neurons as perceiving that thing. Therefore if we have a very good mental model, we should be able to perform experiments in our imagination and translate the results to the real world.</p>
                    <p>Imagine that an intelligent machine existed, and then work our way back. Invent a story about how we would get there.</p>
                    <h4 id="materials-looking-for-a-place-to-belong"><span class="section_number">9.17.8</span><span class="section_title">Materials looking for a place to belong</span></h4>
                    <p>Logic has <em>syntax</em> (form) and <em>semantics</em> (meaning). Grammar determines the <em>well-formed formulas</em>. Semantics maps a well-formed formulas to an <em>interpretation</em>. (What are the terms? Mathematical logic lecture notes or book?)</p>
                    <p>The <em>extension</em> of a predicate <span class="math inline">\(p\)</span> is <span class="math inline">\(\{x~|~p(x)\}\)</span>.</p>
                    <p>A formula in first-order logic is <em>Skolemized</em> or is in <em>Skolem normal form</em> iff it has the form <span class="math inline">\(\forall x_1 \ldots \forall x_n ~ M\)</span> where <span class="math inline">\(M\)</span> is a quantifier-free formula in conjunctive normal form. A formula is in <em>conjunctive normal form</em> iff … <a href="http://mathworld.wolfram.com/SkolemizedForm.html">http://mathworld.wolfram.com/SkolemizedForm.html</a></p>
                    <p>A <em>Herbrand universe</em> is … An <em>interpretation</em> is … A <em>formal system</em> is … A <em>formal language</em> is …</p>
                    <p><em>Curry-Howard correspondence</em> relates logic and type. A value <span class="math inline">\(x : T\)</span> is a proof the logic formula isomorphic to <span class="math inline">\(T\)</span>.</p>
                    <h4 id="genetic-algorithm"><span class="section_number">9.17.9</span><span class="section_title">Genetic algorithm</span></h4>
                    <p>A <em>genetic algorithm</em> is an iterated randomized mixing filtering optimization. Generalized genetic algorithm: Let <span class="math inline">\(\fun{Pop}\)</span> be the population type. Let <span class="math inline">\(t : \Nat\)</span> be time. Let <span class="math inline">\(\fun{pop}~t : \fun{Pop}\)</span> be the population at time <span class="math inline">\(t\)</span>. Let <span class="math inline">\(\fun{fit}: \fun{Pop}\to \fun{Pop}\)</span> be the fitness filter a.k.a. selection function a.k.a. selection pressure function. Let <span class="math inline">\(\fun{mate}: \fun{Pop}\to \fun{Pop}\to \fun{Pop}\)</span> be the next-population function, including mutation, birth, death, mating. Let <span class="math inline">\(\fun{sur}~(t+1) = \fun{fit}~(\fun{pop}~t)\)</span> be the survivor set at time <span class="math inline">\(t+1\)</span>. The algorithm is the equation <span class="math inline">\(\forall t \in \Nat : \fun{pop}~(t+1) = \fun{sur}~(t+1) + \fun{mate}~(\fun{pop}~t)\)</span>. Observe the sequence of populations <span class="math inline">\(\fun{pop}~0, \ldots, \fun{pop}~t\)</span>. A genetic algorithm, an iterated search algorithm, is a mono-unary algebra. Genetic algorithm is like tree search. The mating function is the fringe function. A genetic algorithm is a stochastic process. A genetic algorithm takes a filtering and mating algorithm and produces a search algorithm.</p>
                    <p>Simulated annealing.</p>
                    <p>Randomized search algorithm.</p>
                    <h4 id="draft"><span class="section_number">9.17.10</span><span class="section_title">Draft</span></h4>
                    <p>Is a company, which consists of undoubtedly intelligent people, intelligent?</p>
                    <p>Alan Turing proposed the Turing test.</p>
                    <p>Intelligence is what intelligence tests measure.</p>
                    <p>I think we use the word 'intelligence' to refer to a stabilizing behavior that is complex enough to elude a simple explanation.</p>
                    <p>I think we agree that we are intelligent.</p>
                    <p>We cannot know if something is intrinsically intelligent. We can only determine intelligence from what we can observe.</p>
                    <h4 id="how-do-we-determine-how-intelligent-something-is"><span class="section_number">9.17.11</span><span class="section_title">How do we determine how intelligent something is?</span></h4>
                    <p>An intelligent being may elude detection by pretending to be unintelligent.</p>
                    <h4 id="the-brain-at-a-time-is-a-big-array-function."><span class="section_number">9.17.12</span><span class="section_title">The brain at a time is a big array function.</span></h4>
                    <p>Can we formulate it in a way that does not depend on linear time?</p>
                    <h4 id="control-needs-feedback."><span class="section_number">9.17.13</span><span class="section_title">Control needs feedback.</span></h4>
                    <p>There is also open-loop or feed-forward control, but complex control needs feedback.</p>
                    <h4 id="consciousness"><span class="section_number">9.17.14</span><span class="section_title">Consciousness</span></h4>
                    <p>Consciousness needs sensory input.</p>
                    <p>Consciousness needs feedback.</p>
                    <p>Self concept needs feedback.</p>
                    <p>If there is not a feedback, a system cannot distinguish itself from its environment. The self concept will never arise.</p>
                    <p>If a brain can immediately control a thing, then that thing is part of the brain's self concept. If the brain can't, it's not.</p>
                    <p>If a brain often gets certain input shortly after it produces certain output, it will associate the output with its self concept.</p>
                    <p>The self is the thing under conscious control.</p>
                    <h4 id="intelligence-is-a-spectrum."><span class="section_number">9.17.15</span><span class="section_title">Intelligence is a spectrum.</span></h4>
                    <p>Is a human intelligent?</p>
                    <p>Is a rock intelligent?</p>
                    <p>A human is more intelligent than a rock.</p>
                    <p>Is a human pretending to be a rock intelligent?</p>
                    <p>Can an intelligent system look non-intelligent (hide its intelligence)?</p>
                    <p>We can measure intelligence as numbers.</p>
                    <p>Intelligence needs learning.</p>
                    <p>Adapting needs learning.</p>
                    <p>We say X adapts to Y iff Y surprises X less as time goes by. (Whose idea is this?)</p>
                    <p>Intelligence needs state.</p>
                    <p>State needs time.</p>
                    <h4 id="intelligence-is-control."><span class="section_number">9.17.16</span><span class="section_title">Intelligence is control.</span></h4>
                    <p>An intelligent system is a special case of control system.</p>
                    <h4 id="intelligence-is-relative."><span class="section_number">9.17.17</span><span class="section_title">Intelligence is relative.</span></h4>
                    <p>Intelligence relative to something is a real number.</p>
                    <h4 id="intelligence-needs-the-ability-to-adapt."><span class="section_number">9.17.18</span><span class="section_title">Intelligence needs the ability to adapt.</span></h4>
                    <h4 id="every-software-system-is-a-state-machine."><span class="section_number">9.17.19</span><span class="section_title">Every software system is a state machine.</span></h4>
                    <h4 id="currys-y-combinator-makes-a-fixed-point-equation"><span class="section_number">9.17.20</span><span class="section_title">Curry's Y combinator makes a fixed point equation</span></h4>
                    <h4 id="what-are-the-limits-of-intelligence"><span class="section_number">9.17.21</span><span class="section_title">What are the limits of intelligence?</span></h4>
                    <h4 id="phase-space-learning"><span class="section_number">9.17.22</span><span class="section_title">Phase-space learning?</span></h4>
                    <p>There is a boundary: the agent, and the environment. How many functions do we need to model it?</p>
                    <p>One function that is an endofunction of phase space. The agent state is a subspace of that phase space. The environment state is another subspace of that phase space.</p>
                    <p>The idea is to represent the how the phase space changes in a small time. The number of variables should equal to the degree of freedom of the system.</p>
                    <h4 id="what-are-the-ways-of-describing-a-system"><span class="section_number">9.17.23</span><span class="section_title">What are the ways of describing a system?</span></h4>
                    <ul>
                    <li><p>function from time to state</p></li>
                    <li><p>endofunction of phase space</p></li>
                    </ul>
                    <p>State space and phase space are the same. State space is for discrete systems. Phase space is for continuous systems.</p>
                    <h4 id="what-fields-does-this-book-depend-on"><span class="section_number">9.17.24</span><span class="section_title">What fields does this book depend on?</span></h4>
                    <p>Topology <span class="citation" data-cites="Topology">[<a href="#ref-Topology">20</a>]</span></p>
                    <p>Functional analysis</p>
                    <p>Dynamical system</p>
                    <p>Control theory</p>
                    <p>Fixed point theory</p>
                    <p>Neurophysiology</p>
                    <p>Computer science</p>
                    <p>Cybernetics</p>
                    <p><a href="https://en.wikipedia.org/wiki/Connectionism">https://en.wikipedia.org/wiki/Connectionism</a></p>
                    <p><a href="https://en.wikipedia.org/wiki/Cybernetics">https://en.wikipedia.org/wiki/Cybernetics</a></p>
                    <p>Biological neuron model <a href="https://en.wikipedia.org/wiki/Biological_neuron_model">https://en.wikipedia.org/wiki/Biological_neuron_model</a></p>
                    <p>An introduction to mathematical physiology <a href="https://people.maths.ox.ac.uk/fowler/courses/physiol/physiolnotes.pdf">https://people.maths.ox.ac.uk/fowler/courses/physiol/physiolnotes.pdf</a></p>
                    <p>Learning and Transfer of Learning with No Feedback: An Experimental Test Across Games <a href="http://repository.cmu.edu/cgi/viewcontent.cgi?article=1040&amp;context=sds">http://repository.cmu.edu/cgi/viewcontent.cgi?article=1040&amp;context=sds</a></p>
                    <p>Perceptual learning without feedback in non-stationary contexts: Data and model <a href="http://socsci-dev.ss.uci.edu/maplab/webdocs/petrovdosherlu06.pdf">http://socsci-dev.ss.uci.edu/maplab/webdocs/petrovdosherlu06.pdf</a></p>
                    <p>Neural coding <a href="https://en.wikipedia.org/wiki/Neural_coding">https://en.wikipedia.org/wiki/Neural_coding</a></p>
                    <p>Pulse-frequency modulation in brain neurons</p>
                    <p>Reward system</p>
                    <h4 id="supervised-to-unsupervised"><span class="section_number">9.17.25</span><span class="section_title">Supervised to unsupervised</span></h4>
                    <p>Can a supervised learning algorithm always be made into an unsupervised learning algorithm?</p>
                    <h4 id="approximation-to-optimization"><span class="section_number">9.17.26</span><span class="section_title">Approximation to optimization</span></h4>
                    <p>Can an approximation scheme always be made into an optimization scheme?</p>
                    <h4 id="optimal-clustering"><span class="section_number">9.17.27</span><span class="section_title">Optimal clustering</span></h4>
                    <p>Given a set of points, what is the optimal clustering/partition?</p>
                    <h4 id="optimal-approximation"><span class="section_number">9.17.28</span><span class="section_title">Optimal approximation</span></h4>
                    <p>Given a set of points <span class="math inline">\(\{(x_1,y_1),\ldots,(x_n,y_n)\}\)</span> (samples of a function), what is the function that optimally approximates those samples? The approximation error is <span class="math inline">\(\sum_k y_k - f(x_k)\)</span>. Let <span class="math inline">\(F\)</span> be the set of all integrable real-to-real functions. Define <span class="math inline">\(M(f) = \int_\Real f\)</span> as the infinite integral of <span class="math inline">\(f\)</span>. Define the complexity of <span class="math inline">\(f\)</span> as <span class="math inline">\(C(f) = \sum_{k=1}^\infty M(D_k(f))\)</span> where <span class="math inline">\(D\)</span> is the derivative operator.</p>
                    <h4 id="meta-approximation"><span class="section_number">9.17.29</span><span class="section_title">Meta-approximation</span></h4>
                    <p>Given set of points <span class="math inline">\(D = \{(x_1,y_1),\ldots,(x_n,y_n)\}\)</span>, find <span class="math inline">\(g\)</span> that finds <span class="math inline">\(f\)</span> that approximates <span class="math inline">\(D\)</span>.</p>
                    <p>Let <span class="math inline">\(F\)</span> be the set of all real-to-real functions. Can we craft a measure on <span class="math inline">\(F\)</span>? Can we craft a probability measure on <span class="math inline">\(F\)</span>? Can we craft a universal prior for <span class="math inline">\(F\)</span> like Solomonoff did for bitstrings?</p>
                    <p>What is the best way to update the approximator using the approximation error?</p>
                    <h4 id="chemotaxis-is-an-example-of-optimization."><span class="section_number">9.17.30</span><span class="section_title">Chemotaxis is an example of optimization.</span></h4>
                    <p><a href="http://www.mit.edu/~kardar/teaching/projects/chemotaxis(AndreaSchmidt)/finding_food.htm">http://www.mit.edu/~kardar/teaching/projects/chemotaxis(AndreaSchmidt)/finding_food.htm</a> We can model chemotaxis as gradient following (ascent or descent). Andrea Schmidt described chemotaxis as a biased random walk. The bias is the chemical concentration gradient.</p>
                    <p>Chemotaxis is intelligence.</p>
                    <h3 id="wonderings"><span class="section_number">9.18</span><span class="section_title"><span class="todo TODO">TODO</span> Wonderings</span></h3>
                    <div class="local_table_of_contents">
                    <ul>
                    <li><span class="section_number">9.18.1</span><span class="section_title"><a href="#reading-list">Reading list</a></span><span class="word_count">(16w~1m)</span></li>
                    <li><span class="section_number">9.18.2</span><span class="section_title"><a href="#adversarial-random-process">Adversarial random process</a></span><span class="word_count">(13w~1m)</span></li>
                    <li><span class="section_number">9.18.3</span><span class="section_title"><a href="#neural-networks">Neural networks</a></span><span class="word_count">(83w~1m)</span></li>
                    </ul>
                    </div>
                    <h4 id="reading-list"><span class="section_number">9.18.1</span><span class="section_title">Reading list</span></h4>
                    <p>Statistical learning</p>
                    <p>Inverse problem theory <span class="citation" data-cites="tarantola2005inverse">[<a href="#ref-tarantola2005inverse">34</a>]</span></p>
                    <p>? <span class="citation" data-cites="SepLogicAi">[<a href="#ref-SepLogicAi">35</a>]</span></p>
                    <p>Wiener cybernetics book? <span class="citation" data-cites="WienerCyber">[<a href="#ref-WienerCyber">38</a>]</span></p>
                    <p>approximation theory? <span class="citation" data-cites="ApproxThePrac">[<a href="#ref-ApproxThePrac">36</a>]</span></p>
                    <p>Semi-supervised learning?</p>
                    <p>What is rational?</p>
                    <p>Moravec's paradox</p>
                    <h4 id="adversarial-random-process"><span class="section_number">9.18.2</span><span class="section_title">Adversarial random process</span></h4>
                    <p><span class="math inline">\(P\)</span> tries to predict <span class="math inline">\(G\)</span>. <span class="math inline">\(G\)</span> tries to make <span class="math inline">\(P\)</span> wrong.</p>
                    <h4 id="neural-networks"><span class="section_number">9.18.3</span><span class="section_title">Neural networks</span></h4>
                    <p>Neural networks is one architecture that makes machine trainable. Neural network is not necessarily the best architecture for intelligence. Evolution is a greedy optimization algorithm.</p>
                    <p>Topologically, a neural network layer is a continuous map. It transforms the input space into a more separable space. Consider the set of points that satisfy the classifier. This set is a manifold. A neural network layer stretches, rotates, manipulates that manifold. The output wants to be box-shaped. But isn't this just the idea of Kohonen's self-organizing maps?</p>
                    <h3 id="reading-list-1"><span class="section_number">9.19</span><span class="section_title">Reading list?</span></h3>
                    <p>Neural Architecture Search with Reinforcement Learning Barret Zoph, Quoc V. Le <a href="https://arxiv.org/abs/1611.01578">https://arxiv.org/abs/1611.01578</a></p>
                    <p><a href="http://artint.info/html/ArtInt.html">http://artint.info/html/ArtInt.html</a></p>
                    <p><a href="https://en.wikipedia.org/wiki/Book:Machine_Learning_%E2%80%93_The_Complete_Guide">https://en.wikipedia.org/wiki/Book:Machine_Learning_%E2%80%93_The_Complete_Guide</a></p>
                    <p><a href="https://en.wikipedia.org/wiki/Wikipedia:WikiProject_Mathematics/Reference_resources">https://en.wikipedia.org/wiki/Wikipedia:WikiProject_Mathematics/Reference_resources</a></p>
                    <p>An Automatic Clustering Technique for Optimal Clusters <a href="https://arxiv.org/pdf/1109.1068.pdf">https://arxiv.org/pdf/1109.1068.pdf</a></p>
                    <p><a href="https://en.wikipedia.org/wiki/State-Action-Reward-State-Action">https://en.wikipedia.org/wiki/State-Action-Reward-State-Action</a></p>
                    <p><a href="http://blog.minitab.com/blog/adventures-in-statistics-2/understanding-hypothesis-tests:-confidence-intervals-and-confidence-levels">http://blog.minitab.com/blog/adventures-in-statistics-2/understanding-hypothesis-tests:-confidence-intervals-and-confidence-levels</a></p>
                    <p><a href="http://greenteapress.com/thinkstats2/html/index.html">http://greenteapress.com/thinkstats2/html/index.html</a></p>
                    <p><a href="https://elitedatascience.com/learn-machine-learning">https://elitedatascience.com/learn-machine-learning</a></p>
                    <p><a href="http://www.mit.edu/~9.520/fall14/Classes/mtheory.html">http://www.mit.edu/~9.520/fall14/Classes/mtheory.html</a></p>
                    <p><a href="https://arxiv.org/pdf/1311.4158v5.pdf">https://arxiv.org/pdf/1311.4158v5.pdf</a></p>
                    <p>Unsupervised learning of invariant representations with low sample complexity: the magic of sensory cortex or a new framework for machine learning?</p>
                    <p><a href="http://www.stat.yale.edu/Courses/1997-98/101/confint.htm">http://www.stat.yale.edu/Courses/1997-98/101/confint.htm</a></p>
                    <p><a href="http://www.itl.nist.gov/div898/handbook/prc/section1/prc14.htm">http://www.itl.nist.gov/div898/handbook/prc/section1/prc14.htm</a></p>
                    <div class="local_table_of_contents">
                    <ul>
                    <li><span class="section_number">9.19.1</span><span class="section_title"><a href="#machine-learning-2">Machine learning</a></span><span class="word_count">(97w~1m)</span></li>
                    <li><span class="section_number">9.19.2</span><span class="section_title"><a href="#popular-science">Popular science</a></span><span class="word_count">(15w~1m)</span></li>
                    </ul>
                    </div>
                    <h4 id="machine-learning-2"><span class="section_number">9.19.1</span><span class="section_title">Machine learning</span></h4>
                    <p>Algorithmic Aspects of Machine Learning Matrices <a href="http://people.csail.mit.edu/moitra/docs/bookex.pdf">http://people.csail.mit.edu/moitra/docs/bookex.pdf</a></p>
                    <p><a href="http://www.deeplearningbook.org/">http://www.deeplearningbook.org/</a></p>
                    <p>Pedro Domingos: &quot;The Master Algorithm&quot; | Talks at Google - YouTube <a href="https://www.youtube.com/watch?v=B8J4uefCQMc">https://www.youtube.com/watch?v=B8J4uefCQMc</a> slides: <a href="https://www.slideshare.net/SessionsEvents/pedro-domingos-professor-university-of-washington-at-mlconf-atl-91815">https://www.slideshare.net/SessionsEvents/pedro-domingos-professor-university-of-washington-at-mlconf-atl-91815</a> Grand unified theory of machine learning</p>
                    <p>A Tour of Machine Learning Algorithms <a href="http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/">http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/</a></p>
                    <p>Asynchronous Methods for Deep Reinforcement Learning <a href="https://arxiv.org/abs/1602.01783">https://arxiv.org/abs/1602.01783</a></p>
                    <p>Active learning of inverse models with intrinsically motivated goal exploration in robots (2013) <a href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.278.5254">http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.278.5254</a></p>
                    <p>One-bit compressed sensing by linear programming (2011) <a href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.413.5719">http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.413.5719</a></p>
                    <p>Approximate Clustering without the Approximation <a href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.141.222">http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.141.222</a></p>
                    <p>Fully Automatic Cross-associations (2004) clustering algorithm with no magic numbers <a href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.67.9951">http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.67.9951</a></p>
                    <p>One and done? Optimal decisions from very few samples (2009) <a href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.211.6874">http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.211.6874</a></p>
                    <p>Whatever Next? Predictive Brains, Situated Agents, and the Future of Cognitive Science. (2012) <a href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.259.7600">http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.259.7600</a></p>
                    <h4 id="popular-science"><span class="section_number">9.19.2</span><span class="section_title">Popular science</span></h4>
                    <p><a href="https://qz.com/1161771/we-looked-at-the-major-scientific-discoveries-from-five-years-ago-to-see-where-they-are-now/">https://qz.com/1161771/we-looked-at-the-major-scientific-discoveries-from-five-years-ago-to-see-where-they-are-now/</a></p>
                    <p><a href="https://inside.com/lists/technically-sentient/recent_issues">https://inside.com/lists/technically-sentient/recent_issues</a></p>
                    <p>6 areas of AI and machine learning to watch closely <a href="https://medium.com/@NathanBenaich/6-areas-of-artificial-intelligence-to-watch-closely-673d590aa8aa#.sp7w03rk5">https://medium.com/@NathanBenaich/6-areas-of-artificial-intelligence-to-watch-closely-673d590aa8aa#.sp7w03rk5</a></p>
                    <p>Differentiable neural computers <a href="https://deepmind.com/blog/differentiable-neural-computers/">https://deepmind.com/blog/differentiable-neural-computers/</a></p>
                    <h3 id="independent-scholar-citizen-science"><span class="section_number">9.20</span><span class="section_title">Independent scholar? Citizen science?</span></h3>
                    <ul>
                    <li><a href="https://en.wikipedia.org/wiki/Independent_scholar">https://en.wikipedia.org/wiki/Independent_scholar</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Citizen_science">https://en.wikipedia.org/wiki/Citizen_science</a></li>
                    </ul>
                    <h3 id="what-is-the-relationship-between-ai-and-ml"><span class="section_number">9.21</span><span class="section_title">What is the relationship between AI and ML?</span></h3>
                    <p>ML is a subset of AI.</p>
                    <p>Then what is the rest of AI that is not ML?</p>
                    <ul>
                    <li>Ethics? Philosophy? Rule systems?</li>
                    <li><a href="https://ai.stackexchange.com/questions/35/what-is-the-difference-between-artificial-intelligence-and-machine-learning">AI SE 35: What is the difference between artificial intelligence and machine learning?</a></li>
                    <li>What is intelligence without learning? Non-adaptive intelligence? Static intelligence?</li>
                    </ul>
                    <h2 id="appendices"><span class="section_number">10</span><span class="section_title">(Appendices?)</span></h2>
                    <h2 id="approximation-theory"><span class="section_number">11</span><span class="section_title">Approximation theory?</span></h2>
                    <div class="local_table_of_contents">
                    <ul>
                    <li><span class="section_number">11.1</span><span class="section_title"><a href="#why">Why</a></span><span class="word_count">(177w~1m)</span></li>
                    <li><span class="section_number">11.2</span><span class="section_title"><a href="#other">Other</a></span><span class="word_count">(634w~4m)</span></li>
                    <li><span class="section_number">11.3</span><span class="section_title"><a href="#approximation-theory-and-machine-learning">Approximation theory and machine learning</a></span><span class="word_count">(18w~1m)</span></li>
                    <li><span class="section_number">11.4</span><span class="section_title"><a href="#approximation-by-truncation">Approximation by truncation</a></span><span class="word_count">(387w~2m)</span></li>
                    </ul>
                    </div>
                    <h3 id="why"><span class="section_number">11.1</span><span class="section_title">Why</span></h3>
                    <p>We are interested in approximation theory because we want to justify how neural networks work.</p>
                    <ul>
                    <li>2016, article, &quot;Deep vs. shallow networks: An approximation theory perspective&quot;, <a href="https://arxiv.org/abs/1608.03287">pdf available</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Explainable_Artificial_Intelligence">WP:Explainable Artificial Intelligence</a></li>
                    </ul>
                    <p>We should begin by skimming the 1998 book &quot;A Short Course on Approximation Theory&quot; by N. L. Carothers (<a href="http://fourier.math.uoc.gr/~mk/approx1011/carothers.pdf">pdf</a>). Then we should skim the 2017 lecture notes &quot;Lectures on multivariate polynomial approximation&quot; (<a href="http://www.math.unipd.it/~demarchi/MultInterp/LectureNotesMI.pdf">pdf</a>).</p>
                    <p>The phrase &quot;x <em>approximates</em> y&quot; means &quot;x is <em>close</em> to y&quot;, which implies distance, which implies metric space.</p>
                    <p>How close is the approximation? Suppose that the function <span class="math inline">\(g\)</span> approximates the function <span class="math inline">\(f\)</span> in interval <span class="math inline">\(I\)</span>. Then:</p>
                    <ul>
                    <li>The &quot;approximation error at <span class="math inline">\(x\)</span>&quot; is <span class="math inline">\(g(x) - f(x)\)</span>.</li>
                    <li>The &quot;maximum absolute error&quot; is <span class="math inline">\(\max_{x \in I} \abs{g(x) - f(x)}\)</span>.</li>
                    </ul>
                    <p>How do we measure the distance between two <span class="math inline">\(\Real \to \Real\)</span> functions <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span>? There are several ways. Which should we use?</p>
                    <ul>
                    <li>The maximum norm, in interval <span class="math inline">\(I\)</span> is <span class="math inline">\(\max_{x \in I} \abs{f(x) - g(x)}\)</span>. This norm is also called uniform norm, supremum norm, Chebyshev norm, infinity norm, norm-infinity, <span class="math inline">\(L_\infty\)</span>-norm. Why is it called &quot;uniform&quot;? <a href="https://en.wikipedia.org/wiki/Uniform_norm">WP:Uniform norm</a>.</li>
                    <li>What is this norm called? <span class="math inline">\(\int_{x \in I} [f(x)-g(x)]^2 ~ dx\)</span>.</li>
                    </ul>
                    <h3 id="other"><span class="section_number">11.2</span><span class="section_title">Other</span></h3>
                    <ul>
                    <li>Courses
                    <ul>
                    <li>2017, <a href="https://www.nada.kth.se/~olofr/Approx/">Approximation Theory, 7.5 ECTS</a></li>
                    <li>2012, syllabus, Drexel University, Math 680-002 (Approximation Theory), <a href="http://www.math.drexel.edu/~foucart/TeachingFiles/S12/Math680Syl.pdf">pdf</a></li>
                    <li>2002, <a href="http://math.ucdenver.edu/~aknyazev/teaching/02/5667/">MATH 5667-001: Introduction to Approximation Theory, CU-Denver, Fall 02</a>.</li>
                    </ul></li>
                    <li>Subfields of approximation theory
                    <ul>
                    <li>Classical approximation theory deals with univariate real functions <span class="math inline">\(\Real \to \Real\)</span>.</li>
                    <li>Multivariate approximation theory deals with multivariate real functions <span class="math inline">\(\Real^m \to \Real^n\)</span>.</li>
                    </ul></li>
                    <li>Scenarios
                    <ul>
                    <li>Suppose we want to approximate the function <span class="math inline">\(f\)</span>, but we don't know the equation for <span class="math inline">\(f\)</span>; we only have a few input-output samples.
                    <ul>
                    <li>Can we approximate <span class="math inline">\(f\)</span>?</li>
                    <li>How do approximation and curve-fitting relate?</li>
                    </ul></li>
                    </ul></li>
                    <li>Overview
                    <ul>
                    <li>What is a multivariate polynomial?</li>
                    <li>Commonly conflated concepts</li>
                    </ul></li>
                    <li>The <em>uniform norm</em> is …</li>
                    <li>Best approximation is …</li>
                    <li>Uniform approximation is best approximation in uniform norm.</li>
                    <li><a href="https://en.wikipedia.org/wiki/Approximation_theory#Remez&#39;s_algorithm">https://en.wikipedia.org/wiki/Approximation_theory#Remez's_algorithm</a>
                    <ul>
                    <li><a href="https://en.wikipedia.org/wiki/Remez_algorithm">https://en.wikipedia.org/wiki/Remez_algorithm</a>
                    <ul>
                    <li>Inputs: a function, and an interval.</li>
                    <li>Output: an optimal polynomial approximating the input function in the input interval.</li>
                    </ul></li>
                    </ul></li>
                    <li>What are Bernstein polynomials? What question does the Weierstrass approximation theorem answer?
                    <ul>
                    <li><a href="http://www4.ncsu.edu/~mtchu/Teaching/Lectures/MA530/chapter7.pdf">http://www4.ncsu.edu/~mtchu/Teaching/Lectures/MA530/chapter7.pdf</a></li>
                    </ul></li>
                    <li><a href="https://en.wikipedia.org/wiki/Chebyshev_polynomials">WP:Chebyshev polynomials</a>
                    <ul>
                    <li>Why is it important? How does it relate to best approximation?
                    <ul>
                    <li>&quot;Chebyshev polynomials are important in approximation theory because the roots of the Chebyshev polynomials of the first kind, which are also called Chebyshev nodes, are used as nodes in polynomial interpolation. The resulting interpolation polynomial minimizes the problem of Runge's phenomenon and provides an approximation that is close to the polynomial of best approximation to a continuous function under the maximum norm.&quot;</li>
                    </ul></li>
                    </ul></li>
                    <li>Machine learning as relation approximation
                    <ul>
                    <li>Machine learning, statistical modelling, function approximation, and curve fitting are related.</li>
                    <li>Generalize function approximation to relation approximation.</li>
                    <li>A function can be stated as a relation.</li>
                    <li>A relation can be stated as a function.</li>
                    </ul></li>
                    <li>Consider the least-square solution to an overdetermined system of linear equations. Is such solution a kind of approximation?
                    <ul>
                    <li>There is no exact solution to begin with?</li>
                    <li>Why is it called &quot;least-squares <em>approximation</em>&quot;?</li>
                    <li>How can you approximate something that does not exist?
                    <ul>
                    <li>1.2 approximates 1.23. Both 1.2 and 1.23 exist.</li>
                    <li>Contrarily, there is no X such that AX = B.</li>
                    </ul></li>
                    </ul></li>
                    <li>What are approximation schemes?
                    <ul>
                    <li><a href="https://en.wikipedia.org/wiki/Polynomial-time_approximation_scheme">https://en.wikipedia.org/wiki/Polynomial-time_approximation_scheme</a></li>
                    </ul></li>
                    <li>How do we approximate a function? Is it even possible to approximate arbitrary functions?
                    <ul>
                    <li>If the function is analytic, we can truncate its Taylor series.
                    <ul>
                    <li>Commonly-used differentiable functions are analytic.</li>
                    </ul></li>
                    <li>Chebyshev polynomials?</li>
                    <li>If we have an approximation scheme, we may be able to improve it.
                    <ul>
                    <li><a href="https://en.wikipedia.org/wiki/Series_acceleration">https://en.wikipedia.org/wiki/Series_acceleration</a>
                    <ul>
                    <li><a href="https://en.wikipedia.org/wiki/Aitken%27s_delta-squared_process">https://en.wikipedia.org/wiki/Aitken%27s_delta-squared_process</a></li>
                    </ul></li>
                    </ul></li>
                    <li>google search: machine learning approximation theory
                    <ul>
                    <li><a href="https://math.stackexchange.com/questions/2680158/approximation-theory-for-deep-learning-models-where-to-start">Approximation Theory for Deep Learning Models: Where to Start? - Mathematics Stack Exchange</a></li>
                    <li><a href="http://www.vision.jhu.edu/tutorials/ICCV15-Tutorial-Math-Deep-Learning-Intro-Rene-Joan.pdf">http://www.vision.jhu.edu/tutorials/ICCV15-Tutorial-Math-Deep-Learning-Intro-Rene-Joan.pdf</a></li>
                    <li>2017, slides, &quot;From approximation theory to machine learning: New perspectives in the theory of function spaces and their applications&quot;, <a href="http://npfsa2017.uni-jena.de/l_notes/vybiral.pdf">pdf</a></li>
                    <li>2018, article, &quot;Approximation theory, Numerical Analysis and Deep Learning&quot;, <a href="http://at.yorku.ca/c/b/p/g/30.htm">abstract</a>
                    <ul>
                    <li>&quot;the problem of numerically solving a large class of (high-dimensional) PDEs (such as linear Black-Scholes or diffusion equations) can be cast into a classical supervised learning problem which can then be solved by deep learning methods&quot;</li>
                    </ul></li>
                    </ul></li>
                    </ul></li>
                    <li>Determine whether we need to read these
                    <ul>
                    <li>Very likely
                    <ul>
                    <li>2015, slides, &quot;Best polynomial approximation: multidimensional case&quot;, <a href="https://carma.newcastle.edu.au/meetings/spcom/talks/Sukhorukova-SPCOM_2015.pdf">pdf</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Bernstein_polynomial#Approximating_continuous_functions">https://en.wikipedia.org/wiki/Bernstein_polynomial#Approximating_continuous_functions</a>
                    <ul>
                    <li><a href="https://en.wikipedia.org/wiki/Pointwise_convergence">https://en.wikipedia.org/wiki/Pointwise_convergence</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Uniform_convergence">https://en.wikipedia.org/wiki/Uniform_convergence</a></li>
                    </ul></li>
                    <li><a href="https://en.wikipedia.org/wiki/Approximation">https://en.wikipedia.org/wiki/Approximation</a>
                    <ul>
                    <li><a href="https://en.wikipedia.org/wiki/Approximation_theory">https://en.wikipedia.org/wiki/Approximation_theory</a>
                    <ul>
                    <li>is a branch of <a href="https://en.wikipedia.org/wiki/Functional_analysis">https://en.wikipedia.org/wiki/Functional_analysis</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Approximation_theory#Chebyshev_approximation">https://en.wikipedia.org/wiki/Approximation_theory#Chebyshev_approximation</a></li>
                    </ul></li>
                    <li><a href="https://en.wikipedia.org/wiki/Approximate_computing">https://en.wikipedia.org/wiki/Approximate_computing</a>
                    <ul>
                    <li>example: <a href="https://en.wikipedia.org/wiki/Artificial_neural_network">https://en.wikipedia.org/wiki/Artificial_neural_network</a></li>
                    </ul></li>
                    </ul></li>
                    <li><a href="https://en.wikipedia.org/wiki/Telescoping_series">https://en.wikipedia.org/wiki/Telescoping_series</a></li>
                    </ul></li>
                    <li>Likely
                    <ul>
                    <li>2018, slides, &quot;Deep Learning: Approximation of Functions by Composition&quot;, <a href="http://helper.ipam.ucla.edu/publications/dlt2018/dlt2018_14936.pdf">pdf</a>
                    <ul>
                    <li>classical approximation vs deep learning</li>
                    </ul></li>
                    <li>2013, short survey article draft, &quot;Multivariate approximation&quot;, <a href="http://num.math.uni-goettingen.de/schaback/research/papers/MultApp_01.pdf">pdf</a></li>
                    <li>1995, short introduction, &quot;Multivariate Interpolation and Approximation by Translates of a Basis Function&quot;, <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.45.2194&amp;rep=rep1&amp;type=pdf">pdf</a></li>
                    <li>1989, article, &quot;A Theory of Networks for Approximation and Learning&quot;, <a href="http://www.dtic.mil/docs/citations/ADA212359">pdf available</a>
                    <ul>
                    <li>What is the summary, especially about learning and approximation theory?</li>
                    </ul></li>
                    </ul></li>
                    <li>Unlikely
                    <ul>
                    <li>Survey-like
                    <ul>
                    <li>2006, chapter, &quot;Topics in multivariate approximation theory&quot;, <a href="https://www.researchgate.net/publication/226303661_Topics_in_multivariate_approximation_theory">pdf available</a></li>
                    <li>1982, article, &quot;Topics in multivariate approximation theory&quot;, <a href="http://www.dtic.mil/dtic/tr/fulltext/u2/a116248.pdf">pdf</a></li>
                    <li>1986, &quot;Multivariate Approximation Theory: Selected Topics&quot;, <a href="https://epubs.siam.org/doi/book/10.1137/1.9781611970197">paywall</a></li>
                    </ul></li>
                    <li>Theorem
                    <ul>
                    <li>2017, article, &quot;Multivariate polynomial approximation in the hypercube&quot;, <a href="https://people.maths.ox.ac.uk/trefethen/hypercube_published.pdf">pdf</a></li>
                    </ul></li>
                    <li>2017, article, &quot;Selected open problems in polynomial approximation and potential theory&quot;, <a href="http://drna.padovauniversitypress.it/system/files/papers/BaranCiezEgginkKowalskaNagyPierzcha%C5%82a_DRNA2017.pdf">pdf</a></li>
                    <li>2017, article, &quot;High order approximation theory for Banach space valued functions&quot;, <a href="https://ictp.acad.ro/jnaat/journal/article/view/1112">pdf available</a></li>
                    <li>Articles summarizing people's works
                    <ul>
                    <li>2017, article, &quot;Michael J.D. Powell's work in approximation theory and optimisation&quot;, <a href="https://www.sciencedirect.com/science/article/abs/pii/S0021904517301053">paywall</a></li>
                    <li>2000, article, &quot;Weierstrass and Approximation Theory&quot;, <a href="https://www.sciencedirect.com/science/article/pii/S0021904500935081">paywall</a></li>
                    </ul></li>
                    <li>2013, article, &quot;[1312.5540] Emerging problems in approximation theory for the numerical solution of nonlinear PDEs of integrable type&quot;, <a href="https://arxiv.org/abs/1312.5540">pdf available</a></li>
                    <li>1985, article, &quot;Some problems in approximation theory and numerical analysis - IOPscience&quot;, <a href="http://iopscience.iop.org/article/10.1070/RM1985v040n01ABEH003526">pdf available</a></li>
                    <li>2011, article, &quot;Experiments on Probabilistic Approximations&quot;, <a href="https://people.eecs.ku.edu/~jerzygb/c154-clark.pdf">pdf</a></li>
                    </ul></li>
                    </ul></li>
                    <li>Less relevant overview
                    <ul>
                    <li>Why do we approximate?
                    <ul>
                    <li>Because it is practically inevitable.
                    <ul>
                    <li>Fundamental reason: Because computers are finite.</li>
                    <li>Practical reason: Trade-off between computation time and precision.
                    <ul>
                    <li>The more error we can afford, the faster we can run.
                    <ul>
                    <li>May be related: 2013 monograph &quot;Faster Algorithms via Approximation Theory&quot; <a href="http://theory.epfl.ch/vishnoi/Publications_files/approx-survey.pdf">pdf</a></li>
                    </ul></li>
                    </ul></li>
                    </ul></li>
                    </ul></li>
                    <li>2018 book &quot;Recent Advances in Constructive Approximation Theory&quot; <a href="https://www.springer.com/us/book/9783319921648">paywall</a></li>
                    </ul></li>
                    </ul>
                    <h3 id="approximation-theory-and-machine-learning"><span class="section_number">11.3</span><span class="section_title">Approximation theory and machine learning</span></h3>
                    <p>Conference: &quot;Approximation Theory and Machine Learning&quot;, at Purdue University, September 29 - 30, 2018</p>
                    <ul>
                    <li><a href="http://www.math.purdue.edu/calendar/conferences/machinelearning/">http://www.math.purdue.edu/calendar/conferences/machinelearning/</a></li>
                    <li><a href="http://www.math.purdue.edu/calendar/conferences/machinelearning/abstracts.php">http://www.math.purdue.edu/calendar/conferences/machinelearning/abstracts.php</a></li>
                    </ul>
                    <h3 id="approximation-by-truncation"><span class="section_number">11.4</span><span class="section_title">Approximation by truncation</span></h3>
                    <p>We can approximate a series by <em>truncating</em> it.</p>
                    <p>Suppose that the series <span class="math inline">\(y = x_0 + x_1 + \ldots\)</span> converges.</p>
                    <p>Suppose that the sequence <span class="math inline">\(\langle x_0, x_1, \ldots \rangle\)</span> converges to zero.</p>
                    <p>Pick where to cut. Pick a natural number <span class="math inline">\(n\)</span>.</p>
                    <p>Then the series <span class="math inline">\(x_0 + \ldots + x_n\)</span> approximates the series <span class="math inline">\(y\)</span>. We cut its tail. We take finitely many summands from the beginning.</p>
                    <p>Here come examples: Truncate all the series!</p>
                    <div class="local_table_of_contents">
                    <ul>
                    <li><span class="section_number">11.4.1</span><span class="section_title"><a href="#power-series-truncation">Power series truncation</a></span><span class="word_count">(258w~2m)</span></li>
                    <li><span class="section_number">11.4.2</span><span class="section_title"><a href="#iteration-truncation">Iteration truncation</a></span><span class="word_count">(62w~1m)</span></li>
                    </ul>
                    </div>
                    <h4 id="power-series-truncation"><span class="section_number">11.4.1</span><span class="section_title">Power series truncation</span></h4>
                    <p>Below we truncate a power series.</p>
                    <p>Decimal truncation: <span class="math inline">\(1.2\)</span> approximates <span class="math inline">\(1.23\)</span>. Remember that a decimal number is a series. For example, the number <span class="math inline">\(1.23\)</span> is the power series <span class="math display">\[ \ldots 01.230 \ldots = \ldots + 0 \cdot 10^1 + 1 \cdot 10^0 + 2 \cdot 10^{-1} + 3 \cdot 10^{-2} + 0 \cdot 10^{-3} + \ldots. \]</span></p>
                    <p>Polynomial truncation: <span class="math inline">\(1 + x\)</span> approximates <span class="math inline">\(1 + x + x^2\)</span> for <span class="math inline">\(x\)</span> near zero.</p>
                    <p>Taylor series truncation: <span class="math inline">\(1 + x + \frac{x^2}{2}\)</span> approximates <span class="math inline">\(e^x\)</span> for <span class="math inline">\(x\)</span> near zero. Remember the Taylor series expansion <span class="math inline">\(e^x = \sum_{n \in \Nat} \frac{x^n}{n!}\)</span>.</p>
                    <p>Below we truncate the ratio of two power series.</p>
                    <p>Rational truncation: <span class="math inline">\(12/23\)</span> approximates <span class="math inline">\(123/234\)</span>.</p>
                    <p><a href="https://en.wikipedia.org/wiki/Pad%C3%A9_approximant">WP:Padé approximation</a> is a truncation of a ratio of series.</p>
                    <p>Fourier series truncation: The <a href="https://en.wikipedia.org/wiki/Fourier_series#Example_1:_a_simple_Fourier_series">Wikipedia example</a> animates how a Fourier series converges to the sawtooth function as more terms are added.</p>
                    <p>Digression: Is a (complex) Fourier series a power series? Reminder: A Fourier series looks like <span class="math inline">\(\sum_{k=0}^{\infty} c_k e^{ikt}\)</span>.</p>
                    <p><a href="https://en.wikipedia.org/wiki/Laurent_series">WP:Laurent series</a> truncation?</p>
                    <ol>
                    <li><p>Digression: What is an analytic function?</p>
                    <p>A function is <em>analytic</em> iff it can be represented by power series.</p>
                    <p>Formally, a function <span class="math inline">\(f\)</span> is <em>analytic</em> iff for every <span class="math inline">\(x \in \dom(f)\)</span>, we can write <span class="math inline">\(f(x)\)</span> as a power series.</p>
                    <p>See also <a href="https://en.wikipedia.org/wiki/Power_series#Analytic_functions">WP:Definition of &quot;analytic function&quot;</a>.</p>
                    <p>Taylor series expansion is illustrated in the 2015 slides &quot;Taylor Series: Expansions, Approximations and Error&quot; (<a href="https://relate.cs.illinois.edu/course/cs357-f15/file-version/2978ddd5db9824a374db221c47a33f437f2df1da/media/cs357-slides6.pdf">pdf</a>)</p></li>
                    <li><p>Digression: What is the relationship between polynomial and power series?</p>
                    <p>A polynomial is an algebraic expression. It is not a function.</p>
                    <p>Power series is a kind of infinite polynomial.</p>
                    <p><a href="https://en.wikipedia.org/wiki/Formal_power_series">WP:Formal power series</a>: &quot;A formal power series is a generalization of a polynomial, where the number of terms is allowed to be infinite.&quot;</p></li>
                    </ol>
                    <h4 id="iteration-truncation"><span class="section_number">11.4.2</span><span class="section_title">Iteration truncation</span></h4>
                    <ul>
                    <li><a href="https://en.wikipedia.org/wiki/Iterated_function">WP:Iterated function</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Iterative_method">WP:Iterative method</a></li>
                    <li><a href="http://mathworld.wolfram.com/NewtonsIteration.html">Newton's Iteration</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Methods_of_computing_square_roots#Babylonian_method">WP:Methods of computing square roots, the Babylonian method</a></li>
                    <li>An iteration converges to an attractive fixed point.</li>
                    </ul>
                    <p>Example: Let <span class="math inline">\(f(x) = x + \frac{1}{x}\)</span>.</p>
                    <p>Continued fraction truncation: We know that <span class="math display">\[ 1 + \frac{1}{1 + \frac{1}{1 + \ldots}} = \frac{1 + \sqrt{5}}{2} = \Phi. \]</span> We can truncate that continued fraction to approximate <span class="math inline">\(\Phi\)</span>.</p>
                    <p>Seeing those examples makes me wonder whether all approximations are truncation.</p>
                    <h2 id="bibliography-1" class="unnumbered"><span class="section_number">12</span><span class="section_title">Bibliography</span></h2>
                    <div id="refs" class="references">
                    <div id="ref-AshbyBrain">
                    <p>[1] Ashby, W.R. 1954. <em>Design for a brain</em>. Wiley.</p>
                    </div>
                    <div id="ref-BattleThree">
                    <p>[2] Battle, S. 2015. A mobile homeostat with three degrees of freedom. <em>IMA conference on mathematics of robotics</em> (St Anne’s College, University of Oxford, UK, 2015).</p>
                    </div>
                    <div id="ref-BattleHom">
                    <p>[3] Battle, S. 2014. Ashby’s mobile homeostat. (2014), 110–123.</p>
                    </div>
                    <div id="ref-DeepArch">
                    <p>[4] Bengio, Y. <em>Learning deep architectures for AI</em>. Technical Report #1312.</p>
                    </div>
                    <div id="ref-RepLearn">
                    <p>[5] Bengio, Y. et al. Representation learning: A review and new perspectives.</p>
                    </div>
                    <div id="ref-sep-artificial-intelligence">
                    <p>[6] Bringsjord, S. and Govindarajulu, N.S. 2018. Artificial intelligence. <em>The stanford encyclopedia of philosophy</em>. E.N. Zalta, ed. <a href="https://plato.stanford.edu/archives/fall2018/entries/artificial-intelligence/">https://plato.stanford.edu/archives/fall2018/entries/artificial-intelligence/</a>; Metaphysics Research Lab, Stanford University.</p>
                    </div>
                    <div id="ref-friston2010free">
                    <p>[7] Friston, K. 2010. The free-energy principle: A unified brain theory? <em>Nature Reviews Neuroscience</em>. 11, 2 (2010), 127–138.</p>
                    </div>
                    <div id="ref-friston2006free">
                    <p>[8] Friston, K. et al. 2006. A free energy principle for the brain. <em>Journal of Physiology-Paris</em>. 100, 1 (2006), 70–87.</p>
                    </div>
                    <div id="ref-GacsVitanyiSolomonoff">
                    <p>[9] Gács, P. and Vitányi, P.M.B. 2011. Raymond J. Solomonoff 1926–2009. <em>IEEE Information Theory Society Newsletter</em>. 61, 1 (2011), 11–16.</p>
                    </div>
                    <div id="ref-Organ">
                    <p>[10] Glushko, R.J. ed. 2013. <em>The discipline of organizing</em>. MIT Press.</p>
                    </div>
                    <div id="ref-GoertzelAgi">
                    <p>[11] Goertzel, B. 2015. Artificial general intelligence. <em>Scholarpedia</em>. 10, 11 (2015), 31847.</p>
                    </div>
                    <div id="ref-DeepLearning">
                    <p>[12] Goodfellow, I. et al. 2016. <em>Deep learning</em>. MIT Press.</p>
                    </div>
                    <div id="ref-AlgoInfTh">
                    <p>[13] Hutter, M. 2007. Algorithmic information theory. <em>Scholarpedia</em>. 2, 3 (2007), 2519.</p>
                    </div>
                    <div id="ref-hutter2004universal">
                    <p>[14] Hutter, M. 2004. <em>Universal artificial intelligence: Sequential decisions based on algorithmic probability</em>. Springer Science &amp; Business Media.</p>
                    </div>
                    <div id="ref-izbicki2013hlearn">
                    <p>[15] Izbicki, M. 2013. HLearn: a machine learning library for Haskell. <em>Proceedings of the fourteenth symposium on trends in functional programming, brigham young university, utah</em> (2013).</p>
                    </div>
                    <div id="ref-Legg2007Collection">
                    <p>[16] Legg, S. and Hutter, M. 2007. A collection of definitions of intelligence. <em>Frontiers in Artificial Intelligence and applications</em>. 157, (2007), 17.</p>
                    </div>
                    <div id="ref-DefineMachIntel">
                    <p>[17] Legg, S. and Hutter, M. 2007. Universal intelligence: A definition of machine intelligence. (2007).</p>
                    </div>
                    <div id="ref-LiangCs221">
                    <p>[18] Liang, P. 2016. Lecture notes in Stanford University CS221: Artificial Intelligence: Principles and Techniques.</p>
                    </div>
                    <div id="ref-SystemManage">
                    <p>[19] Mele, C. et al. 2010. A brief review of systems theories and their managerial applications. <em>Service Science</em>. 2, 1-2 (2010), 126–135. DOI:<a href="https://doi.org/10.1287/serv.2.1\_2.126">https://doi.org/10.1287/serv.2.1\_2.126</a>. url: &lt;<a href="http://dx.doi.org/10.1287/serv.2.1_2.126">http://dx.doi.org/10.1287/serv.2.1_2.126</a>&gt;.</p>
                    </div>
                    <div id="ref-Topology">
                    <p>[20] Morris, S.A. 2016. <em>Topology without tears</em>.</p>
                    </div>
                    <div id="ref-nalbantov2006nearest">
                    <p>[21] Nalbantov, G. et al. 2006. <em>Nearest convex hull classification</em>. url: &lt;<a href="https://pdfs.semanticscholar.org/a833/81e279fa548aca034f310fff6385c3d6b809.pdf">https://pdfs.semanticscholar.org/a833/81e279fa548aca034f310fff6385c3d6b809.pdf</a>&gt;.</p>
                    </div>
                    <div id="ref-negnevitsky2005artificial">
                    <p>[22] Negnevitsky, M. 2005. <em>Artificial intelligence: A guide to intelligent systems</em>. Pearson Education.</p>
                    </div>
                    <div id="ref-NilsLogicAi">
                    <p>[23] Nilsson, N.J. 1991. Logic and artificial intelligence. <em>Artificial Intelligence</em>. 47, 1-3 (Feb. 1991), 31–56. DOI:<a href="https://doi.org/10.1016/0004-3702(91)90049-P">https://doi.org/10.1016/0004-3702(91)90049-P</a>. url: &lt;<a href="http://dx.doi.org/10.1016/0004-3702(91)90049-P">http://dx.doi.org/10.1016/0004-3702(91)90049-P</a>&gt;.</p>
                    </div>
                    <div id="ref-PickeringCyber">
                    <p>[24] Pickering, A. 2010. <em>The cybernetic brain: Sketches of another future</em>. University of Chicago Press.</p>
                    </div>
                    <div id="ref-rabaey2003microbial">
                    <p>[25] Rabaey, K. et al. 2003. A microbial fuel cell capable of converting glucose to electricity at high rate and efficiency. <em>Biotechnology letters</em>. 25, 18 (2003), 1531–1535.</p>
                    </div>
                    <div id="ref-rigollet2015ocw">
                    <p>[26] Rigollet, P. 18.657 Mathematics of Machine Learning.</p>
                    </div>
                    <div id="ref-SlomanTuringIrrelevance">
                    <p>[27] Sloman, A. 2002. <em>The irrelevance of turing machines to artificial intelligence</em>. The MIT Press, Cambridge, Mass.</p>
                    </div>
                    <div id="ref-WdsIntelSlide">
                    <p>[28] Smith, W.D. 2006. Mathematical definition of “intelligence”, after mathematical definition of “intelligence” (and consequences). (2006).</p>
                    </div>
                    <div id="ref-WdsIntel">
                    <p>[29] Smith, W.D. 2006. Mathematical definition of “intelligence” (and consequences). (2006).</p>
                    </div>
                    <div id="ref-solomonoff1996does">
                    <p>[30] Solomonoff, R. 1996. Does algorithmic probability solve the problem of induction. <em>Oxbridge Research, POB</em>. 391887, (1996). url: &lt;<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.70.1572&amp;rep=rep1&amp;type=pdf">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.70.1572&amp;rep=rep1&amp;type=pdf</a>&gt;.</p>
                    </div>
                    <div id="ref-SolAlpProb2011">
                    <p>[31] Solomonoff, R.J. 2011. Algorithmic probability – its discovery – its properties and application to strong AI. <em>Randomness through computation: Some answers, more questions</em>. H. Zenil, ed. World Scientific Publishing Company. 1–23.</p>
                    </div>
                    <div id="ref-GodelMachImpl">
                    <p>[32] Steunebrink, B.R. and Schmidhuber, J. Towards an actual Gödel machine implementation: a lesson in self-reflective systems.</p>
                    </div>
                    <div id="ref-SuttonBartoRein">
                    <p>[33] Sutton, R.S. and Barto, A.G. 1998. <em>Reinforcement learning: An introduction</em>. MIT Press.</p>
                    </div>
                    <div id="ref-tarantola2005inverse">
                    <p>[34] Tarantola, A. 2005. <em>Inverse problem theory and methods for model parameter estimation</em>. SIAM.</p>
                    </div>
                    <div id="ref-SepLogicAi">
                    <p>[35] Thomason, R. 2016. Logic and artificial intelligence. <em>The stanford encyclopedia of philosophy</em>. E.N. Zalta, ed. <a href="https://plato.stanford.edu/archives/win2016/entries/logic-ai/">https://plato.stanford.edu/archives/win2016/entries/logic-ai/</a>; Metaphysics Research Lab, Stanford University.</p>
                    </div>
                    <div id="ref-ApproxThePrac">
                    <p>[36] Trefethen, L.N. <em>Approximation theory and approximation practice</em>.</p>
                    </div>
                    <div id="ref-white2012generalized">
                    <p>[37] White, M. and Schuurmans, D. 2012. Generalized optimal reverse prediction. <em>Artificial intelligence and statistics</em> (2012), 1305–1313. url: &lt;<a href="http://proceedings.mlr.press/v22/white12/white12.pdf">http://proceedings.mlr.press/v22/white12/white12.pdf</a>&gt;.</p>
                    </div>
                    <div id="ref-WienerCyber">
                    <p>[38] Wiener, N. 1961. <em>Cybernetics or control and communication in the animal and the machine</em>. MIT Press.</p>
                    </div>
                    <div id="ref-xu2009optimal">
                    <p>[39] Xu, L. et al. 2009. Optimal reverse prediction: A unified perspective on supervised, unsupervised and semi-supervised learning. <em>Proceedings of the 26th annual international conference on machine learning</em> (2009), 1137–1144. url: &lt;<a href="http://people.ee.duke.edu/~lcarin/OptReversePred.pdf">http://people.ee.duke.edu/~lcarin/OptReversePred.pdf</a>&gt;.</p>
                    </div>
                    <div id="ref-CircuitFitzHughNagumo">
                    <p>[40] Zhao, J. and Kim, Y.-B. 2007. Circuit implementation of FitzHugh-Nagumo neuron model using field programmable analog arrays. <em>50th midwest symposium on circuits and systems (mwscas)</em> (2007), 772–775.</p>
                    </div>
                    <div id="ref-zhu2014high">
                    <p>[41] Zhu, Z. et al. 2014. A high-energy-density sugar biobattery based on a synthetic enzymatic pathway. <em>Nature communications</em>. 5, (2014).</p>
                    </div>
                    <div id="ref-openworm">
                    <p>[42] OpenWorm. <a href="http://www.openworm.org/">http://www.openworm.org/</a>.</p>
                    </div>
                    </div>
                    <section class="footnotes">
                    <hr />
                    <ol>
                    <li id="fn1"><p><a href="http://www.sussex.ac.uk/informatics/punctuation/essaysandletters/footnotes">http://www.sussex.ac.uk/informatics/punctuation/essaysandletters/footnotes</a><a href="#fnref1" class="footnote-back">↩</a></p></li>
                    <li id="fn2"><p><a href="http://users.cs.cf.ac.uk/Dave.Marshall/AI2/node131.html#SECTION000151000000000000000">http://users.cs.cf.ac.uk/Dave.Marshall/AI2/node131.html#SECTION000151000000000000000</a><a href="#fnref2" class="footnote-back">↩</a></p></li>
                    <li id="fn3"><p><a href="https://en.wikipedia.org/wiki/Philosophy_of_artificial_intelligence">https://en.wikipedia.org/wiki/Philosophy_of_artificial_intelligence</a><a href="#fnref3" class="footnote-back">↩</a></p></li>
                    <li id="fn4"><p><a href="https://www.etymonline.com/word/intelligence">https://www.etymonline.com/word/intelligence</a><a href="#fnref4" class="footnote-back">↩</a></p></li>
                    <li id="fn5"><p><a href="http://www.dictionary.com/browse/intelligent">http://www.dictionary.com/browse/intelligent</a><a href="#fnref5" class="footnote-back">↩</a></p></li>
                    <li id="fn6"><p><a href="https://en.wikipedia.org/wiki/Duck_typing">https://en.wikipedia.org/wiki/Duck_typing</a><a href="#fnref6" class="footnote-back">↩</a></p></li>
                    <li id="fn7"><p><a href="https://www.etymonline.com/word/predict">https://www.etymonline.com/word/predict</a><a href="#fnref7" class="footnote-back">↩</a></p></li>
                    <li id="fn8"><p><a href="https://christophm.github.io/interpretable-ml-book/">https://christophm.github.io/interpretable-ml-book/</a><a href="#fnref8" class="footnote-back">↩</a></p></li>
                    <li id="fn9"><p><a href="https://stats.stackexchange.com/questions/214381/what-exactly-is-the-mathematical-definition-of-a-classifier-classification-alg">https://stats.stackexchange.com/questions/214381/what-exactly-is-the-mathematical-definition-of-a-classifier-classification-alg</a><a href="#fnref9" class="footnote-back">↩</a></p></li>
                    <li id="fn10"><p><a href="https://ndutoitblog.wordpress.com/2018/04/01/defining-machine-learning-with-maths/">https://ndutoitblog.wordpress.com/2018/04/01/defining-machine-learning-with-maths/</a><a href="#fnref10" class="footnote-back">↩</a></p></li>
                    <li id="fn11"><p><a href="https://en.wikipedia.org/wiki/Partition_of_a_set">https://en.wikipedia.org/wiki/Partition_of_a_set</a><a href="#fnref11" class="footnote-back">↩</a></p></li>
                    <li id="fn12"><p><a href="https://arxiv.org/abs/1802.08864">https://arxiv.org/abs/1802.08864</a><a href="#fnref12" class="footnote-back">↩</a></p></li>
                    <li id="fn13"><p><a href="https://en.wikipedia.org/wiki/Cluster_hypothesis">https://en.wikipedia.org/wiki/Cluster_hypothesis</a><a href="#fnref13" class="footnote-back">↩</a></p></li>
                    <li id="fn14"><p><a href="https://medium.com/continual-ai/why-continuous-learning-is-the-key-towards-machine-intelligence-1851cb57c308">https://medium.com/continual-ai/why-continuous-learning-is-the-key-towards-machine-intelligence-1851cb57c308</a><a href="#fnref14" class="footnote-back">↩</a></p></li>
                    <li id="fn15"><p><a href="https://www.cs.uic.edu/~liub/lifelong-learning.html">https://www.cs.uic.edu/~liub/lifelong-learning.html</a><a href="#fnref15" class="footnote-back">↩</a></p></li>
                    <li id="fn16"><p><a href="https://en.wikipedia.org/wiki/Nearest_centroid_classifier">https://en.wikipedia.org/wiki/Nearest_centroid_classifier</a><a href="#fnref16" class="footnote-back">↩</a></p></li>
                    <li id="fn17"><p><a href="https://link.springer.com/chapter/10.1007/978-3-642-28942-2_24">https://link.springer.com/chapter/10.1007/978-3-642-28942-2_24</a><a href="#fnref17" class="footnote-back">↩</a></p></li>
                    <li id="fn18"><p><a href="https://www.researchgate.net/publication/262204053_Nearest_Cluster_Classifier">https://www.researchgate.net/publication/262204053_Nearest_Cluster_Classifier</a><a href="#fnref18" class="footnote-back">↩</a></p></li>
                    <li id="fn19"><p><a href="https://en.wikipedia.org/wiki/Aitken%27s_delta-squared_process">https://en.wikipedia.org/wiki/Aitken%27s_delta-squared_process</a><a href="#fnref19" class="footnote-back">↩</a></p></li>
                    <li id="fn20"><p><a href="https://en.wikipedia.org/wiki/Sequence_transformation">https://en.wikipedia.org/wiki/Sequence_transformation</a><a href="#fnref20" class="footnote-back">↩</a></p></li>
                    <li id="fn21"><p><a href="http://people.idsia.ch/~juergen/">http://people.idsia.ch/~juergen/</a><a href="#fnref21" class="footnote-back">↩</a></p></li>
                    <li id="fn22"><p><a href="https://www.reddit.com/r/MachineLearning/comments/2xcyrl/i_am_j%C3%BCrgen_schmidhuber_ama/">https://www.reddit.com/r/MachineLearning/comments/2xcyrl/i_am_j%C3%BCrgen_schmidhuber_ama/</a><a href="#fnref22" class="footnote-back">↩</a></p></li>
                    <li id="fn23"><p><a href="https://www.theguardian.com/technology/2017/apr/18/robot-man-artificial-intelligence-computer-milky-way">https://www.theguardian.com/technology/2017/apr/18/robot-man-artificial-intelligence-computer-milky-way</a><a href="#fnref23" class="footnote-back">↩</a></p></li>
                    <li id="fn24"><p><a href="http://bootstrappingartificialintelligence.fr/WordPress3/">http://bootstrappingartificialintelligence.fr/WordPress3/</a><a href="#fnref24" class="footnote-back">↩</a></p></li>
                    <li id="fn25"><p><a href="https://www.nytimes.com/2018/06/20/technology/deep-learning-artificial-intelligence.html">https://www.nytimes.com/2018/06/20/technology/deep-learning-artificial-intelligence.html</a><a href="#fnref25" class="footnote-back">↩</a></p></li>
                    <li id="fn26"><p><a href="https://www.reddit.com/r/askscience/comments/3ib6hi/where_is_the_strong_ai_bottleneck/">https://www.reddit.com/r/askscience/comments/3ib6hi/where_is_the_strong_ai_bottleneck/</a><a href="#fnref26" class="footnote-back">↩</a></p></li>
                    <li id="fn27"><p><a href="https://www.theguardian.com/science/2012/oct/03/philosophy-artificial-intelligence">https://www.theguardian.com/science/2012/oct/03/philosophy-artificial-intelligence</a><a href="#fnref27" class="footnote-back">↩</a></p></li>
                    <li id="fn28"><p><a href="https://aeon.co/essays/how-close-are-we-to-creating-artificial-intelligence">https://aeon.co/essays/how-close-are-we-to-creating-artificial-intelligence</a><a href="#fnref28" class="footnote-back">↩</a></p></li>
                    </ol>
                    </section>
                </div>
            </div>
        </main>
                        <div id="disqus_thread"></div>
<script>
/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
var disqus_config = function () {
    this.page.url = "https://edom.github.io/intelligence.html";  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = "https://edom.github.io/intelligence.html"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://edom-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
                        <footer class="site-footer h-card">
            <data class="u-url" href="/"></data>
            <div class="wrapper">
                <p>This page was created on 2017-06-22 03:57:00 +0700.</p>
                <p class="rss-subscribe">There is an
                    <a href="/feed.xml">RSS feed</a>, but it's unused because this site is a wiki, not a blog.</p>
                <p>Stop writing books, papers, and blogs!
                    Write a personal wiki instead!
                    Or, even better, contribute to a community wiki.
                </p>
            </div>
        </footer>
    </body>
</html>
