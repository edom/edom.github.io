<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Approximating functions</title>
  <meta name="description" content="Personal website">

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="https://edom.github.io/approx.html">
  <link rel="alternate" type="application/rss+xml" title="Erik Dominikus&#39;s wiki" href="/feed.xml">

  

  
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-12628443-6', 'auto');
  ga('send', 'pageview');

</script>
  

  

  
  <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    jax: ["input/TeX","input/MathML","input/AsciiMath",

    "output/CommonHTML"

    ],
    extensions: ["tex2jax.js","mml2jax.js","asciimath2jax.js","MathMenu.js","MathZoom.js","AssistiveMML.js", "a11y/accessibility-menu.js"],
    TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
        , equationNumbers: {
            autoNumber: "AMS"
        }
    },
    "CommonHTML": {
        scale: 100
    },
    "fast-preview": {
        disabled: true,
    }
});
  </script>
  
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js" async></script>
  
</head>


  <body>

    <header class="site-header" role="banner">
  <div class="wrapper">
    <a class="site-title" rel="author" href="/">Erik Dominikus&#39;s wiki</a>
  </div>
</header>


    
  <div style="display:none;">\(
\renewcommand\emptyset{\varnothing}
\newcommand\abs[1]{\left|#1\right|}
\newcommand\dom{\textrm{dom}}
\newcommand\cod{\textrm{cod}}
\newcommand\Bernoulli{\textrm{Bernoulli}}
\newcommand\Binomial{\textrm{Binomial}}
\newcommand\Expect[1]{\mathbb{E}[#1]}
\newcommand\Nat{\mathbb{N}}
\newcommand\Integers{\mathbb{Z}}
\newcommand\Real{\mathbb{R}}
\newcommand\Rational{\mathbb{Q}}
\newcommand\Complex{\mathbb{C}}
\newcommand\Pr{\mathrm{P}}
\newcommand\Time{\text{Time}}
\newcommand\DTime{\text{DTime}}
\newcommand\NTime{\text{NTime}}
\newcommand\TimeP{\text{P}}
\newcommand\TimeNP{\text{NP}}
\newcommand\TimeExp{\text{ExpTime}}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand\bbA{\mathbb{A}}
\newcommand\bbC{\mathbb{C}}
\newcommand\bbD{\mathbb{D}}
\newcommand\bbE{\mathbb{E}}
\newcommand\bbN{\mathbb{N}}
\newcommand\frakI{\mathfrak{I}}
% deprecated; use TimeExp
\newcommand\ExpTime{\text{ExpTime}}
\newcommand\Compute{\text{Compute}}
\newcommand\Search{\text{Search}}
% model theory structure
\newcommand\struc[1]{\mathcal{#1}}
  \)</div>
    

    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Approximating functions</h1>
  </header>

  <div class="post-content">
    <p>We are interested in approximation theory because we want to justify how neural networks work.</p>

<ul>
  <li>2016, article, “Deep vs. shallow networks: An approximation theory perspective”, <a href="https://arxiv.org/abs/1608.03287">pdf available</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Explainable_Artificial_Intelligence">WP:Explainable Artificial Intelligence</a></li>
</ul>

<p>We should begin by skimming the 1998 book “A Short Course on Approximation Theory” by N. L. Carothers (<a href="http://fourier.math.uoc.gr/~mk/approx1011/carothers.pdf">pdf</a>).
Then we should skim the 2017 lecture notes “Lectures on multivariate polynomial approximation” (<a href="http://www.math.unipd.it/~demarchi/MultInterp/LectureNotesMI.pdf">pdf</a>).</p>

<p>The phrase “x <em>approximates</em> y” means “x is <em>close</em> to y”, which implies distance, which implies metric space.</p>

<p>How close is the approximation?
Suppose that the function \( g \) approximates the function \( f \) in interval \( I \).
Then:</p>

<ul>
  <li>The “approximation error at \( x \)” is \( g(x) - f(x) \).</li>
  <li>The “maximum absolute error” is \( \max_{x \in I} \abs{g(x) - f(x)} \).</li>
</ul>

<p>How do we measure the distance between two \( \Real \to \Real \) functions \( f \) and \( g \)?
There are several ways.
Which should we use?</p>

<ul>
  <li>The maximum norm, in interval \( I \) is \( \max_{x \in I} \abs{f(x) - g(x)} \).
This norm is also called uniform norm, supremum norm, Chebyshev norm, infinity norm, norm-infinity, \( L_\infty \)-norm.
Why is it called “uniform”?
<a href="https://en.wikipedia.org/wiki/Uniform_norm">WP:Uniform norm</a>.</li>
  <li>What is this norm called? \( \int_{x \in I} [f(x)-g(x)]^2 ~ dx \).</li>
</ul>

<p><a href="/atrunc.html">Are all approximations truncation?</a>
Are there other approximation schemes beside series truncation?
Are probabilistic approximations such as Monte Carlo approximations also truncation?</p>

<h2 id="other">Other</h2>

<ul>
  <li>Courses
    <ul>
      <li>2017, <a href="https://www.nada.kth.se/~olofr/Approx/">Approximation Theory, 7.5 ECTS</a></li>
      <li>2012, syllabus, Drexel University, Math 680-002 (Approximation Theory), <a href="http://www.math.drexel.edu/~foucart/TeachingFiles/S12/Math680Syl.pdf">pdf</a></li>
      <li>2002, <a href="http://math.ucdenver.edu/~aknyazev/teaching/02/5667/">MATH 5667-001: Introduction to Approximation Theory, CU-Denver, Fall 02</a>.</li>
    </ul>
  </li>
  <li>Subfields of approximation theory
    <ul>
      <li>Classical approximation theory deals with univariate real functions \( \Real \to \Real \).</li>
      <li>Multivariate approximation theory deals with multivariate real functions \( \Real^m \to \Real^n \).</li>
    </ul>
  </li>
  <li>Scenarios
    <ul>
      <li>Suppose we want to approximate the function \( f \),
  but we don’t know the equation for \( f \);
  we only have a few input-output samples.
        <ul>
          <li>Can we approximate \( f \)?</li>
          <li>How do approximation and curve-fitting relate?</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Overview
    <ul>
      <li>What is a multivariate polynomial?</li>
      <li>Commonly conflated concepts
        <ul>
          <li>Approximation is not estimation.
            <ul>
              <li>Approximation converges.
  Estimation doesn’t, because the actual value is unknown.</li>
              <li>Approximation doesn’t guess.
  Estimation does.</li>
              <li>Approximation has error.
  Estimation has uncertainty.</li>
              <li>Approximation is part of analysis.
  Estimation is part of statistics.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>The <em>uniform norm</em> is …</li>
  <li>Best approximation is …</li>
  <li>Uniform approximation is best approximation in uniform norm.</li>
  <li>https://en.wikipedia.org/wiki/Approximation_theory#Remez’s_algorithm
    <ul>
      <li>https://en.wikipedia.org/wiki/Remez_algorithm
        <ul>
          <li>Inputs: a function, and an interval.</li>
          <li>Output: an optimal polynomial approximating the input function in the input interval.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>What are Bernstein polynomials?
What question does the Weierstrass approximation theorem answer?
    <ul>
      <li>http://www4.ncsu.edu/~mtchu/Teaching/Lectures/MA530/chapter7.pdf</li>
    </ul>
  </li>
  <li><a href="https://en.wikipedia.org/wiki/Chebyshev_polynomials">WP:Chebyshev polynomials</a>
    <ul>
      <li>Why is it important?
  How does it relate to best approximation?
        <ul>
          <li>“Chebyshev polynomials are important in approximation theory because the roots of the Chebyshev polynomials of the first kind, which are also called Chebyshev nodes, are used as nodes in polynomial interpolation.
  The resulting interpolation polynomial minimizes the problem of Runge’s phenomenon and provides an approximation that is close to the polynomial of best approximation to a continuous function under the maximum norm.”</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Machine learning as relation approximation
    <ul>
      <li>Machine learning, statistical modelling, function approximation, and curve fitting are related.</li>
      <li>Generalize function approximation to relation approximation.</li>
      <li>A function can be stated as a relation.</li>
      <li>A relation can be stated as a function.</li>
    </ul>
  </li>
  <li>Consider the least-square solution to an overdetermined system of linear equations.
Is such solution a kind of approximation?
    <ul>
      <li>There is no exact solution to begin with?</li>
      <li>Why is it called “least-squares <em>approximation</em>”?</li>
      <li>How can you approximate something that does not exist?
        <ul>
          <li>1.2 approximates 1.23. Both 1.2 and 1.23 exist.</li>
          <li>Contrarily, there is no X such that AX = B.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>What are approximation schemes?
    <ul>
      <li>https://en.wikipedia.org/wiki/Polynomial-time_approximation_scheme</li>
    </ul>
  </li>
  <li>How do we approximate a function?
Is it even possible to approximate arbitrary functions?
    <ul>
      <li>If the function is analytic, we can truncate its Taylor series.
        <ul>
          <li>Commonly-used differentiable functions are analytic.</li>
        </ul>
      </li>
      <li>Chebyshev polynomials?</li>
      <li>If we have an approximation scheme, we may be able to improve it.
        <ul>
          <li>https://en.wikipedia.org/wiki/Series_acceleration
            <ul>
              <li>https://en.wikipedia.org/wiki/Aitken%27s_delta-squared_process</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>google search: machine learning approximation theory
        <ul>
          <li><a href="https://math.stackexchange.com/questions/2680158/approximation-theory-for-deep-learning-models-where-to-start">Approximation Theory for Deep Learning Models: Where to Start? - Mathematics Stack Exchange</a></li>
          <li>http://www.vision.jhu.edu/tutorials/ICCV15-Tutorial-Math-Deep-Learning-Intro-Rene-Joan.pdf</li>
          <li>2017, slides, “From approximation theory to machine learning: New perspectives in the theory of function spaces and their applications”, <a href="http://npfsa2017.uni-jena.de/l_notes/vybiral.pdf">pdf</a></li>
          <li>2018, article, “Approximation theory, Numerical Analysis and Deep Learning”, <a href="http://at.yorku.ca/c/b/p/g/30.htm">abstract</a>
            <ul>
              <li>“the problem of numerically solving a large class of (high-dimensional) PDEs (such as linear Black-Scholes or diffusion equations) can be cast into a classical supervised learning problem which can then be solved by deep learning methods”</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Determine whether we need to read these
    <ul>
      <li>Very likely
        <ul>
          <li>2015, slides, “Best polynomial approximation: multidimensional case”, <a href="https://carma.newcastle.edu.au/meetings/spcom/talks/Sukhorukova-SPCOM_2015.pdf">pdf</a></li>
          <li>https://en.wikipedia.org/wiki/Bernstein_polynomial#Approximating_continuous_functions
            <ul>
              <li>https://en.wikipedia.org/wiki/Pointwise_convergence</li>
              <li>https://en.wikipedia.org/wiki/Uniform_convergence</li>
            </ul>
          </li>
          <li>https://en.wikipedia.org/wiki/Approximation
            <ul>
              <li>https://en.wikipedia.org/wiki/Approximation_theory
                <ul>
                  <li>is a branch of https://en.wikipedia.org/wiki/Functional_analysis</li>
                  <li>https://en.wikipedia.org/wiki/Approximation_theory#Chebyshev_approximation</li>
                </ul>
              </li>
              <li>https://en.wikipedia.org/wiki/Approximate_computing
                <ul>
                  <li>example: https://en.wikipedia.org/wiki/Artificial_neural_network</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>https://en.wikipedia.org/wiki/Telescoping_series</li>
        </ul>
      </li>
      <li>Likely
        <ul>
          <li>2018, slides, “Deep Learning: Approximation of Functions by Composition”, <a href="http://helper.ipam.ucla.edu/publications/dlt2018/dlt2018_14936.pdf">pdf</a>
            <ul>
              <li>classical approximation vs deep learning</li>
            </ul>
          </li>
          <li>2013, short survey article draft, “Multivariate approximation”, <a href="http://num.math.uni-goettingen.de/schaback/research/papers/MultApp_01.pdf">pdf</a></li>
          <li>1995, short introduction, “Multivariate Interpolation and Approximation by Translates of a Basis Function”, <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.45.2194&amp;rep=rep1&amp;type=pdf">pdf</a></li>
          <li>1989, article, “A Theory of Networks for Approximation and Learning”, <a href="http://www.dtic.mil/docs/citations/ADA212359">pdf available</a>
            <ul>
              <li>What is the summary, especially about learning and approximation theory?</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Unlikely
        <ul>
          <li>Survey-like
            <ul>
              <li>2006, chapter, “Topics in multivariate approximation theory”, <a href="https://www.researchgate.net/publication/226303661_Topics_in_multivariate_approximation_theory">pdf available</a></li>
              <li>1982, article, “Topics in multivariate approximation theory”, <a href="http://www.dtic.mil/dtic/tr/fulltext/u2/a116248.pdf">pdf</a></li>
              <li>1986, “Multivariate Approximation Theory: Selected Topics”, <a href="https://epubs.siam.org/doi/book/10.1137/1.9781611970197">paywall</a></li>
            </ul>
          </li>
          <li>Theorem
            <ul>
              <li>2017, article, “Multivariate polynomial approximation in the hypercube”, <a href="https://people.maths.ox.ac.uk/trefethen/hypercube_published.pdf">pdf</a></li>
            </ul>
          </li>
          <li>2017, article, “Selected open problems in polynomial approximation and potential theory”, <a href="http://drna.padovauniversitypress.it/system/files/papers/BaranCiezEgginkKowalskaNagyPierzcha%C5%82a_DRNA2017.pdf">pdf</a></li>
          <li>2017, article, “High order approximation theory for Banach space valued functions”, <a href="https://ictp.acad.ro/jnaat/journal/article/view/1112">pdf available</a></li>
          <li>Articles summarizing people’s works
            <ul>
              <li>2017, article, “Michael J.D. Powell’s work in approximation theory and optimisation”, <a href="https://www.sciencedirect.com/science/article/abs/pii/S0021904517301053">paywall</a></li>
              <li>2000, article, “Weierstrass and Approximation Theory”, <a href="https://www.sciencedirect.com/science/article/pii/S0021904500935081">paywall</a></li>
            </ul>
          </li>
          <li>2013, article, “[1312.5540] Emerging problems in approximation theory for the numerical solution of nonlinear PDEs of integrable type”, <a href="https://arxiv.org/abs/1312.5540">pdf available</a></li>
          <li>1985, article, “Some problems in approximation theory and numerical analysis - IOPscience”, <a href="http://iopscience.iop.org/article/10.1070/RM1985v040n01ABEH003526">pdf available</a></li>
          <li>2011, article, “Experiments on Probabilistic Approximations”, <a href="https://people.eecs.ku.edu/~jerzygb/c154-clark.pdf">pdf</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Less relevant overview
    <ul>
      <li>Why do we approximate?
        <ul>
          <li>Because it is practically inevitable.
            <ul>
              <li>Fundamental reason: Because computers are finite.</li>
              <li>Practical reason: Trade-off between computation time and precision.
                <ul>
                  <li>The more error we can afford, the faster we can run.
                    <ul>
                      <li>May be related: 2013 monograph “Faster Algorithms via Approximation Theory” <a href="http://theory.epfl.ch/vishnoi/Publications_files/approx-survey.pdf">pdf</a></li>
                    </ul>
                  </li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>2018 book “Recent Advances in Constructive Approximation Theory” <a href="https://www.springer.com/us/book/9783319921648">paywall</a></li>
    </ul>
  </li>
</ul>

  </div>

</article>

      </div>
    </main>

    
    
<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
var disqus_config = function () {
    this.page.url = "https://edom.github.io/approx.html";  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = "https://edom.github.io/approx.html"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://edom-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    

    <footer class="site-footer h-card">
    <data class="u-url" href="/"></data>

    <div class="wrapper">
        <p>This page was created on 2018-08-17 22:52 +0700.</p>
        <p class="rss-subscribe">There is an <a href="/feed.xml">RSS feed</a>,
        but it's unused because this site is a wiki, not a blog.</p>
        <p>Stop writing books, papers, and blogs! Write a personal wiki instead! Or, even better, contribute to a community wiki.</p>
    </div>

</footer>


  </body>

</html>
